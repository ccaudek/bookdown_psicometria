\chapter{Stima della funzione a posteriori}
\label{chapter:stima_funzione_aposteriori} 


%--------------------------------------------------------------------

Nel capitolo~\ref{chapter:updating_discrete} abbiamo visto che, quando usiamo il teorema di Bayes per calcolare la distribuzione a posteriori del parametro di un modello statistico, al denominatore del teorema di Bayes troviamo un integrale.
Tale integrale, nella maggior parte dei casi, non si può risolvere per via analitica.
%Gli integrali sono uno dei motivi per cui, nelle scuole superiori, molti studenti sviluppano un atteggiamento \enquote{poco benevolo} nei confronti della matematica.
%E adesso li incontriamo di nuovo, gli integrali.
%A questo proposito ci sono due notizie.
%La brutta notizia è che non andranno mai via: l'analisi Bayesiana è piena di integrali e non possiamo farne a meno.
%La buona notizia è che nessuno li sa risolvere. 
%Intendo dire, nessuno li sa risolvere in forma analitica, risolvendo delle equazioni.
%Per cui, anni di studio di analisi matematica qui risultano del tutto inutili.
%Ovvero, l'integrale che sta al denominatore del teorema di Bayes si può risolvere in maniera analitica nei casi più semplici (in tutti i casi che discussi in queste dispense).
%Ma tale procedura non si estende a casi più complicati, ovvero alla maggior parte dei modelli statistici di interesse per la psicologia.
%Quindi, se nella maggior parte dei casi che ci interessano, l'analisi matematica non può essere usata per trovare la soluzione, è sensato fare uno sforzo intellettuale molto grande per riuscire a trovare, con quel metodo, una soluzione a dei casi \enquote{semplici}, sapendo però che tali metodi di soluzione non si generalizzano ai casi che veramente ci interessano come psicologi?
%%Ovviamente no.
%%Per cui procederemo in modo diverso. 
%%La capacità di risolvere gli risolvere gli integrali analiticamente ci serve a ben poco, nei casi di maggiore interesse per la psicologia, per cui dobbiamo sviluppare la capacità di trovare una soluzione a questo problema utilizzando un metodo diverso. 
% risulta intrattabile per la maggior parte dei modelli statistici di interesse per la psicologia.
L'inferenza bayesiana si sviluppa dunque mediante una stima numerica della funzione a posteriori.
Dato che questi metodi sono \enquote{computazionalmente intensivi}, possono solo essere svolti mediante software. 
In anni recenti i metodi Bayesiani di analisi dei dati sono diventati sempre più popolari proprio perché la potenza di calcolo necessaria per svolgere tali calcoli è ora alla portata di tutti.
Questo non era vero solo pochi decenni fa.
Per capire come la distribuzione a posteriori possa essere approssimata per via numerica esamineremo qui tre diverse tecniche che possono essere utilizzate a questo scopo:
\begin{enumerate}
\item i metodi numerici convenzionali,
\item il metodo dell'approssimazione quadratica,
\item i metodi Monte Carlo basati su Catena di Markov (MCMC).
\end{enumerate}


\section{Metodi numerici convenzionali}
\label{sec:met_numerici_convenzionali}

È possibile stimare l'intera distribuzione a posteriori mediante metodi numerici convenzionali.
Questo è l'approccio più semplice.
Tuttavia, anche se tali metodi possono fornire risultati accuratissimi, a causa della \enquote{maledizione della dimensionalità}, tali procedure numeriche sono utilizzabili solo nel caso di modelli statistici semplici, con non più di due parametri.
Nella pratica concreta tali metodi vengono sostituiti da altre tecniche più efficienti in quanto, anche in comuni modelli utilizzati in psicologia, vengono stimati centinaia se non migliaia di parametri.
Nell'esempio che faremo in questa sezione risulterà chiara la ragione per cui, in tali circostanze, non è possibile usare una tale procedura.
I metodi numerici convenzionali sono invece utili come strumento didattico in quanto ci forniscono una procedura molto diretta e intuitiva che rende molto trasparente il processo dell'aggiornamento Bayesiano.
Per questa ragione esamineremo qui un esempio relativo a tale procedura esaminando un modello statistico che dipende da un solo parametro sconosciuto.


\subsection{La procedura dell'approssimazione numerica}
\label{sec:appross_numerica}

È molto semplice trovare una approssimazione numerica della distribuzione a posteriori e ciò può essere fatto come indicato di seguito.
Anche se la maggior parte dei parametri è continua (ovvero, in linea di principio ciascun parametro può assumere un numero infinito di valori), possiamo ottenere un'eccellente approssimazione della  distribuzione a posteriori considerando solo una griglia finita di valori dei parametri.
Per calcolare la probabilità a posteriori in corrispondenza di ciascun particolare valore del parametro, chiamiamolo $\theta'$, è sufficiente moltiplicare la probabilità a priori di $\theta'$ per il valore della funzione di verosimiglianza in corrispondenza di $\theta'$.
Una stima della distribuzione a posteriori si genera ripetendo questo procedimento per ciascun valore nella griglia.


\subsection{Un esempio pratico}
\label{sec:es_pratico_zetsche}

Facciamo un esempio concreto consideriando nuovamente la ricerca di \citet{zetsche_future_2019}.
Questi autori si sono chiesti se gli individui depressi manifestino delle aspettative accurate circa il loro umore futuro, oppure se tali aspettative siano distorte negativamente.
Consideriamo qui i 30 partecipanti dello studio di \citet{zetsche_future_2019} che hanno riportato la presenza di un episodio di depressione maggiore in atto.
All'inizio della settimana di test a questi pazienti è stato chiesto di valutare l'umore che si aspettavano di sentire nei giorni seguenti della settimana.
Mediante una app, i partecipanti dovevano poi valutare il proprio umore in cinque momenti diversi di ciascuno dei cinque giorni successivi.
Lo studio considera diverse emozioni, ma qui ci concentriamo solo sulla tristezza.
%Qui ci concentreremo qui su questi dati e descriveremo un'analisi diversa da quella fatta dagli autori (che è notevolmente più complessa).

Sulla base dei dati forniti dagli autori, abbiamo calcolato la media dei giudizi  relativi al livello di tristezza raccolti da ciascun partecipante tramite la app.
Tale media è stata poi sottratta dall'aspettativa del livello di tristezza fornita all'inizio della settimana.
Per semplificare l'analisi abbiamo considerato la discrepanza tra aspettative e realtà come un evento dicotomico: valori positivi di tale differenza indicano che le aspettative circa il livello di tristezza sono maggiori del livello di tristezza che in seguito viene effettivamente esperito; ciò significa che le aspettative sono negativamente distorte (evento codificato con \enquote{1}).
Si può dire il contrario (le aspettative sono positivamente distorte) se tale differenza assume valori negativi (evento codificato con \enquote{0}).
%dunque, valori positivi di tale differenza indicano che le aspettative circa il livello di tristezza erano maggiori del livello di tristezza che in seguito era stato  effettivamente esperito).
%Abbiamo codificato con 1 i valori positivi di tale differenza (aspettative negativamente distorte) e con 0 i valori negativi (aspettative positivamente distorte).
Nel campione dei 30 partecipanti clinici qui esaminati, 23 partecipanti manifestano delle aspettative negativamente distorte e 7 partecipanti manifestano delle aspettative positivamente distorte.
Chiamiamo $\theta$ la probabilità dell'evento \enquote{le aspettative del partecipante sono distorte negativamente}. 
Ci poniamo il problema di ottenere la stima a posteriori di $\theta$, dati i 23 "successi" in 30 prove che sono stati osservati.
Per questo esempio considereremo 50 valori egualmente spaziati per il parametro $\theta$: 0.000, 0.0204, \dots, 0.978, 1.000.

\subsubsection{Distribuzione a priori}

Supponiamo che le nostre credenze a priori sulla tendenza di un individuo clinicamente depresso a manifestare delle aspettative distorte negativamente circa il suo umore futuro siano molto scarse.
Assumiamo quindi per $\theta$ una distribuzione iniziale uniforme nell'intervallo [0, 1].
% ovvero
%\begin{equation}
%p(\theta) = 1, \quad \text{per } 0 \leq \theta \leq 1.
%\end{equation}
Dato che consideriamo soltanto $n = 50$ valori del parametro $\theta$, creiamo un vettore di 50 elementi che conterrà i corrispondenti valori della distribuzione a priori, scalando ciascun valore di questo vettore per $n$ in modo tale che la somma di tutti i valori della distribuzione a priori (0.02, 0.02, \dots, 0.02, 0.02) sia uguale a 1.0 (in questo modo viene definita una funzione di massa di probabilità).
La distribuzione a priori così costruita è riportata nella figura~\ref{fig:Zetsche_prior1_ll_posterior}, pannello in alto.

\subsubsection{Funzione di verosimiglianza}

Calcoliamo ora la funzione di verosimiglianza utilizzando i 50 valori $\theta$ che abbiamo preso in considerazione.
Per ciascuno dei 50 valori $\theta$ applichiamo la formula della probabilità binomiale tendendo sempre costanti i valori dei dati (ovvero 23 \enquote{successi} in 30 prove).
Procedendo in questo modo creiamo la funzione di verosimiglianza riportata nella  figura~\ref{fig:Zetsche_prior1_ll_posterior}, pannello intermedio.
Per esempio, per il valore $\theta = 0.816$ l'ordinata della funzione di verosimiglianza sarà
\begin{align}
\binom{30}{23}& \cdot 0.816^{23} \cdot (1 - 0.816)^{7} = 0.135\notag
\end{align}
e per $\theta = 0.837$ l'ordinata della funzione di verosimiglianza sarà
\begin{align}
\binom{30}{23}& \cdot 0.837^{23} \cdot (1 - 0.837)^{7} = 0.104.\notag
\end{align}

\subsubsection{La stima della distribuzione a posteriori}

La distribuzione a posteriori del parametro $\theta$ è data dal prodotto della verosimiglianza e della distribuzione a priori, scalata per una costante di normalizzazione.
Quindi, facendo il prodotto dei valori della distribuzione a priori e i valori della funzione di verosimiglianza otteniamo la funzione a posteriori non standardizzata. 
Dato che la distribuzione a priori è uniforme, per ottenere questo risultato è sufficiente moltiplicare ciascun valore della funzione di verosimiglianza per 0.02.
Per esempio, per i due valori della funzione di verosimiglianza che abbiamo calcolato sopra, avremo $0.135 \cdot 0.02$ e $0.104 \cdot 0.02$.
Avendo svolto questo prodotto per tutti i 50 valori della funzione di verosimiglianza, dobbiamo poi dividere ciascuno dei 50 numeri così trovati per la costante di normalizzazione.
Nel caso discreto, trovare il denominatore del teorema di Bayes è molto facile: esso è 
dato dalla somma di tutti i valori della distribuzione a posteriori non normalizzata.
Per i dati presenti, tale costante di normalizzazione è uguale a 0.032.
Possiamo dunque standardizzare i due valori trovati sopra nel modo seguente: $0.135 \cdot 0.02 / 0.032$ e $0.104 \cdot 0.02 / 0.032$.
Così facendo, otterremo il risultato per cui la somma di tutti e 50 i valori della distribuzione a posteriori normalizzata sarà uguale a 1.0.
In questo particolare esempio, la distribuzione a posteriori trovata come descritto sopra non è altro che la versione normalizzata della funzione di verosimiglianza: questo avviene perché la distribuzione a priori uniforme non ha aggiunto altre informazioni oltre a quelle che erano già fornite dalla funzione di verosimiglianza.
Il risultato è riportato nella figura~\ref{fig:Zetsche_prior1_ll_posterior}, pannello in basso.

\begin{figure}[h!]
 \centering
 \includegraphics[width=\textwidth]{Zetsche_prior1_ll_posterior.pdf}
 \caption{Rappresentazione della distribuzione a priori per il parametro $\theta$, ovvero la probabilità di aspettative future distorte negativamente \citep{zetsche_future_2019} (pannello in alto), funzione di verosimiglianza (pannello intermedio) e distribuzione a posteriori (pannello in basso). 
Le tre funzioni sono state calcolate utilizzando 50 modalità equi-spaziate del parametro $\theta$. 
I segmenti verticali rappresentano l'intensità della funzione in corrispondenza di ciascuna modalità parametro $\theta$. 
Nel pannello in alto e nel pannello in basso la somma delle lunghezze dei segmenti verticali è pari ad 1.0; ciò non si verifica, invece, nel caso del pannello intermedio.}
 \label{fig:Zetsche_prior1_ll_posterior}
 \end{figure}

\subsection{Un esempio pratico (versione 2)}

Continuiamo la discussione dell'esempio precedente supponendo che la letteratura precedente ci fornisca delle informazioni a proposito di $\theta$, ovvero sulla probabilità che le aspettative future di un individuo clinicamente depresso siano distorte negativamente.
In tali circostanze, invece di utilizzare la distribuzione uniforme per $p(\theta)$,
definiamo la distribuzione a priori per $\theta$ come una distribuzione che ha la forma di una Beta di parametri $\alpha = 2$ e $\beta = 10$. 
In questo modo, la distribuzione a priori di $\theta$ ritiene molto plausibili valori bassi di $\theta$, mentre i valori $\theta$ superiori a 0.5 vengono considerati impossibili. 
Questo è equivalente a dire che ci aspettiamo che le aspettative relative all'umore futuro siano distorte negativamente solo per pochissimi individui clinicamente depressi -- in altre parole, ci aspettiamo che la maggioranza degli individui clinicamente depressi sia inguaribilmente ottimista.  
Questa è, ovviamente, un'opinione a priori molto difficile da giustificare. 
La esamino qui, non perché abbia senso nel contesto dei dati di \citet{zetsche_future_2019}, ma soltanto per fare un esempio che mostra come la distribuzione a posteriori fornisca una sorta di \enquote{compromesso} tra la distribuzione a priori e la verosimiglianza. 

Con calcoli del tutto simili a quelli descritti sopra si giunge alla distribuzione a posteriori rappresentata nella figura~\ref{fig:Zetsche_2_posterior}. 
Si noti come, nella figura~\ref{fig:Zetsche_2_posterior}, la distribuzione a priori e la distribuzione a posteriori sono molto diverse.
In particolare, si noti che la distribuzione a posteriori risulta spostata verso destra su posizioni più vicine a quelle della verosimiglianza.
Discuteremo in seguito l'influenza della distribuzione a priori sull'inferenza finale.

\begin{figure}
 \label{fig:Zetsche_2_posterior}
 \centering
 \includegraphics[width=\textwidth]{Zetsche_prior2_ll_posterior.pdf}
 \caption{Rappresentazione della distribuzione a priori per il parametro $\theta$, ovvero la probabilità di aspettative future distorte negativamente \citep{zetsche_future_2019} (pannello in alto), funzione di verosimiglianza (pannello intermedio) e distribuzione a posteriori (pannello in basso). La differenza con la figura~\ref{fig:Zetsche_prior1_ll_posterior} è che la distribuzione a priori segue la forma di una Beta(2, 10). 
Di conseguenza, facendo un confronto con quanto accade nella figura~\ref{fig:Zetsche_prior1_ll_posterior}, la distribuzione a posteriori si sposta verso sinistra.}
\end{figure}

\subsection{Sommario della funzione a posteriori}

Una volta calcolata la distribuzione a posteriori dobbiamo riassumerla in qualche modo.
Nel caso in cui venga usato il metodo di approssimazione numerica, il problema del calcolo delle aree sottese alla funzione a posteriori in qualunque intervallo può essere risolto in vari modi.
Tuttavia, questo problema trova una soluzione molto più semplice se viene utilizzato un  metodo diverso per la stima della distribuzione a posteriori, come vedremo di seguito.
Non discuteremo dunque la possibile soluzione di questo problema nel caso presente, che rappresenta solo un esempio didattico.


\section{Approssimazione quadratica}

I metodi numerici convenzionali possono essere usati solo quando il numero di parametri da stimare è piccolo.
La ragione di ciò sta nella cosiddetta \enquote{maledizione della dimensionalità}. 
Vediamo cosa significa.
Nel caso di un solo parametro, supponiamo di utilizzare una griglia di 100 valori.
Per due parametri avremo bisogno di $100^2$ valori.
Ma già per 10 parametri avremo bisogno di $10^{10}$ valori -- è facile capire che una tale quantità di valori sia troppo grande anche per un computer potente come quello che utilizziamo normalmente.
Dobbiamo dunque affrontare il problema in un altro modo.

Una possibile soluzione al nostro problema ci viene fornita dal metodo dell'approssimazione quadratica. 
La motivazione di tale metodo è la seguente.
Sappiamo che, in generale, la regione della distribuzione a posteriori che si trova in prossimità del suo massimo può essere ben approssimata dalla forma di una distribuzione Normale.
Descrivere la distribuzione a posteriori mediante la distribuzione Normale significa utilizzare un'approssimazione che viene, appunto, chiamata \enquote{quadratica}\footnote{Tale approssimazione si dice quadratica perché il logaritmo di una distribuzione gaussiana forma una parabola e la parabola è una funzione quadratica -- dunque, mediante questa approssimazione descriviamo il logaritmo della distribuzione a posteriori mediante una parabola.}.

L'approssimazione quadratica si pone due obiettivi.
\begin{enumerate}
\item Trovare la moda della distribuzione a posteriori.
Ci sono varie procedure di ottimizzazione, implementate in \textsf{R}, in grado di trovare il massimo di una distribuzione.
\item
Stimare la curvatura della distribuzione in prossimità della moda.
Una stima della curvatura è sufficiente per trovare un'approssimazione quadratica dell'intera distribuzione.
In alcuni casi, questi calcoli possono essere fatti seguendo una procedura analitica, ma solitamente vengono usate delle tecniche numeriche.
\end{enumerate}

Una descrizione della distribuzione a posteriori ottenuta mediante l'approssimazione quadratica si ottiene mediante la funzione \texttt{quap()} contenuta nel pacchetto \texttt{rethinking}.
Tale pacchetto, creato da Richard McElreath per accompagnare il suo testo \emph{Statistical Rethinking}$^2$, può essere scaricato utilizzando le istruzioni seguenti\footnote{
È possibile che, per i diversi sistemi operativi, sia necessaria l'installazione di componenti ulteriori. Si veda \url{https://www.rdocumentation.org/packages/devtools/versions/1.13.6}.
}:
\begin{lstlisting}
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github(
  "rmcelreath/rethinking", ref="Experimental"
)
\end{lstlisting}
Vedremo nel capitolo~\ref{chapter:stat_models} come tale funzione possa essere usata\footnote{
Le analisi Bayesiane che discuteremo in queste dispense, per la maggior parte, faranno uso della funzione \texttt{quap()}.
È dunque fondamentale che gli studenti installino il pacchetto \texttt{rethinking} sul loro computer.
}.
Dal nostro punto di vista non è importante capire come si svolgono in pratica i calcoli necessari per la stima della distribuzione a posteriori con il metodo dell'approssimazione quadratica.
Quello che è importante capire è il significato della distribuzione a posteriori e questo significato è stato chiarito nella \S~\ref{sec:appross_numerica}.
L'approssimazione quadratica fornisce risultati simili (o identici) a quelli ottenuti con il metodo descritto nella \S~\ref{sec:appross_numerica}.
Il vantaggio dell'approssimazione quadratica è che disponiamo di una serie di funzioni \R\, che svolgono questi calcoli per noi.

In realtà, l'approssimazione quadratica è poco usata in pratica, perché per problemi complessi è più conveniente usare i metodi Monte Carlo basati su Catena di Markov (MCMC) che verranno descritti nella \S~\ref{sec:mmmc}.
Tuttavia, per potere utilizzare i metodi MCMC è necessario installare sul proprio computer del software aggiuntivo e tale operazione, in generale, è piuttosto complessa. 
Non è l'obiettivo di questo insegnamento approfondire questo aspetto. 
%risulta piuttosto complesso installare sul proprio computer il software necessario per potere utilizzare i metodi MCMC.
Per questa ragione, per svolgere gli esercizi che verranno assegnati, sarà sufficiente fare ricorso al metodo dell'approssimazione quadratica; ovvero sarà sufficiente usare la funzione \texttt{quap()}.


%--------------------------------------------------------------------
\section{Integrazione con metodo Monte Carlo}
\label{sec:integr_MC}

%Quando il modello statistico include vettori di parametri l'integrale al denominatore del teorema di Bayes è in più dimensioni e calcolarlo analiticamente non è sempre agevole o addirittura impossibile. 
%Nonostante l'aumentata potenza di calcolo dei moderni computer, anche le soluzioni per via numerica (come quelle che abbiamo descritto nella \S~\ref{sec:met_numerici_convenzionali}) all'aumentare della dimensione del vettore dei parametri diventano via via più dispendiose fino a diventare inutilizzabili in tempi ragionevoli.
%Diventa quindi necessario trovare soluzioni alternative e quella maggiormente usata nell'inferenza bayesiana è la stima dell'integrale tramite simulazione. 
%%L'approccio che viene usato in questi casi è quello di generare un grande numero di valori casuali dalla distribuzione a posteriori per poi poterne calcolare misure di sintesi e poter fare inferenza.
%%Nell'inferenza bayesiana la distribuzione a posteriori viene solitamente stimata con metodi Monte Carlo basati su Catena di Markov, come vedremo nella sezione~\ref{sec:mmmc}.
%Il metodo Monte Carlo gioca un ruolo centrale in questa procedura.
Prima di introdurre i metodi MCMC per la stima della funzione a posteriori, spendiamo due parole sul metodo Monte Carlo quale tecnica che consente il calcolo degli integrali mediante simulazione numerica.
Il termine Monte-Carlo si riferisce al fatto che per la computazione si ricorre
ad un ripetuto campionamento casuale attraverso la generazioni di sequenze di numeri casuali.


%--------------------------------------------------------------------
\subsection{Legge forte dei grandi numeri}
\label{sec:legge_forte_grandi_numeri}

L'integrazione con metodo Monte Carlo trova la sua giustificazione nella \emph{Legge forte dei grandi numeri} la quale può essere espressa nei termini seguenti.
Data una successione di variabili casuali $Y_{1}, Y_{2},\dots, Y_{n},\dots$ indipendenti e identicamente distribuite con media $\mu$, ne segue che
\begin{equation}
P\left( \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n Y_i = \mu \right) = 1.
\end{equation}
Ciò significa che, al crescere di $n$, la media delle realizzazioni di $Y_{1}, Y_{2},\dots, Y_{n},\dots$ converge con probabilità 1 al vero valore $\mu$.

Un esempio della legge forte dei grandi numeri riguarda una serie di lanci di una moneta dove $Y=1$ significa \enquote{testa} e $Y=0$ significa \enquote{croce}. 
Per la legge forte dei grandi numeri, nel caso di una moneta equilibrata la proporzione di eventi \enquote{testa} converge alla vera probabilità dell'evento \enquote{testa} 
\[
\frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \frac{1}{2}
\]
con probabilità di uno.

Quello che è stato detto sopra non è che un modo sofisticato per dire che, se vogliamo calcolare un'approssimazione del valore atteso di una variabile aleatoria, non dobbiamo fare altro che la media aritmetica di un grande numero di realizzazione di tale variabile aleatoria.
Come è facile intuire, l'approssimazione migliora al crescere del numero di dati che abbiamo a disposizione.


%%--------------------------------------------------------------------
%\subsection{Esempio di integrazione di Monte Carlo}
%\label{sec:example_MC_integration}
%
%Una delle applicazioni più comuni del calcolo integrale è quella che consente di calcolare l'area sottesa al grafico di una funzione. 
%Considereremo ora il metodo dell'integrazione di Monte Carlo che permette di trovare in maniera semplice una soluzione approssimata del problema per mezzo di una simulazione.
%Data una funzione qualunque, ad esempio
%\[
%f(x) = 1 + x^{2}, \quad \textrm{per} {}-\infty < x < {}+\infty,    
%\]
%poniamoci il problema di trovare l'area sottesa alla funzione nell'intervallo compreso tra -1 e +1.
%Se utilizziamo \textsf{R}, una soluzione a questo problema può essere trovata  utilizzando la funzione {\tt integrate()}\footnote{
%La soluzione analitica di tale problema fornisce il seguente risultato:
%\[
%\begin{array}{lcl}    
%\int\limits_{{}-1}^{1} f(x)dx & = & \int\limits_{{}-1}^{1} 1 + x^{2} dx \\    
%& = & \left( 1 + \frac{1}{3}1^{3} \right) - \left({}-1 + \frac{1}{3}\left({}-1\right)^{3} \right)   \\    
%& = & 1.33 + 1.33 \\    
%& = & 2.67.
%\end{array}     
%\]
%}:
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%int_fun <- function(x) 1 + x^2
%integrate(int_fun,-1, 1)
%# 2.666667 with absolute error < 3e-14
%\end{lstlisting}
%\end{mdframed} 
%
%
%Il nostro scopo, tuttavia, è quello di utilizzare una procedura intuitiva per calcolare, in maniera approssimata, l'area desiderata.  
%L'integrazione con metodo Monte Carlo genera casualmente un numero prefissato di punti di valutazione all'interno del dominio per poi determinare il numero di punti di valutazione che sono collocati al di sotto della funzione considerata. 
%Nel caso presente, tale metodo è stato implementato definendo un rettangolo la cui base ha coordinate $(-1, 0)$ e $(1, 0)$, con un'altezza arbitrariamente più alta della funzione considerata. 
%All'interno di tale rettangolo viene posto un numero predeterminato di punti di valutazione in posizioni casuali. 
%L'area cercata è data dal  numero relativo di punti di valutazione le cui coordinate $y$ sono minori di quelle della funzione considerata. Questo processo, implementato nella funzione \R\; qui non riportata 
%%{\tt mc\_int()} riportata qui sotto, 
%è illustrato nelle figure~\ref{fig:MC_integr_1} e \ref{fig:MC_integr_3} per i casi di 100 e \num{10000} punti. 
%All'aumentare del numero di punti l'approssimazione migliora. 
%Si noti che il risultato trovato in questo modo è simile alla soluzione corretta.
%
%\begin{figure}[!htb]
%\minipage{0.49\textwidth}
%  \includegraphics[width=\linewidth]{MC_integr_1.pdf}
%  \caption{100 punti.}\label{fig:MC_integr_1}
%\endminipage\hfill
%\minipage{0.49\textwidth}%
%  \includegraphics[width=\linewidth]{MC_integr_3.pdf}
%  \caption{10,000 punti.}\label{fig:MC_integr_3}
%\endminipage
%\end{figure}


%--------------------------------------------------------------------
\section{Metodi MC basati su Catena di Markov}
\label{sec:mmmc}

%Quando il modello statistico include vettori di parametri l'integrale al denominatore del teorema di Bayes è in più dimensioni e calcolarlo analiticamente non è sempre agevole o addirittura impossibile. 
%Nonostante l'aumentata potenza di calcolo dei moderni computer, anche le soluzioni per via numerica (come quelle che abbiamo descritto nella \S~\ref{sec:met_numerici_convenzionali}) all'aumentare della dimensione del vettore dei parametri diventano via via più dispendiose fino a diventare inutilizzabili in tempi ragionevoli.
%Diventa quindi necessario trovare soluzioni alternative e quella maggiormente usata nell'inferenza bayesiana è la stima dell'integrale tramite simulazione. 
%%L'approccio che viene usato in questi casi è quello di generare un grande numero di valori casuali dalla distribuzione a posteriori per poi poterne calcolare misure di sintesi e poter fare inferenza.

I metodi Monte Carlo basati su Catena di Markov consentono di costruire sequenze di punti (le \enquote{catene}) nello spazio dei parametri, la cui densità è proporzionale alla distribuzione di probabilità a posteriori a cui siamo interessati.
Questo, evidentemente, è il risultato vorremmo ottenere.
Ma cosa sono le catene di Markov?
In termini formali possiamo dire che una catena di Markov è una sequenza di variabili aleatorie $Y_{1}, Y_{2},\dots, Y_{n}$ tale che la dipendenza della distribuzione di $Y_{i+1}$ dai valori di $Y_{1}, \dots, Y_{i}$ è interamente dovuta al valore di $Y_i$, cioè il passaggio ad uno stato del sistema dipende unicamente dallo stato immediatamente precedente e non dal come si è giunti a tale stato (dalla storia). 
Per questo motivo si dice che un processo markoviano è senza memoria. 

In generale è possibile generare catene di Markov che convergono ad una soluzione unica e stazionaria tale per cui gli elementi della catena sono campioni dalla distribuzione di interesse.
Nel caso dell'inferenza Bayesiana la distribuzione di interesse è la distribuzione a posteriori, $p(\theta \mid x)$.
Le catene di Markov possono quindi essere utilizzate per stimare i valori di aspettazione di variabili rispetto alla distribuzione a posteriori. 
In altre parole, possiamo utilizzare le catene di Markov per stimare i valori a posteriori dei parametri sconosciuti di un modello statistico -- un esempio è il parametro $p$ nel problema del mappamondo che abbiamo discusso in precedenza.

La generazioni di elementi di una catena ha una natura probabilistica e
esistono diversi algoritmi per costruire catene di Markov. 
%La simulazione Monte Carlo genera un grande numero di valori casuali dalla distribuzione a posteriori per poi poterne ricavare misure di sintesi e potere fare inferenza.
%Sono stati messi a punto tanti algoritmi diversi che consentono di estrarre  valori casuali dalla distribuzione a posteriori e ne vengono continuamente inventati di nuovi.
Due aspetti da tenere in considerazione sotto questo punto di vista sono il periodo di \emph{burn-in} e le correlazioni tra punti. 
Al crescere degli step della catena, la distribuzione di target viene sempre meglio approssimata. 
All'inizio del campionamento però la distribuzione può essere significativamente lontana da quella stazionaria. 
Ci vuole un certo tempo prima di raggiungere la distribuzione stazionaria
di equilibrio e tale periodo è detto di \emph{burn-in}. 
I campioni provenienti da tale parte iniziale della catena vanno tipicamente scartati perché possono non rappresentare accuratamente la distribuzione desiderata.

Normalmente, un algoritmo MCMC genera catene di Markov di campioni, ognuno dei quali è autocorrelato a quelli generati immediatamente prima e dopo di lui.
Conseguentemente campioni successivi non sono indipendenti ma formano una catena di Markov con un certo grado di correlazione.
Questa correlazione introduce una distorsione nella soluzione finale che si ottiene con questo metodo.
% (se si vogliono campioni indipendenti si può ad esempio considerare solo quelli generati ogni un certo numero, ad esempio ogni cento, e scartare il resto).
L'arte dei diversi algoritmi MCMC risiede nel rendere il meccanismo efficiente e capace di produrre un risultato non distorto, il che implica la riduzione al minimo del tempo di \emph{burn-in} e della correlazione tra i diversi campioni. 
%Ciò può dipendere ovviamente dalle specifiche caratteristiche del problema e dalla distribuzione che si vuole esplorare.

Presentiamo ora, in una forma intuitiva, l'algoritmo di Metropolis, ovvero il primo algoritmo MCMC che è stato proposto.
Tale algoritmo è stato  sviluppato in seguito per renderlo via via più efficiente.
Il nostro obiettivo, però, è solo quello di capire la logica sottostante -- lasciamo agli ingegneri la soluzione del problema dell'efficienza.


%--------------------------------------------------------------------
\subsection{Il problema del turista viaggiatore}
\label{sec:turista_viagg}

%Questo algoritmo ha una classica applicazione nella risoluzione del cosiddetto
%\enquote{problema del commesso viaggiatore}.
%Il commesso deve visitare N città in N giorni e si chiede in che ordine le deve visitare in modo da minimizzare il costo dei viaggi.
L'algoritmo di Metropolis è stato presentato usando varie metafore: quella di un politico che viaggia tra isole diverse \citep{doing_bayesian_data_an}, o quella di un monarca che, anche lui, si sposta tra le isole di un arcipelago \citep{stat_rethinking}.
Qui mutiamo leggermente la metafora e immaginiamo un turista in vacanza su un'isola che ha 10 spiagge di grandezza diversa.
Le spiagge si succedono in ordine di grandezza crescente: partendo dalla spiaggia più piccola si arriva ad una spiaggia un po' più grande, via via fino ad arrivare all'ultima spiaggia, la decima, che è la più grande di tutte.  
Dato che l'isola è circolare, la decima spiaggia confina con la prima spiaggia.

Per non annoiarsi, il nostro turista vuole passare un po' di tempo su ogni spiaggia, ma con il vincolo che il tempo passato su ciascuna spiaggia deve essere proporzionale alla grandezza della spiaggia.  
Infatti, il turista preferisce le spiagge più grandi; nel contempo, però, vuole anche visitare spiagge diverse, quindi il vincolo descritto sopra sembra un buon compromesso tra il desiderio di cambiare spiaggia di tanto in tanto e il desiderio di scegliere la spiaggia più grande.

Essendo in vacanza, il turista non vuole preparare un calendario che stabilisca in anticipo la spiaggia da visitare ogni giorno, ma vuole decidere in maniera rilassata e un po' casuale, ogni mattina, restando però fedele al vincolo che si è dato.
Al bar incontra un altro turista, l'ingegnere Metropolis, che gli suggerisce come fare per raggiungere il suo obiettivo.
Seguendo le istruzioni di Metropolis, il nostro turista decide di comportarsi nel modo seguente.

\begin{enumerate}
  \item 
  Ogni mattina si pone il problema di decidere se ritornare nella spiaggia dov'era stato il giorno prima (chiamiamola spiaggia \emph{corrente}) oppure andare in una delle due spiagge contigue.
  \item 
  Lancia una moneta.
  Se esce testa, considera la possibilità di andare nella spiaggia a che confina con la spiaggia corrente muovendosi in senso orario; se esce croce, considera la possibilità di andare nella spiaggia a che confina con la spiaggia corrente muovendosi in senso antiorario.
La spiaggia selezionata in questo modo viene chiamata spiaggia \emph{proposta}.
  \item 
  Dopo avere trovato la spiaggia proposta, il turista deve decidere se effettivamente andare su quella spiaggia oppure no e, per decidere, procede in questo modo.
  Prende un numero di conchiglie proporzionale alla grandezza della spiaggia proposta -- per esempio, se la spiaggia proposta è la numero 7, allora prenderà 7 conchiglie.
  Prende un numero di sassolini proporzionale alla grandezza della spiaggia corrente -- per esempio, se la spiaggia corrente è la numero 6, allora prenderà 6 sassolini.
  \item
  Se il numero di conchiglie è maggiore del numero di sassolini, il turista si sposta sempre nella spiaggia proposta.
  Ma se ci sono meno conchiglie che sassolini, scarta un numero di sassolini uguale al numero di conchiglie e mette gli oggetti rimanenti in un sacchetto -- per esempio, se la spiaggia proposta è la 5 e la spiaggia corrente è la 6, allora metterà nel sacchetto 5 conchiglie e 1 sassolino.
  Mescola bene ed estrae dal sacchetto un oggetto: se è una conchiglia si sposta nella spiaggia proposta, se è un sassolino resta nella spiaggia corrente.
  Di conseguenza, la probabilità che il turista cambi spiaggia (ovvero $\frac{5}{6}$) è uguale al numero di conchiglie diviso per il numero originale di sassolini.
%  il turista si sposterà nella spiaggia più piccola con probabilità $p_{\text{spostamento}} = \frac{p_{\text{proposta}}}{p_{\text{corrente}}}$.
\end{enumerate}
Decidere di procedere in questo modo potrebbe sembrare un modo per rovinarsi le vacanze.
%In effetti, quando ha incontrato il turista al bar, Metopolis forse aveva bevuto qualche bicchiere di troppo.
Incredibilmente, però, questo algoritmo funziona!
Seguendo la proposta di Metropolis, il turista passerà su ciascuna spiaggia un numero di giorni proporzionale alla grandezza della spiaggia.

\citet{stat_rethinking} ha implementato in \textsf{R} l'algoritmo di Metropolis nel modo seguente:
\begin{lstlisting}
num_weeks <- 1e5
positions <- rep(0, num_weeks)
current <- 10
for (i in 1:num_weeks) {
  # record current position
  positions[i] <- current
  # flip coin to generate proposal
  proposal <- current + sample(c(-1, 1), size = 1)
  # now make sure he loops around the archipelago
  if (proposal < 1) proposal <- 10
  if (proposal > 10) proposal <- 1
  # move?
  prob_move <- proposal / current
  current <- ifelse(runif(1) < prob_move, proposal, current)
}
\end{lstlisting}
Le istruzioni seguenti sono state usate per generare la figura~\ref{fig:rethinking_fig_9_2} a sinistra.
 \begin{lstlisting}
ggplot(
    data.frame(x = 1:100, y = positions[1:100]),
    aes(x, y)
) +
  geom_point(color = "#8184FC") +
  labs(
    x = "Giorno",
    y = "Isola"
  ) +
  scale_y_continuous(breaks=1:10)
\end{lstlisting}
Le istruzioni seguenti sono state usate per generare la figura~\ref{fig:rethinking_fig_9_2} a destra.
Tale figura raffigura proprio il risultato che si voleva ottenere: il numero di giorni di permanenza su ciascuna spiaggia è proporzionale alla grandezza della spiaggia. 
\begin{lstlisting}
ggplot(
  data.frame(x = 1:10, y = as.numeric(table(positions))),
  aes(x = x, xend = x, y = 0, yend = y)
  ) +
  geom_segment(color = "#8184FC", size = 1.5) +
  labs(
    x = "Isola",
    y = "Numero di giorni"
  ) +
  scale_x_continuous(breaks=1:10)
\end{lstlisting}


\begin{figure}[h!]
 \centering
 \includegraphics[width=\textwidth]{rethinking_fig_9_2}
 \caption{
Risultati dell'algoritmo di Metropolis utilizzato dal turista viaggiatore. Il pannello di sinistra mostra la spiaggia scelta dal turista (asse verticale) in funzione di ciascun giorno della sua vacanza (asse orizzontale). In una giorno particolare, è quasi impossibile dire in quale spiaggia si troverà il turista. Il pannello di destra mostra il comportamento a lungo termine dell'algoritmo il quale rivela che il tempo trascorso dal turista su ciascuna spiaggia risulta essere proporzionale alla grandezza della spiaggia.
 }
 \label{fig:rethinking_fig_9_2}
 \end{figure}
 
L'algoritmo funziona anche se il turista decide di spostarsi dalla spiaggia corrente a qualunque altra spiaggia, non solo su quelle confinanti. 
Inoltre, l'algoritmo funziona per qualunque numero di spiagge e anche se il turista non sa quante spiagge ci sono sull'isola.
Affinché l'algoritmo funzioni è solo necessario conoscere la grandezza della spiaggia corrente e quella della spiaggia proposta.

 
\subsection{L'algoritmo di Metropolis}
 
L'algoritmo descritto nella sezione precedente è un caso speciale dell'algoritmo di Metropolis e questo algoritmo è un caso speciale dei metodi MCMC.
L'algoritmo di Metropolis, al di là dell'uso che ne fa il fortunato turista dell'esempio discusso prima, viene in realtà impiegato per per ottenere una sequenza di campioni casuali da una distribuzione a posteriori la cui forma è, solitamente, sconosciuta.
Fuor di metafora:
\begin{itemize}
\item 
le \enquote{i numeri delle spiagge} corrispondono ai valori del parametro -- non è necessario che il parametro assuma solo valori discreti, può anche assumere un insieme continuo di valori;
\item 
la \enquote{grandezza della spiaggia} corrisponde alla densità a posteriori associata a ciascuno dei possibili valori del parametro;
\item
i \enquote{giorni di permanenza su una spiaggia} corrispondono al numero di campioni estratti dalla distribuzione a posteriori.
\end{itemize}

L'aspetto cruciale di questa discussione è il fatto che, all'aumentare delle ripetizioni dell'algoritmo di Metropolis, la distribuzione dei valori prodotti tende sempre di più alla distribuzione a posteriori del parametro $\theta$, anche se questa è sconosciuta.
Per un grande numero di passi della catena l'approssimazione è sufficiente.
Con questo metodo è dunque possibile generare un grande numero di campioni casuali dalla distribuzione a posteriori per poi poterne calcolare misure di sintesi e potere fare inferenza.

 
%\subsection{Un esempio pratico}
%
%Consideriamo nuovamente i dati di \citet{zetsche_future_2019} e poniamoci il problema di trovare, mediante l'algoritmo di Metropolis, la distribuzione a posteriori del parametro $\mu$ di una distribuzione Normale. 
%A questo scopo, utilizzeremo i 30 valori BDI-II del campione clinico di \citet{zetsche_future_2019} e li chiameremo \texttt{x}.
%Iniziamo a definire la funzione di verosimiglianza mediante le seguenti istruzioni.
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%likelihood <- function(param) {
%  sum(dnorm(x, param, true_sd, log = TRUE))
%}
%\end{lstlisting}
%\end{mdframed} 
%
%
%Per ciascun valore della variabile \texttt{param}, la funzione \texttt{likelihood()} determina l'ordinata della funzione di verosimiglianza. 
%Dobbiamo applicare tale funzione tenendo costanti i dati, il che significa, nel caso presente, che dobbiamo calcolare il valore della funzione di probabilità congiunta $p(\boldsymbol{x} \mid \mu, \sigma) = f(x_1) + f(x_2) + \dots f(x_{30})$, laddove $f(x)$ è la densità Gaussiana per il valore \texttt{x}, con parametri $\mu$ = \texttt{param} e $\sigma$ assunto noto e uguale alla deviazione standard del campione. 
%Dato che abbiamo specificato l'argomento \texttt{log = TRUE}, invece di calcolare semplicemente la densità Normale per ciascun valore \texttt{x}, prendiamo il logaritmo di tale valore di densità. 
%Per calcolare la funzione di probabilità congiunta $p(\boldsymbol{x} \mid \mu, \sigma)$ è necessario fare il prodotto dei valori di densità di ciascuna osservazione (dato che le osservazioni vengono assunte essere indipendenti). 
%Tuttavia, dato che vogliamo calcolare la log-verosimiglianza, avendo trasformato i valori di densità in logaritmi, è sufficiente sommare. 
%Infatti la funzione \texttt{likelihood} ritorna la somma del logaritmo dei 30 valori di densità che sono stati calcolati, per uno specifico valore del parametro $\mu$ = \texttt{param}. 
%
%La funzione di verosimiglianza così definita deve essere combinata con una distribuzione a priori per il parametro $\mu$.
%L'uso di una appropriata distribuzione a priori produce un effetto di regolarizzazione (\emph{smoothing}) dei dati.  
%Ovvero, riduce l'effetto di distorsione sulla stima del parametro derivante dalla (possibile) presenza di osservazioni anomale. 
%Inoltre, se abbiamo ragioni forti per avere delle aspettative rispetto al valore possibile della nostra stima, una distribuzione a priori \emph{informativa} verrà combinata con le informazioni fornite dal campione per produrre una stima \enquote{razionale} a posteriori.
%
%Nel nostro caso, decidiamo di scegliere, per fare un esempio, una distribuzione a priori centrata sul valore 9.5, ovvero sul valore BDI-II che rappresenta il limite superiore della categoria di \enquote{depressione minima}. 
%Scegliamo una distribuzione Normale con una deviazione standard sufficientemente grande così da coprire tutta la regione di interesse dei valori del parametro, dando più peso però ai valori di depressione prossimi a 9.5.
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%prior <- function(param) {
%  dnorm(param, mean = 9.5, sd = 30, log = TRUE)
%}
%\end{lstlisting}
%\end{mdframed} 
%
%Si noti che, anche in questo caso, abbiamo espresso la distribuzione a priori su una scala logaritmica.
%
%La funzione a posteriori si ottiene dal prodotto della densità a priori e della verosimiglianza. 
%Dato che stiamo usando un scala logaritmica, dobbiamo semplicemente sommare.
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%posterior <- function(param) {
%  likelihood(param) + prior(param)
%}
%\end{lstlisting}
%\end{mdframed} 
%
%
%La distribuzione proposta che viene utilizzata in questa implementazione dell'algoritmo di Metropolis sarà una Normale. 
%Il valore proposto da tale distribuzione ausiliaria corrisponde ad un valore selezionato a caso da una distribuzione Normale con media uguale al valore del parametro attualmente considerato nella catena e con una deviazione standard \enquote{adeguata}. 
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%proposal_distribution <- function(param) {
%  rnorm(1, mean = param, sd = 5)
%}
%\end{lstlisting}
%\end{mdframed} 
%
%In questo esercizio, la deviazione standard è stata scelta empiricamente in modo tale da ottenere un tasso di accettazione sensato. 
%È stato mostrato che un tasso di accettazione ottimale dovrebbe essere tra il 20\% e il 30\%. 
%Se il tasso di accettazione è troppo grande, infatti, l'algoritmo esplora uno spazio troppo ristretto della distribuzione a posteriori. 
%Il tasso di accettazione è influenzato dalla distribuzione proposta: in generale, tanto più la distribuzione proposta è simile alla distribuzione target, tanto più alto diventa il tasso di accettazione.
%
%Possiamo adesso implementare l'algoritmo di Metropolis:
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%run_metropolis_MCMC <- function(startvalue, iterations) {
%  chain <- vector(length = iterations + 1)
%  chain[1] <- startvalue
%  for (i in 1:iterations) {
%    proposal <- proposal_distribution(chain[i])
%    probab <- exp(posterior(proposal) - posterior(chain[i]))
%    if (runif(1) < probab) {
%      chain[i + 1] <- proposal
%    } else {
%      chain[i + 1] <- chain[i]
%    }
%  }
%  chain
%}
%\end{lstlisting}
%\end{mdframed} 
%
%
%L'algoritmo di Metropolis si comporta come il turista viaggiatore.  
%Confronta il valore corrente del parametro (la grandezza della spiaggia del giorno prima) con il valore proposto del parametro (la spiaggia su cui il turista può spostarsi).
%Se la densità della distribuzione proposta è maggiore della densità corrente, allora viene accettata (il turista si sposta sempre nella spiaggia più grande quando ne ha l'occasione).
%Se la densità della distribuzione proposta è minore della densità corrente, allora verrà accettata con una probabilità pari al rapporto tra la densità proposta e la densità corrente (questo garantisce al turista viaggiatore di campionare anche le altre spiagge e di impedire di limitarsi a restare sulla spiaggia più grande, una volta che l'ha raggiunta; questo \enquote{campionamento} era stato effettuato  dal turista viaggiatore mediante il giochino delle conchiglie e dei sassolini).
%
%L'aspetto importante di questo modo di procedere è il seguente: se la distribuzione proposta ha una forma simile alla distribuzione a posteriori, allora l'algoritmo di Metropolis farà in modo che la probabilità di accettare un valore proposto del parametro sarà proporzionale alla densità a posteriori.
%In altre parole, questo significa che i valori proposti accettati saranno un campione casuale della distribuzione a posteriori. 
%Si sceglie come distribuzione proposta la distribuzione Normale in quanto la distribuzione a posteriori ha solitamente una forma simile alla Normale. 
%
%Nello specifico, nella funzione \verb+run_metropolis_MCMC()+, la prima riga definisce un vettore, chiamato \texttt{chain}, dove verranno salvati i valori del parametro che saranno campionati dalla distribuzione a posteriori.
%La seconda riga inizializza tale vettore utilizzando un valore iniziale che verrà fornito in seguito. 
%Il ciclo \texttt{for ()} viene utilizzato per specificare i valori della passeggiata aleatoria (\emph{random walk}) che definisce l'insieme di punti che vengono campionati dalla distribuzione a posteriori. 
%L'algoritmo procede nel modo seguente.
%\begin{itemize}
%\item Si inizia scegliendo un valore a caso del parametro $\mu$, \verb+start_value+.
%\item Si sceglie a caso un altro valore, simile a quello attualmente in esame, prendendolo a caso dalla \enquote{distribuzione proposta} -- nell'algoritmo, la distribuzione proposta è una Normale centrata sul valore corrente della catena. 
%\item Si calcola il rapporto $\frac{p(new)}{p(old)}$, laddove la funzione $p(\cdot)$ è la funzione a posteriori, \texttt{new} è il valore proposto e \texttt{old} è il valore attuale della catena. 
%Dato che le funzioni \texttt{likelihood} e \texttt{posterior} hanno trasformato i valori su scala logaritmica, anziché un rapporto calcoliamo una differenza. 
%Inoltre, dato che vogliamo calcolare un rapporto tra probabilità, e siamo su una scala logaritmica, esponenziamo il risultato ottenuto dalla differenza in modo tale da ritornare sulla scala delle probabilità.
%\item A questo punto dobbiamo decidere se mantenere il valore corrente del parametro (\texttt{old}) o se accettare il valore proposto (\texttt{new}). 
%Le istruzioni condizionali \texttt{if-else} fanno in modo che venga accettato il valore \texttt{new} con probabilità $\frac{p(new)}{p(old)}$. 
%Come nel caso del turista viaggiatore, se $p(new) > p(old)$, si accetta sempre il valore proposto del parametro. 
%Questo significa che si accettano sempre valori del parametro che sono più vicini al massimo della funzione a posteriori rispetto al valore corrente (assumiamo qui che la distribuzione a posteriori sia unimodale). 
%Questo comportamento dipende dalla linea di codice specificata da \texttt{else}. Altrimenti, si accetta il valore proposto con probabilità $\frac{p(new)}{p(old)}$. 
%Infatti, ogni volta che viene eseguita, la condizione \texttt{(runif(1) < probab)} viene estratto un numero casuale compreso tra 0 e 1 e viene confrontato con \texttt{probab}. 
%Se il numero casuale è minore di \texttt{probab}, la condizione viene verificata e la proposta viene accettata. 
%Se \texttt{probab} è un numero vicino allo zero, la probabilità di estrarre un numero casuale (compreso tra 0 e 1) minore di \texttt{probab} è ovviamente molto piccola; il contrario accade se \texttt{probab} è un numero vicino ad 1. 
%In questo modo, il valore proposto viene accettato con una probabilità uguale al rapporto $\frac{p(new)}{p(old)}$. 
%\end{itemize}
%
%Lanciamo ora la simulazione.
%Con le istruzioni seguenti specifichiamo una catena di \num{10000} passi.
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%startvalue <- runif(1, 0, 100)
%niter <- 1e4
%chain <- run_metropolis_MCMC(startvalue, niter)
%\end{lstlisting}
%\end{mdframed} 
%
%
%Le istruzioni successive specificano che la prima metà dei valori proposti accettati saranno scartati, in quanto potrebbero essere influenzati dal valore iniziale.
%
%
%
%\begin{mdframed}
%\begin{lstlisting}
%burnIn <- niter / 2
%acceptance <- 1 - mean(duplicated(chain[-(1:burnIn)]))
%acceptance
%\end{lstlisting}
%\end{mdframed} 
%
%
%La funzione \texttt{duplicate()} nel blocco di codice precedente individua il numero di righe duplicate, ovvero il numero di casi nei quali la proposta non è stata accettata. 
%Otteniamo in questo modo un livello di accettazione del 29\% -- come abbiamo detto sopra, questo livello è buono.
%La figura~\ref{fig:metropolis_bdi} presenta una descrizione della distribuzione a posteriori così ottenuta.
%
%\begin{figure}[h!]
% \centering
% \includegraphics[width=0.74\textwidth, angle =270]{metropolis_bdi}
% \caption{Pannello in alto a sinistra: Istogramma per i valori campionati dalla distribuzione a posteriori. 
% Pannello in alto a destra: risultato grafico dopo il periodo di burn-in scartato per il parametro $\mu$.
%Pannello in basso a sinistra: funzione di verosimiglianza. Pannello in basso a destra: funzione a posteriori per il parametro $\mu$ non normalizzata.}
% \label{fig:metropolis_bdi}
% \end{figure}
%
%Si noti un punto importante: l'algoritmo di Metropolis-Hastings funziona bene anche se la distribuzione proposta è molto diversa dalla distribuzione target.
%Nella figura~\ref{fig:metropolis_proposal_post} l'istogramma indica, per gli stessi dati della figura~\ref{fig:metropolis_bdi}, una distribuzione proposta lontana dalla distribuzione target.
%Ciò nonostante, la moda della distribuzione a posteriori (curva blu) continua ad essere nell'intorno del valore di massima verosimiglianza (ovvero, la media del campione) indicato dalla linea tratteggiata grigia.
%
%\begin{figure}[h!]
% \centering
% \includegraphics[width=\textwidth]{metropolis_proposal_post}
% \caption{L'istogramma rappresenta una distribuzione proposta lontana dalla distribuzione target. La linea blu è la funzione a posteriori individuata dall'algoritmo di Metropolis-Hasting. La linea verticale trateggiata grigia individua la stima di massima verosimiglianza.}
% \label{fig:metropolis_proposal_post}
% \end{figure}
%
%
%%--------------------------------------------------------------------
%\subsection{Passeggiate aleatorie}
%\label{sec:passeggiate_aleatorie}
%
%Il primo riferimento alla simulazione Monte Carlo con catena di Markov (MCMC) si trova nella letteratura fisica. 
%\citet{metropolis_ulam_1949} e \citet{metropolist_etal_1953} descrivono ciò che è noto come l'algoritmo di Metropolis. 
%Tale algoritmo costituisce il fondamento dei metodi MCMC. 
%\citet{hastings_1970} generalizzò il loro lavoro producendo l'algoritmo Metropolis-Hastings. 
%In seguito, \citet{geman_geman_1984} si sono occupati dell'analisi di immagini usando quello che ora viene chiamato il campionamento di Gibbs (\emph{Gibbs sampling}). 
%La tecnica di campionamento che attualmente si è dimostrata la più efficiente è chiamata Hamiltonian (o Hybrid) Monte-Carlo (HMC) ed è implementata nel linguaggio Stan che può essere utilizzato con \R.
%
%
%%%--------------------------------------------------------------------
%%\subsection{Passeggiate aleatorie}
%%\label{sec:passeggiate_aleatorie}
%%
%%%I metodi MCMC riguardano l'uso di tecniche di campionamento casuale e di simulazione al computer per stimare la distribuzione a posteriori $p(\theta \mid x)$. 
%%%L'inferenza probabilistica attraverso catene di Markov consiste nel costruire sequenze di punti (le ``catene'') nello spazio dei parametri, la cui densità è proporzionale alla distribuzione di probabilità a posteriori a cui siamo interessati. 
%%%Il termine Monte-Carlo si riferisce al fatto che per la computazione si ricorre ad un ripetuto campionamento casuale (attraverso la generazioni di sequenze di numeri casuali). 
%%%Una catena di Markov è una passeggiata aleatoria (\emph{random walk}), ovvero una sequenza di variabili aleatorie $\theta^1, \theta^2, \dots$ nella quale la variabile aleatoria $\theta^t$ dipende solo dal suo immediato predecessore $\theta^{t-1}$. 
%%%Si può pensare ad una catena di Markov applicata al campionamento come ad un meccanismo che attraversa casualmente una distribuzione target senza avere memoria di dove sia stato in precedenza: il passaggio ad uno stato del sistema dipende unicamente dallo stato immediatamente precedente e non dal come si è giunti a tale stato (dalla storia). 
%%
%%Il primo riferimento alla simulazione Monte Carlo con catena di Markov (MCMC) si trova nella letteratura fisica. 
%%Metropolis e Ulam (1949) e Metropolis et al. (1953) descrivono ciò che è noto come l'algoritmo di Metropolis. 
%%Tale algoritmo costituisce il fondamento dei metodi MCMC. 
%%Hastings (1970) generalizzò il loro lavoro producendo l'algoritmo Metropolis-Hastings. 
%%In seguito, Geman e Geman (1984) si sono occupati dell'analisi di immagini usando quello che ora viene chiamato il campionamento di Gibbs (o campionatore di Gibbs, \emph{Gibbs sampling}). 
%%La tecnica di campionamento che attualmente si è dimostrata la più efficiente è chiamata Hamiltonian (o Hybrid) Monte-Carlo (HMC).
%%
%%Il metodo MCMC costituisce il fondamento del moderno calcolo bayesiano. 
%%Solo per i  modelli bayesiani più semplici è possibile ottenere in forma analitica la distribuzione a posteriori. 
%%Anche nei modelli moderatamente complessi le densità a posteriori sono troppo difficili da stabilire per via analitica. 
%%Con il metodo MCMC è invece possibile generare campioni da una qualunque densità a posteriori $p(\theta \mid x)$ e usare tali campioni per approssimare il valore atteso delle quantità di interesse. 
%%Diversi altri aspetti del metodo MCMC hanno contribuito al suo successo. 
%%Se l'algoritmo di simulazione è implementato correttamente viene garantita la convergenza alla distribuzione target $p(\theta \mid x)$ sotto un ampio spettro di condizioni, indipendentemente da dove è stata inizializzata la catena. 
%%In altre parole, una catena di Markov è in grado di migliorare la sua approssimazione alla vera distribuzione $p(\theta \mid x)$ a ogni iterazione della simulazione. 
%%Inoltre, se la catena di Markov viene eseguita per un tempo molto lungo (il che è spesso richiesto), è possibile stimare $p(\theta \mid x)$ a qualunque livello di precisione. 
%%Inoltre, l'algoritmo di simulazione è facilmente estensibile a modelli con un elevato numero di parametri o ad alta complessità, sebbene la ``maledizione della dimensionalità'' (\emph{curse of dimensionality}) spesso causi problemi nella pratica.\Hair\footnote{La maledizione della dimensionalità, o ``esplosione combinatoriale'', si riferisce al fatto che, quando la dimensionalità del problema aumenta, i dati diventano sparsi nello spazio decisionale. 
%%In altri termini, diventa necessario prendere in considerazione un numero così elevato di elementi da rendere il problema di trovare una soluzione ottimale eccedente la potenza computazionale a disposizione oppure risolubile in un tempo che aumenta in modo esponenziale.}


%--------------------------------------------------------------------
\section*{Conclusioni}
\label{sec:pregi_inferenza_bayes}

%Pregi dell'inferenza bayesiana

Lo scopo di questa discussione è stato quello di mostrare come sia possibile combinare le nostre conoscenze a priori (espresse nei termini di una densità di probabilità) con le evidenze fornite dai dati (espresse nei termini della funzione di verosimiglianza), così da determinare, mediante il teorema di Bayes, una distribuzione a posteriori, la quale condensa l'incertezza che si ha sul parametro $\theta$. 
Per illustrare tale problema, nel caso più semplice abbiamo considerato una situazione nella quale $\theta$ corrisponde alla probabilità di successo in una sequenza di prove Bernoulliane. 
Abbiamo visto come, in queste circostanze, è ragionevole esprimere le nostre credenze a priori mediante la densità Beta, con opportuni parametri. 
L'inferenza rispetto ad una proporzione rappresenta un caso particolare, ovvero un caso nel quale la distribuzione a priori è Beta e la verosimiglianza è Binomiale. 
In tali circostanze, anche la distribuzione a posteriori sarà una distribuzione Beta. 
Per questa ragione, in questo caso specifico, i parametri della distribuzione a posteriori possono essere determinati analiticamente (la soluzione richiede una serie di passaggi algebrici che qui non vengono discussi). 
In generale, però, tale approccio non è perseguibile.

La determinazione della distribuzione a posteriori richiede il calcolo della funzione di verosimiglianza e dell'integrale che si trova al denominatore del rapporto di Bayes. 
Nel caso di parametri continui, però, spesso tale integrale può essere impossibile da risolvere analiticamente. 
In passato, tale difficoltà è stata affrontata limitando l'analisi statistica al caso di funzioni di verosimiglianza semplici, le quali possono essere combinate con distribuzioni a priori coniugate per la verosimiglianza, così da produrre un integrale trattabile. 

Invece di approcci matematici analitici, un'altra classe di metodi fa ricorso all'approssimazione numerica dell'integrale. 
Tale approssimazione numerica dipende dall'uso di metodi MCMC, ovvero dipende dall'uso di una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla grande potenza di calcolo dei moderni computer. 
% In generale, tuttavia, un'esatta soluzione matematica al problema della determinazione della distribuzione a posteriori può essere raramente trovata quando vengono considerate situazioni più complesse. In tali circostanze, la possibilità di descrivere la distribuzione a posteriori dipende dall'uso di metodi Monte Carlo basati su Catena di Markov (MCMC), ovvero dipende dall'uso di una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla grande potenza di calcolo dei moderni computer. Per un'approfondimento delle tecniche MCMC si rimanda a testi specializzati come, ad esempio, Kruschke (2015) e McElreath (2015).
Lo sviluppo di software che rendono sempre più semplice l'uso dei metodi MCMC, insieme all'incremento della potenza di calcolo dei computer, ha contribuito a rendere sempre più popolare il metodo dell'inferenza Bayesiana che, in questo modo, può essere estesa a problemi di qualunque grado di complessità.



 