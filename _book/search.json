[{"path":"index.html","id":"benvenuti","chapter":"Benvenuti","heading":"Benvenuti","text":"Questo WORK--PROGRESS è stato aggiornato il giorno: 18 Gen 2021.Le presenti dispense contengono il materiale delle lezioni dell’insegnamento Psicometria B000286 (.. 2020/2021) rivolto agli studenti del primo anno del Corso di Laurea Scienze e Tecniche Psicologiche dell’Università di Firenze.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"una-breve-introduzione-al-modello-di-regressione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"Capitolo 1 Una breve introduzione al modello di regressione","text":"questo capitolo verrà presentata un’introduzione “pratica” ’analisi della regressione, nella quale ci preoccuperemo di capire cosa serve come si interpretano risultati che tale metodo statistico produce. Nel capitolo successivo, gli stessi argomenti verranno trattati un modo più “formale” e con maggiori approfondimenti teorici. Questo capitolo contiene tutto quello che c’è da sapere e non si può non sapere su questo argomento. L’ho pensato per miei laureandi, ovvero per degli studenti che devono usare queste procedure statistiche per risolvere un problema pratico (quello di concludere la tesi). L’altro capitolo è più convenzionalmente “didattico” ed è stato pensato primo luogo per chi deve dare l’esame di Psicometria. Questo primo capitolo su questo tema può essere dunque pensato come un’introduzione “gentile” ciò che verrà discusso nel prossimo capitolo.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"regressione-bivariata","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.1 Regressione bivariata","text":"La regressione bivariata si pone il problema di descrivere la relazione statistica lineare che intercorre tra due variabili, \\(x\\) e \\(y\\). Per relazione “statistica” intendo dire che, nel caso dei dati campionari \\(\\{x, y\\}\\) di cui si occupa il modello di regressione, non c’è mai una “perfetta” relazione lineare (ovvero, punti del diagramma dispersione di \\(\\{x, y\\}\\) non si situano su una retta). alcuni casi, quando guardiamo il diagramma dispersione di \\(\\{x, y\\}\\) ci rendiamo conto che, effetti, punti \\(\\{x, y\\}\\), anche se non si dispongono su una retta, sono sparpagliati attorno ad una retta “virtuale” che passa attraverso la nube di punti. una tale situazione (che è una delle tante possibili, non l’unica), è ragionevole descrivere la relazione tra le variabili \\(x\\) e \\(y\\) mediante la retta che, al meglio, approssima la nube di punti nel diagramma disperiosne. L’analisi di regressione si pone il problema di trovare l’inclinazione di quella retta che passa il più vicino possibile ai punti del diagramma dispersione di \\(\\{x, y\\}\\).","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"scioglimento-del-ghiaccio-marino","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.1.1 Scioglimento del ghiaccio marino","text":"Uno degli impatti più importanti dei cambiamenti climatici che stanno investendo il nostro Pianeta è la riduzione dell’estensione della calotta di ghiaccio marino artico. Esploriamo come l’estensione del ghiaccio marino artico sta cambiando nel tempo utilizzando un modello lineare. dati sono forniti da National Snow Ice Data Center e sono espressi milioni di chilometri quadrati.dati sono seguenti:Quale domanda di ricerca possiamo porci con questi dati? Propongo la seguente domanda.Domanda di ricerca: l’estensione del ghiaccio marino artico sta diminuendo nel tempo?Per esplorare la risposta questa domanda, iniziamo creare una rappresentazione grafica dei dati. Dato che abbiamo due variabi continue (il tempo, espresso anni, e l’estensione del ghiaccio marino artico, milioni di chilometri quadrati), questi dati possono essere rappresentati graficamente mediante un diagramma dispersione.\nVogliamo sapere come varia l’estensione del ghiaccio marino artico funzione del tempo e quindi disegnamo dati ponendo la variabile tempo sull’asse delle ascisse e l’estensione del ghiaccio marino artico sull’asse delle ordinate.\nGuardando la figura vediamo che è ragionevole descrivere mediante una retta la relazione tra l’estensione del ghiaccio marino artico (chiamiamola y) e il tempo (chiamiamolo x). Aggiungiamo al diagramma di dispersione una retta, scegliendola modo tale che si avvicini il più possibile alla nube di punti rappresentata nel grafico. Ovviamente, è possibile scegliere tra infinite rette diverse. La retta che è rappresentata qui è stata scelta base ad un criterio particolare, detto “dei minimi quadrati.” Vedremo meglio seguito cosa questo significa. Per ora ci accontentiamo di riconoscere che la nostra è una buona scelta, per gli scopi presenti.","code":"\ndata.frame(seaice)\n#>    year extent_north extent_south\n#> 1  1979       12.328       11.700\n#> 2  1980       12.337       11.230\n#> 3  1981       12.127       11.435\n#> 4  1982       12.447       11.640\n#> 5  1983       12.332       11.389\n#> 6  1984       11.910       11.454\n#> 7  1985       11.995       11.618\n#> 8  1986       12.203       11.088\n#> 9  1987       12.135       11.554\n#> 10 1988       11.923       12.131\n#> 11 1989       11.967       11.426\n#> 12 1990       11.694       11.410\n#> 13 1991       11.749       11.545\n#> 14 1992       12.110       11.399\n#> 15 1993       11.923       11.420\n#> 16 1994       12.011       11.774\n#> 17 1995       11.415       11.795\n#> 18 1996       11.841       11.769\n#> 19 1997       11.668       11.390\n#> 20 1998       11.757       11.738\n#> 21 1999       11.691       11.761\n#> 22 2000       11.508       11.747\n#> 23 2001       11.600       11.673\n#> 24 2002       11.363       11.222\n#> 25 2003       11.397       11.969\n#> 26 2004       11.240       11.961\n#> 27 2005       10.907       11.695\n#> 28 2006       10.773       11.461\n#> 29 2007       10.474       11.687\n#> 30 2008       10.978       12.239\n#> 31 2009       10.932       12.049\n#> 32 2010       10.711       12.107\n#> 33 2011       10.483       11.501\n#> 34 2012       10.406       12.004\n#> 35 2013       10.897       12.524\n#> 36 2014       10.790       12.776\n#> 37 2015       10.566       12.414\n#> 38 2016       10.151       11.156\n#> 39 2017       10.373       10.693\nseaice %>% \n  ggplot(aes(year, extent_north)) +\n  geom_point()\nseaice %>% \n  ggplot(aes(year, extent_north)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interpretazione-dei-coefficienti-a-e-b","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.2 Interpretazione dei coefficienti \\(a\\) e \\(b\\)","text":"La retta che abbiamo disegnato nella figura precedente rappresenta la risposta alla nostra domanda di ricerca: l’estensione del ghiaccio marino artico sta diminuendo nel tempo.Anziché considerare questa risposta unicamente dal punto di vista grafico, proviamo descrivere la retta disegnata nella figura maniera quantitativa, con dei numeri. Per fare questo, dobbiamo innanzitutto ricordare qual è l’equazione di una retta:\\[\\begin{equation}\ny = + b \\times x\n\\end{equation}\\]Prima di calcolare coefficienti \\(\\) e \\(b\\) della retta di regressione, rimaneggiamo nostri dati. particolare, rinominiamo le variabili e indicizziamo gli anni da 1 39. Nel caso presente, vogliamo sapere se l’estensione del ghiaccio marino artico dall’inizio alla fine del periodo temporale considerato, indipendentemente dal fatto che l’anno iniziale sia il 1979 e l’anno finale il 2017. Quindi sottraiamo 1979 dalle modalità della variabile year modo tale che il primo punto temporale corrisponda zero.Il diagramma dispersione avrà ora la forma seguente:Per trovare coefficienti \\(\\) e \\(b\\) della retta di regressione possiamo usare, ad esempio, la funzione lm():L’output della funzione lm() ci dice che è uguale 12.501 e che b è uguale -0.055. Ma che significato (geometrico) hanno questi valori?Ai coefficienti \\(\\) e \\(b\\) possiamo assegnare la seguente interpretazione:il coefficiente \\(\\) rappresenta il valore della coordinata \\(y\\) (l’estensione del ghiaccio marino artico) della retta di regressione quando la coordinata \\(x\\) vale zero (nel nostro caso, l’anno 1979) – altre parole, corrisponde al punto dove la retta di regressione interseca l’asse \\(y\\) del sistema di assi cartesiani;il coefficiente \\(\\) rappresenta il valore della coordinata \\(y\\) (l’estensione del ghiaccio marino artico) della retta di regressione quando la coordinata \\(x\\) vale zero (nel nostro caso, l’anno 1979) – altre parole, corrisponde al punto dove la retta di regressione interseca l’asse \\(y\\) del sistema di assi cartesiani;il coefficiente \\(b\\) ci dice di quanto aumenta la coordinata \\(y\\) della retta di regressione, quando \\(x\\) aumenta di un’unità.il coefficiente \\(b\\) ci dice di quanto aumenta la coordinata \\(y\\) della retta di regressione, quando \\(x\\) aumenta di un’unità.Il valore \\(\\) = 12.501 significa che, nel 1979, l’estensione del ghiaccio marino artico era pari 12.501 milioni di chilometri quadrati, dato che la modalità x = 0 della variabile indipendente corrisponde ’anno 1979.Il segno di \\(b\\) è negativo; questo significa che l’estensione del ghiaccio marino artico sta diminuendo nel corso del tempo. Il valore -0.055 ci dice che, per ogni anno che passa (nel periodo dal 1979 al 2017), l’estensione del ghiaccio marino artico diminuisce, media, di -0.055 milioni di chilometri quadrati.Se guardiamo la figura, infatti, vediamo che, se ci sposiamo da \\(x = 10\\) \\(x = 20\\) (ovvero, di dieci anni), la coordinata \\(y\\) della retta diminuisce di 10 volte \\(b\\), ovvero di -0.55 milioni di chilometri quadrati. Questo è indicato nella figura qui sotto.","code":"\nseaice <- seaice %>% \n  mutate(\n    x = year - 1979\n  ) %>% \n  rename(\n    y = extent_north\n  )\nglimpse(seaice)\n#> Rows: 39\n#> Columns: 4\n#> $ year         <int> 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 19…\n#> $ y            <dbl> 12.328, 12.337, 12.127, 12.447, 12.332, 11.910, 11.995, …\n#> $ extent_south <dbl> 11.700, 11.230, 11.435, 11.640, 11.389, 11.454, 11.618, …\n#> $ x            <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\nseaice %>% \n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'\nfm <- lm(y ~ x, data = seaice)\ncoef(fm)\n#> (Intercept)           x \n#> 12.50131410 -0.05457389\ndplot <- data.frame(\n  x1 = 10,\n  x2 = 20,\n  y1 = 12.50131410 + -0.05457389 * 10,\n  y2 = 12.50131410 + -0.05457389 * 20\n)\n\nseaice %>%\n  ggplot(aes(x, y)) +\n  geom_point(color=\"gray\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(x = x2, y = y1, xend = x2, yend = y2),\n               arrow=arrow(), size=1.1, data = dplot) +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y1),\n               arrow=arrow(), size=1.1, data = dplot) +\n  annotate(\"text\", x = 24.0, y = 11.75, label = \"10 b = -0.55\") +\n  annotate(\"text\", x = 15, y = 12.15, label = \"delta x = 10.0\")\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"scomposizione-della-y","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.3 Scomposizione della \\(y\\)","text":"Uno degli aspetti importanti della regressione lineare è che, pratica, la retta di regressione scompone ciascun punteggio \\(y\\) due componenti:\\[\\begin{equation}\ny = (+ b \\times x) + e\n\\end{equation}\\]laddove \\(\\hat{y} = + b \\times x\\) è la componente di \\(y\\) che è linearmente predicibile conoscendo \\(x\\), mentre la componente residua, \\(e\\), è la componente di \\(y\\) che non è linearmente predicibile conoscendo \\(x\\).Nei nostri dati, questi significa quanto segue. Esaminiamo le prime 5 osservazioni del nostro campione:Aggiungo al data.frame una colonna che rappresenta valori \\(y\\) predetti dal modello lineare (ovvero, \\(\\hat{y} = + b \\times x\\): le coordinate \\(y\\) della retta di regressione per ciascuna delle osservazioni):Aggiungo ora al data.frame residui del modello di regressione. residui sono definiti come\\[\\begin{equation}\ne = y - (+ b \\times x) = y - \\hat{y}\n\\end{equation}\\]ovveroCiò che volevamo dimostrare è che la somma di \\(\\hat{y}\\) e dei residui è uguale alla \\(y\\). Infatti:altre parole, il modello di regressone scompone la \\(y\\) due componenti: la porzione della \\(y\\) che possiamo prevedere conoscendo \\(x\\) (ovvero, \\(\\hat{y} = + b \\times x\\)) e la porzione della \\(y\\) che non possiamo prevedere sulla base di \\(x\\).","code":"\nseaice %>% \n  dplyr::select(x, y) %>% \n  top_n(5) \n#> Selecting by y\n#>   x      y\n#> 1 0 12.328\n#> 2 1 12.337\n#> 3 3 12.447\n#> 4 4 12.332\n#> 5 7 12.203\nseaice$yhat <- coef(fm)[1] + coef(fm)[2] * seaice$x\nseaice %>% \n  dplyr::select(x, y, yhat) %>% \n  top_n(5) \n#> Selecting by yhat\n#>   x      y     yhat\n#> 1 0 12.328 12.50131\n#> 2 1 12.337 12.44674\n#> 3 2 12.127 12.39217\n#> 4 3 12.447 12.33759\n#> 5 4 12.332 12.28302\nseaice$e <- seaice$y - (coef(fm)[1] + coef(fm)[2] * seaice$x)\nseaice %>% \n  dplyr::select(x, y, yhat, e) %>% \n  top_n(5) \n#> Selecting by e\n#>    x      y     yhat         e\n#> 1 13 12.110 11.79185 0.3181464\n#> 2 15 12.011 11.68271 0.3282942\n#> 3 19 11.757 11.46441 0.2925897\n#> 4 20 11.691 11.40984 0.2811636\n#> 5 22 11.600 11.30069 0.2993114\nseaice$sum <- seaice$yhat + seaice$e\nseaice %>% \n  dplyr::select(x, y, yhat, e, sum) %>% \n  top_n(5) \n#> Selecting by sum\n#>   x      y     yhat           e    sum\n#> 1 0 12.328 12.50131 -0.17331410 12.328\n#> 2 1 12.337 12.44674 -0.10974022 12.337\n#> 3 3 12.447 12.33759  0.10940756 12.447\n#> 4 4 12.332 12.28302  0.04898144 12.332\n#> 5 7 12.203 12.11930  0.08370310 12.203"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"metodo-di-stima-dei-coefficienti-del-modello-di-regressione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.3.1 Metodo di stima dei coefficienti del modello di regressione","text":"Come abbiamo fatto calcolare coefficienti dei minimi quadrati \\(\\) e \\(b\\)? Ci sono tanti metodi; il più comune si chiama “metodo dei minimi quadrati.” Lo esamineremo seguito. Per nostri scopi, è poco importante: per ora ci accontentiamo di chiedere ad R di fare calcoli per noi.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"inferenza-sul-modello-di-regressione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.4 Inferenza sul modello di regressione","text":"Finora siamo riusciti descrivere la relazione statistica tra \\(x\\) e \\(y\\) un campione di osservazioni. Siamo ben consapevoli che, un altro campione di osservazioni, la relazione statistica tra \\(x\\) e \\(y\\) non sarà identica quella osservata nel caso del primo campione – dato che dati saranno diversi. La domanda cruciale dunque diventa la seguente: quanto sono simili coefficienti dei minimi quadrati calcolati sui dati campionari ai coefficienti di un’ipotetica retta di regressione che, sempre mediante il metodo dei minimi quadrati, descriverebbe la relazione tra tutte le osservazioni \\(\\{x, y\\}\\) nella popolazione? Ciò che vogliamo è dunque una quantificazione della nostra incertezza: vogliamo sapere quanto dobbiamo fidarci delle stime campionarie come descrizioni dei valori (sconosciuti) della popolazione. Ci sono due possibilità.Se le stime di \\(\\) e \\(b\\) fornite dal particolare campione che abbiamo osservato sono simili ai valori teorici della popolazione, allora dati del campione ci forniscono informazioni utili per capire quali sono le proprietà della popolazione.Se le stime di \\(\\) e \\(b\\) fornite dal particolare campione che abbiamo osservato sono simili ai valori teorici della popolazione, allora dati del campione ci forniscono informazioni utili per capire quali sono le proprietà della popolazione.Se invece le stime campionarie di \\(\\) e \\(b\\) sono molto diverse dai valori teorici della popolazione, allora dati del campione non ci aiutano capire quali sono le proprietà della popolazione.Se invece le stime campionarie di \\(\\) e \\(b\\) sono molto diverse dai valori teorici della popolazione, allora dati del campione non ci aiutano capire quali sono le proprietà della popolazione.Il problema che abbiamo è quello di decidere quale di queste due situazioni ci troviamo: la prima o la seconda. Questo è il problema dell’inferenza statistica sul modello di regressione.Il problema dell’inferenza viene affrontato utilizzando gli strumenti della teoria della probabilità per costruire un intervallo di valori. Nell’approccio Bayesiano, tale intervallo di valori si chiama intervallo di credibilità. Se calcoliamo, ad esempio, l’intervallo di credibilità ’89% per il parametro \\(\\beta\\) (inclinazione della retta di regressione nella popolazione), interpretiamo tale intervallo di valori nel modo seguente: possiamo dire che “siamo sicuri ’89% che il vero valore di \\(\\beta\\) è contenuto nell’intervallo stimato” – laddove per “vero valore di \\(\\beta\\)” intendiamo il valore del parametro sconosciuto della popolazione. Se calcoliamo l’intervallo di credibilità, ad un dato livello di certezza, possiamo giungere una di tre possibili conclusioni.L’intervallo di credibilità non include lo 0 e il suo limite inferiore è positivo: possiamo concludere, con una certezza dell’89%, che c’è una relazione positiva tra \\(x\\) e \\(y\\) nella popolazione;l’intervallo di credibilità non include lo 0 e il suo limite superiore è negativo: possiamo concludere, con una certezza del’89%, che c’è una relazione negativa tra \\(x\\) e \\(y\\) nella popolazione;l’intervallo di credibilità include lo 0: con un un livello di certezza dell’89%, non possiamo negare la possibilità che nella popolazione la relazione tra \\(x\\) e \\(y\\) sia nulla – ovvero, non abbiamo sufficienti informazioni per sapere se è positiva o negativa.Ritorniamo ora ai nostri dati e calcoliamo, con un metodo che discuteremo seguito, l’intervallo di credibilità ’89%. Per ora non ci preoccupiamo della sintassi dei comandi R, né di cosa significano tali istruzioni. Ci preoccupiamo solo di ottenere le stime dei coefficienti della retta di regressione e gli intervalli di credibilità:L’intervallo di credibilità per \\(b\\) così ottenuto, ovvero [-0.059, -0.049], non include lo zero. Possiamo dunque concludere, con un livello di certezza dell’89%, che nella popolazione c’è una relazione negativa tra \\(x\\) e \\(y\\). altre parole, siamo certi, con un livello di certezza dell’89%, che dal 1979 al 2017 l’estensione del ghiaccio marino artico è diminuita.Se vogliamo, possiamo anche ottenere una stima di \\(b\\) che corrisponde ad un livello di certezza più alto, il che porterà ad un aumento dell’ampiezza dell’interallo di credibilità. Stimiamo dunque l’interallo di credibilità ad un livello di certezza del 99%:Anche questo caso l’interallo di credibilità non include lo zero per cui, anche se pretendiamo un livello di certazza del 99%, confermiamo la conclusione secondo la quale dal 1979 al 2017 l’estensione del ghiaccio marino artico è diminuita.Per quel che riguarda \\(\\), con un livello di certezza dell’99% possiamo dire che il valore dell’intercetta della retta di regressione nella popolazione è incluso nell’intervallo [12.313, 12.661]. Nel caso presente, ciò ha una semplice interpretazione: significa che la nostra stima dell’esensione del ghiaccio marino artico nell’anno 1979 corrisponde un valore compreso tra [12.313, 12.661], con un livello di certezza del 99%.","code":"\nflist <- alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b*x,\n    c(a, b) ~ dnorm(0, 2), \n    sigma ~ dcauchy(0, 2)\n)\n\nfit <- quap(\n  flist, \n  data = list(y=seaice$y, x=seaice$x), \n  start = list(a=0, b=0, sigma=0.1)\n)\n\nout <- round(precis(fit), 3)\nout\n#>         mean    sd   5.5%  94.5%\n#> a     12.487 0.068 12.379 12.595\n#> b     -0.054 0.003 -0.059 -0.049\n#> sigma  0.215 0.024  0.176  0.254\nout1 <- round(precis(fit, prob=0.99), 3)\nout1\n#>         mean    sd   0.5%  99.5%\n#> a     12.487 0.068 12.313 12.661\n#> b     -0.054 0.003 -0.062 -0.046\n#> sigma  0.215 0.024  0.152  0.277"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"osservazione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"Osservazione","text":"Potremmo chiedersi perché sia necessario fare delle inferenze sul fenomeno della variazione dell’estensione del ghiaccio marino artico. Se dati ci dicono che l’estensione del ghiaccio marino artico è diminuita, che bisogno abbiamo di fare un’inferenza? La risposta sembra già essere contenuta nei dati.realtà, le cose sono un po’ più complicate. La misurazione dell’aspetto di interesse, questo caso, l’estensione del ghiaccio marino artico, non è priva di errori di misurazione. È ovvio che, se facessimo misurazioni ripetute nel corso dello stesso anno, otterremmo valori diversi. Inltre, al di là degli errori di misurazione, l’estensione del ghiaccio marino artico è soggetta variazioni stagionali e continue nel tempo e fluttuazioni stagionali.Quindi, dati del nostro campione non catturano perfettamente ciò che vogliamo sapere. Siamo infatti consapevoli del fatto che, se ripetessimo le nostre misurazioni, otterremmo valori diversi. Il problema che abbiamo è dunque il seguente: come facciamo rispondere alla domanda della ricerca avendo osservato solo uno degli infiniti campioni diversi di osservazioni che descrivono l’aspetto che ci interessa? Per quantificare la nostra incertezza associamo alla stima della quantità di interesse (questo caso, la variazione dell’estensione del ghiaccio marino artico nell’unità di tempo) un intervallo di credibilità calcolato utilizzando un determinato livello di certezza.Queste affermazioni sono certamente vere nel caso del ghiaccio marino dell’Artico. Ma un lettore attento si sarà anche reso conto del fatto che tale problema (l’errore di misurazione e la variabilità campionaria) è certamente presente anche quando di occupiamo della misurazione dei costrutti psicologici. Questa è la ragione per la quale, come psicologi, dobbiamo essere grado di quantificare la nostra incertezza quando stimiamo le caratteristiche generali di ciò che ci interessa, ovvero quando vogliamo descrivere ciò che ci interessa al di là degli aspetti idiosincratici del campione di dati che abbiamo osservato.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"errore-standard-della-stima","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.4.1 Errore standard della stima","text":"Il parametto sigma nell’output di precis() ci fornisce un altro utile pezzo di informazione: esso stima infatti quanto sono distanti, media, le osservazioni dalla retta di regressione nella popolazione. Il valore sigma, chiamato errore standard della stima, ci dice che il modello di regressione, stimando la relazione tra \\(y\\) e \\(x\\), compie un errore medio di 0.215 milioni di chilometri quadrati – laddove 0.215 è la stima della distanza media (milioni di chilometri quadrati) tra ciascuno dei punti del diagramma dispersione e la retta di regressione nella popolazione.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"errori-standard-delle-stime-a-e-b","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.4.2 Errori standard delle stime \\(a\\) e \\(b\\)","text":"Il valore sd associato \\(b\\) nell’output di precis() ci dice invece invece di quanto dobbiamo aspettarci che vari la stima di \\(b\\) da campione campione. altri termini, causa della variabilità campionaria, la stima di \\(b\\) assume un valore diverso ciascuno degli infinti campioni di \\(n\\) osservazioni che descrivono la relazione tra \\(x\\) e \\(y\\), nel periodo temporale considerato. Il valore sd uguale 0.003 ci dice che, media, la stima della pendenza della retta di regressione differirà dal vero valore di questo parametro di una quantità pari 0.003 milioni di chilometri quadrati.\nUna affermazione simile può essere fatta proposito del valore sd associato ad \\(\\) nell’output di precis().","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"una-variabile-indipendente-qualitativa","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.5 Una variabile indipendente qualitativa","text":"Il modello statistico della regressione bivariato che abbiamo descritto sopra è molto limitato: può solo descrivere la relazione lineare tra due variabili continue. Estendiamolo ora al caso cui, oltre ad includere un predittore continuo \\(x\\), il modello di regressione comprende anche una variabile che consente di suddividere le osservazioni del campione due gruppi.Per semplicità, simuliamo un insieme di dati che utilizzeremo nella seguente discussione.Esaminiamo il diagramma dispersione che indica chiaramente che ci sono due gruppi distinti di osservazioni:È ovvio, che nel caso presente, non è sufficiente un modello di regressione che ignora il fatto che dati appartengono due gruppi diversi. Infatti, se adattiamo ai dati un’unica retta di regressione, è facile rendersi conto che tale retta si situerà una posizione molto distante dai dati.Per questi dati è ovviamente più sensato adattare una diversa retta di regressione ciascun gruppo di osservazioni:Come possiamo modificare il modello di regressione che abbiamo esaminato precedenza modo tale da essere grado di stimare coefficienti di regressione delle due rette rappresentate nella figura precedente? Questo problema si risolve nel modo seguente. Crediamo una variabile, chiamata dummy, che ha valore 0 per un gruppo, diciamo gr1, e valore 1 per l’altro gruppo:Esaminiamo ora il modello di regressione che include tale variabile dummy, che chiameremo D:\\[\\begin{equation}\ny = \\alpha + \\beta x + \\gamma D + \\varepsilon.\n\\end{equation}\\]Quando \\(D = 0\\):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 0 + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quando \\(D = 1\\):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 1 + \\varepsilon,\\notag\\\\\n &= (\\alpha + \\gamma) + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quindi, se assumiamo che le due rette di regressione siano parallele (nella discussione precedente abbiamo previsto un unico valore \\(\\beta\\)), coefficienti del modello avranno la seguente interpretazione:\\(\\alpha\\) = intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\beta\\) = pendenza della retta di regressione per il gruppo codificato con D = 0;\\(\\gamma\\) = differenza tra l’intercetta della retta di regressione per il gruppo codificato con D = 1 e l’intercetta della retta di regressione per il gruppo codificato con D = 0.","code":"\nset.seed(1234)\nn <- 100\nx1 <- rnorm(n, 4, 1)\ny1 <- 5 + 5 * x1 + rnorm(n, 0, 2)\n\nx2 <- rnorm(n, 4, 1)\ny2 <- -3 + 2 * x2 + rnorm(n, 0, 2)\n\ny <- c(y1, y2)\nx <- c(x1, x2)\ngroup <- rep(c(\"gr1\", \"gr2\"), each = n)\n\nd <- data.frame(x, y, group) \n\nd %>%\n  group_by(group) %>% \n  do(head(., 5))\n#> # A tibble: 10 x 3\n#> # Groups:   group [2]\n#>       x     y group\n#>   <dbl> <dbl> <fct>\n#> 1  2.79 19.8  gr1  \n#> 2  4.28 25.4  gr1  \n#> 3  5.08 30.6  gr1  \n#> 4  1.65 12.3  gr1  \n#> 5  4.43 25.5  gr1  \n#> 6  4.49  4.81 gr2  \n#> # … with 4 more rows\nd %>%\n  ggplot(aes(x, y)) +\n  geom_point(aes(color = group))\nd %>%\n  ggplot(aes(x, y)) +\n  geom_point(aes(color = group)) +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'\nd %>%\n  ggplot(aes(x, y, color = group)) +\n  geom_point(aes(color = group)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\nd$gr <- ifelse(d$group == \"gr1\", 0, 1) \n\nd %>%\n  group_by(gr) %>% \n  do(head(., 5)) %>% \n  as.data.frame()\n#>           x         y group gr\n#> 1  2.792934 19.793718   gr1  0\n#> 2  4.277429 25.437709   gr1  0\n#> 3  5.084441 30.554193   gr1  0\n#> 4  1.654302 12.266556   gr1  0\n#> 5  4.429125 25.493626   gr1  0\n#> 6  4.485227  4.810540   gr2  1\n#> 7  4.696769  4.486980   gr2  1\n#> 8  4.185514  5.012171   gr2  1\n#> 9  4.700734  8.421083   gr2  1\n#> 10 4.311681  5.670615   gr2  1"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interazione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.5.1 Interazione","text":"generale, però, le due rette di regressione non sono parallele. Per potere rappresentare una tale possibilità, introduciamo nel data.frame d una seconda variabile e la chiamiamo DX. Creaimo DX facendo il prodotto delle variabili D e x:Si noti che, quando D = 0, allora DX = 0; quando D = 1, allora DX = x.Riscriviamo il modello di regressione nel modo seguente:\\[\\begin{equation}\ny = \\alpha + \\beta x + \\gamma D + \\zeta DX + \\varepsilon.\n\\end{equation}\\]tali circostanze, quando \\(D = 0\\), abbiamo che:\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\zeta DX  + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 0 + \\zeta \\times 0 + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quando \\(D = 1\\) (ricordiamo: questo caso \\(DX = x\\)):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\zeta DX + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 1 + \\zeta x + \\varepsilon,\\notag\\\\\n &= (\\alpha + \\gamma) + (\\beta + \\zeta) x + \\varepsilon.\\notag\n\\end{align}\\]Ciò significa che coefficienti del modello di regressione avranno ora la seguente interpretazione:\\(\\alpha\\) = intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\beta\\) = pendenza della retta di regressione per il gruppo codificato con D = 0;\\(\\gamma\\) = differenza tra l’intercetta della retta di regressione per il gruppo codificato con D = 1 e l’intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\zeta\\) = differenza tra la pendenza della retta di regressione per il gruppo codificato con D = 1 e la pendenza della retta di regressione per il gruppo codificato con D = 0;Ritorniamo al nostro esempio numerico e calcoliamo coefficienti del modello di regressione utilizzando l’approccio Bayesiano:un tale modello, la prima domanda che dobbiamo porci è se dati giustificano la conclusione secondo la quale, nella popolazione, le due rette hanno una pendenza diversa. Per rispondere tale domanda esaminiamo l’intervallo di credibilità del coefficiente associato al termine d’interazione, ovvero del coefficiente \\(\\zeta\\). R ci dice che l’intervallo di credibilità ’89% per il coefficiente \\(\\zeta\\) è pari [-3.36, -2.41]. Dato che tale intervallo non include lo zero, con un livello di certezza dell’89% concludiamo che la retta di regressione del gruppo codificato con D = 1 (ovvero gr2) ha una pendenza minore della retta di regressione per il gruppo codificato con D = 0 (ovvero gr1).","code":"\nd$DX <- d$gr * d$x\n\nd %>%\n  group_by(gr) %>% \n  do(head(., 5)) %>% \n  as.data.frame()\n#>           x         y group gr       DX\n#> 1  2.792934 19.793718   gr1  0 0.000000\n#> 2  4.277429 25.437709   gr1  0 0.000000\n#> 3  5.084441 30.554193   gr1  0 0.000000\n#> 4  1.654302 12.266556   gr1  0 0.000000\n#> 5  4.429125 25.493626   gr1  0 0.000000\n#> 6  4.485227  4.810540   gr2  1 4.485227\n#> 7  4.696769  4.486980   gr2  1 4.696769\n#> 8  4.185514  5.012171   gr2  1 4.185514\n#> 9  4.700734  8.421083   gr2  1 4.700734\n#> 10 4.311681  5.670615   gr2  1 4.311681\nflist1 <- alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b*x + gamma*d + zeta*DX,\n    c(a, b, gamma, zeta) ~ dnorm(0, 10), \n    sigma ~ dnorm(0, 5)\n)\nfit1 <- quap(\n  flist1, \n  data = list(y=d$y, x=d$x, d=d$gr, DX=d$DX), \n  start = list(a=0, b=0, gamma=0, zeta=0, sigma=0.5)\n  )\n\nprecis(fit1)\n#>            mean        sd       5.5%     94.5%\n#> a      5.202783 0.8167093   3.897524  6.508043\n#> b      4.967164 0.2057473   4.638340  5.295988\n#> gamma -8.569902 1.2250868 -10.527828 -6.611977\n#> zeta  -2.881817 0.2970176  -3.356509 -2.407126\n#> sigma  2.069882 0.1034130   1.904608  2.235155"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"regressione-multipla","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.6 Regressione multipla","text":"","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"assenza-di-interazione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.6.1 Assenza di interazione","text":"Esaminiamo ora dati epi.bfi contenuti nel pacchetto psychTools che contengono 231 osservazioni relative 5 scale dall’Eysenck Personality Inventory, 5 scale del Big 5, valori del Beck Depression Inventory e misure di ansia di stato e ansia di tratto.Supponiamo di chiederi se c’è una relazione tra la depressione (bdi), considerata quale variabile dipendente, e Neuroticismo e ansia di stato. Considereremo seguito il problema della possibile interazione tra Neuroticismo e ansia di stato. Per ora, scriviamo il modello di regressione nel modo seguente:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]Abbiamo visto precedenza come si interpretano coefficienti di regressione nel caso della regressione bivariata. coefficienti di regressione, detti parziali, che fanno parte del modello di regressione multipla hanno però un significato diverso da quello descritto precedenza. Poniamoci dunque il problema di capire qual è la differenza tra un coefficiente di regressione (nel modello bivariato) e un coefficiente parziale di regressione (nel modello di regressione multipla).precedenza abbiamo visto che il coefficiente di regressione \\(\\beta\\) ha il seguente significato: indica la variazione del valore atteso della \\(y\\) base al modello lineare nel caso di un cambiamento unitario della \\(x\\). Ma adesso questa spiegazione non basta, quanto nel modello di regressione multipla non c’è una sola variabile indipendente: nel caso considerato qui ce ne sono due. Dunque, qual è il significato di \\(\\beta_1\\) e di \\(\\beta_2\\)?","code":"\nsuppressMessages(library(\"sjPlot\"))\nsuppressMessages(library(\"sjmisc\"))\nlibrary(\"psychTools\")\ndata(epi.bfi)\nglimpse(epi.bfi)\n#> Rows: 231\n#> Columns: 13\n#> $ epiE     <int> 18, 16, 6, 12, 14, 6, 15, 18, 15, 8, 13, 14, 15, 19, 15, 11,…\n#> $ epiS     <int> 10, 8, 1, 6, 6, 4, 9, 9, 11, 5, 9, 12, 10, 11, 10, 5, 10, 11…\n#> $ epiImp   <int> 7, 5, 3, 4, 5, 2, 4, 7, 3, 2, 3, 3, 4, 7, 4, 6, 5, 6, 2, 4, …\n#> $ epilie   <int> 3, 1, 2, 3, 3, 5, 3, 2, 3, 2, 3, 6, 5, 0, 2, 7, 0, 4, 1, 4, …\n#> $ epiNeur  <int> 9, 12, 5, 15, 2, 15, 12, 10, 1, 10, 9, 1, 2, 3, 7, 13, 18, 1…\n#> $ bfagree  <int> 138, 101, 143, 104, 115, 110, 109, 92, 127, 74, 124, 131, 13…\n#> $ bfcon    <int> 96, 99, 118, 106, 102, 113, 58, 57, 108, 100, 114, 107, 114,…\n#> $ bfext    <int> 141, 107, 38, 64, 103, 61, 99, 94, 108, 61, 100, 99, 114, 13…\n#> $ bfneur   <int> 51, 116, 68, 114, 86, 54, 55, 72, 35, 87, 110, 92, 56, 47, 6…\n#> $ bfopen   <int> 138, 132, 90, 101, 118, 149, 110, 114, 86, 89, 129, 121, 131…\n#> $ bdi      <int> 1, 7, 4, 8, 8, 5, 7, 0, 0, 7, 8, 3, 0, 0, 1, 4, 14, 7, 9, 4,…\n#> $ traitanx <int> 24, 41, 37, 54, 39, 51, 40, 32, 22, 35, 43, 33, 23, 23, 27, …\n#> $ stateanx <int> 22, 40, 44, 40, 67, 38, 32, 41, 26, 31, 39, 25, 32, 23, 28, …"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"significato-dei-coefficienti-parziali-di-regressione","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.6.1.1 Significato dei coefficienti parziali di regressione","text":"La risposta è che un generico \\(\\beta_j\\) nel modello di regressione multipla rappresenta la variazione del valore atteso della \\(y\\) base al modello lineare nel caso di un cambiamento unitario di \\(x_j\\) al netto dell’effetto (lineare) di tutte le altre variabili \\(x\\) incluse nel modello.La seconda parte dell’affermazione precedente (“al netto di…”) è ciò che dobbiamo capire.Iniziamo calcolare con R coefficienti parziali del modello di regressione multipla descritto sopra:coefficienti parziali di regressione sono:Iniziamo considerare il coefficiente parziale associato Neuroticismo. Il valore \\(\\beta_1\\) = 0.054414 ci dice che ci aspettiamo che il valore di depressione aumenti media di \\(\\beta_1\\) = 0.054414 punti quando il livello di Neuroticismo aumenta di un punto e il livello di ansia di stato viene mantenuto costante.\nMa che vuol dire “mantenere costante” il livello di ansia di stato? Questo è un esempio di controllo statistico e può essere spiegato nel modo seguente.Abbiamo visto precedenza che il modello lineare scompone la \\(y\\) due componenti: la componente predicibile da \\(x\\) e la componente della \\(y\\) che un modello lineare non può prevedere.\nScomponiamo dunque valori bdi due componenti: la quota del bdi che l’ansia di stato può prevedere (con un modello lineare) e la quota del bdi che l’ansia di stato non può prevedere. Tale seconda componente di bdi è data dai residui (\\(\\varepsilon\\)) del seguente modello di regressione:\\[\\begin{equation}\n\\text{bdi} = \\alpha + \\beta \\cdot \\text{stateanx} + \\varepsilon.\n\\end{equation}\\]Usando R, tali residui sono dati da:Eseguiamo una scomposizione simile per valori di Neuroticismo: troviamo la quota di bfneur che l’ansia di stato può prevedere e la quota di bfneur che l’ansia di stato non può prevedere. Tale seconda componente di bfneur è data dai residui del seguente modello di regressione:\\[\\begin{equation}\n\\text{bfneur} = \\alpha + \\beta \\cdot \\text{stateanx} + \\varepsilon.\n\\end{equation}\\]Usando R, tali residui sono dati da:Abbiamo così ottenuto la componente della variabile dipendente linermente indipendente da stateanx e la componente di bfneur linermente indipendente da stateanx. altri termini, abbiamo “statisticamente controllato” l’effetto di stateanx su bdi e su bfneur. Se adesso vogliamo sapere qual è l’effetto di bfneur su bdi, indipendentemente dall’effetto di stateanx su entrambe le variabili, basta che eseguiamo l’analisi di regressione sulle componenti di bdi e bfneur che sono linearmente indipendenti da stateanx. Ovvero, dobbiamo calcolare il coefficiente \\(b\\) del seguente modello di regressione:Il coefficiente di regressione del modello precedenteè identico al coefficiente parziale di regressione associato alla variabile bfneur nel modello di regressione multipla:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]Ciò chiarisce il significato di “controllo statistico” e fa capire qual è la differenza tra il coefficiente di regressione del modello bivariato e il coefficiente parziale di regressione nel modello di regressione multipla.","code":"\nm <- lm(bdi ~ bfneur + stateanx, data = epi.bfi)\nout <- coef(m)\nround(out, 3)\n#> (Intercept)      bfneur    stateanx \n#>      -8.039       0.054       0.252\nm1 <- lm(bdi ~ stateanx, data = epi.bfi)\nhead(m1$res)\n#>          1          2          3          4          5          6 \n#> -0.3151358  0.1743948 -4.0501540  1.1743948 -7.0913093 -1.2133308\nm2 <- lm(bfneur ~ stateanx, data = epi.bfi)\nhead(m2$res)\n#>         1         2         3         4         5         6 \n#> -19.12342  27.87881 -24.12070  25.87881 -29.11785 -32.12144\nm3 <- lm(m1$res ~ m2$res)\ncoef(m3)[2]\n#>     m2$res \n#> 0.05441396\nout[2]\n#>     bfneur \n#> 0.05441396"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interazione-tra-i-regressone","chapter":"Capitolo 1 Una breve introduzione al modello di regressione","heading":"1.6.2 Interazione tra i regressone","text":"Esaminiamo ora l’ultima possibilità, ovvero quella di un modello di regressione multipla che contiene due variabili indipendenti che “interagiscono” tra loro.\nCosa significa il concetto di “interazione” tra due variabili indipendenti?Per rispondere questa domanda iniziamo considerare un modello senza interazione tra \\(x_1\\) e \\(x_2\\), ovvero il modello di regressione multipla con due regressori continui che abbiamo descritto precedenza. Esaminiamo una rappresentazione geometrica delle predizioni di tale modello.Per esaminare le predizioni del modello\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]non possiamo procedere come abbiamo fatto precedenza, quando avevamo due gruppi di osservazioni, ovvero, non possiamo disegnare una diversa retta di regressione per ciascun gruppo (non essendoci dei gruppi separati). Possiamo invece usare una funzione R come plot_model() che calcola la retta di regressione di bfneur su bdi selezionando alcuni valori “fissi” della variabile stateanx – nello specifico, la media di stateanx, la media meno una deviazione standard, e la media più una deviazione standard. Si produce così la rappresentazione di tre rette di regressione.Quello che notiamo dalla figura precedente è che le tre rette sono parallele. Un modello di regressione multipla che non include alcun termine di interazione è infatti un modello additivo: per quale che sia il livello della variabile stateanx, l’effetto di bfneur (ovvero, la pendenza della retta di regressione) non cambia. Quindi, l’effetto di stateanx semplicemente si somma ’effetto di bfneur.Una situazione ben diversa si ottiene nel caso di un modello con interazione, come il seguente:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\cdot x_2 + \\varepsilon.\n\\end{equation}\\]Adattiamo questo modello ai datie esaminiamo la souluzione ottenuta:Se rappresentiamo graficamente le predizioni del modellovediamo che, ora, le rette di regressione di bfneur su bdi corrispondenza dei tre valori prescelti di stateanx non sono più parallele. Questo significa che l’effetto di bfneur su bdi (la pendenza della retta di regressione) dipende dal valore di stateanx. Questo è il significato di “interazione”: bfneur ha un effetto diverso su bdi seconda del livello di stateanx.Nel caso presente, vediamo dalla figura che il Neuroticismo ha un effetto più grande sul livello di depressione quando consideriamo coloro che hanno livelli di ansia di stato elevati.coefficienti si interpretano come precedenza. L’intercetta corrisponde al valore atteso di bdi quando bfneur e stateanx hanno il valore di zero. Questo è di poco interesse. Per cui trasformiamo dati modo da utilizzare variabili indipendenti “centrate,” ovvero variabili da cui abbiamo sottratto la media.Riadattiamo il modello con le variabili centrate:Si vede che il coefficiente associato al termine di interazione è rimasto immutato. Ma ora è più facile interpretare il coefficiente dell’intercetta. questo caso, l’intercetta corrisponde al valore atteso di bdi quando bfneur e stateanx assumono il loro valore medio (ovvero zero, per le variabili trasformate). Per quanto riguarda gli effetti separati di bfneur e stateanx, questi non sono interpretabili, quanto la presenza di un’interazione significa, appunto, che bfneur e stateanx non hanno effetti separati su bdi.","code":"\nplot_model(m, type = \"pred\", terms = c(\"bfneur\", \"stateanx\"))\nm4 <- lm(bdi ~ bfneur * stateanx, data = epi.bfi)\nsummary(m4)\n#> \n#> Call:\n#> lm(formula = bdi ~ bfneur * stateanx, data = epi.bfi)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3273 -2.7107 -0.4746  1.9646 14.3104 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)      0.768518   3.763671   0.204   0.8384  \n#> bfneur          -0.040827   0.040952  -0.997   0.3199  \n#> stateanx         0.012673   0.100598   0.126   0.8999  \n#> bfneur:stateanx  0.002501   0.001008   2.483   0.0138 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.416 on 227 degrees of freedom\n#> Multiple R-squared:  0.4229, Adjusted R-squared:  0.4152 \n#> F-statistic: 55.44 on 3 and 227 DF,  p-value: < 2.2e-16\nplot_model(m4, type = \"pred\", terms = c(\"bfneur\", \"stateanx\"))\nepi.bfi$neur_c <- epi.bfi$bfneur - mean(epi.bfi$bfneur)\nepi.bfi$stateanx_c <- epi.bfi$stateanx - mean(epi.bfi$stateanx)\nm5 <- lm(bdi ~ neur_c * stateanx_c, data = epi.bfi)\nsummary(m5)\n#> \n#> Call:\n#> lm(formula = bdi ~ neur_c * stateanx_c, data = epi.bfi)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3273 -2.7107 -0.4746  1.9646 14.3104 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       6.450781   0.319275  20.204  < 2e-16 ***\n#> neur_c            0.058853   0.014445   4.074 6.38e-05 ***\n#> stateanx_c        0.232727   0.030117   7.727 3.51e-13 ***\n#> neur_c:stateanx_c 0.002501   0.001008   2.483   0.0138 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.416 on 227 degrees of freedom\n#> Multiple R-squared:  0.4229, Adjusted R-squared:  0.4152 \n#> F-statistic: 55.44 on 3 and 227 DF,  p-value: < 2.2e-16"}]
