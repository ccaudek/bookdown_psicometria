[{"path":"index.html","id":"benvenuti","chapter":"Benvenuti","heading":"Benvenuti","text":"Questo WORK--PROGRESS è stato aggiornato il giorno: 18 Gen 2021.Le presenti dispense contengono il materiale delle lezioni dell’insegnamento Psicometria B000286 (.. 2020/2021) rivolto agli studenti del primo anno del Corso di Laurea Scienze e Tecniche Psicologiche dell’Università di Firenze.","code":""},{"path":"obiettivi.html","id":"obiettivi","chapter":"Obiettivi","heading":"Obiettivi","text":"L’insegnamento di Psicometria si propone di fornire agli studenti un’introduzione ’analisi dei dati psicologia. Alcuni degli argomenti trattati richiedono delle conoscenze pregresse, soprattutto di tipo matematico. Tali conoscenze sono state aggiunte delle appendici di queste dispense. La lettura di tale materiale è consigliata tutti, sia chi sta studiando gli argomenti proposti per la prima volta, sia chi deve ripassare per colmare eventuali lacune pregresse.","code":""},{"path":"perché-tanta-statistica-in-psicologia.html","id":"perché-tanta-statistica-in-psicologia","chapter":"Perché tanta statistica in psicologia?","heading":"Perché tanta statistica in psicologia?","text":"Sembra sensato spendere due parole su un tema che è importante per gli studenti: quello indicato dal titolo di questa sezione.\nÈ ovvio che agli studenti di psicologia la statistica non piace.\nSe piacesse, forse studierebbero statistica e non psicologia; ma non lo fanno.\nDi conseguenza, gli studenti di psicologia si chiedono: ``perché dobbiamo perdere tanto tempo studiare queste cose quando realtà quello che ci interessa è tutt’altro?’’\nQuesta è una bella domanda.\nPer cercare di rispondere questa domanda introduco il paradosso di Simpson.Consideriamo un fenomeno sociale che ha suscitato un enorme interesse tempi recenti: la brutalità della polizia e le diseguaglianze razziali messe evidenza dalle uccisioni da parte della polizia statunitense.\nPer affrontare questo tema, esaminiamo l’analisi statistica descritta un articolo di Ross et al. (2020).\nLa logica di tale analisi statistica può essere descritta nel modo seguente.\nImmaginiamo due gruppi di individui: Montechi e Capuleti.\nIl 10% dei Montechi e il 20% dei Capuleti commette crimini violenti (ovvero, sono dei criminali).\nun anno, il 14% dei Montechi viene ucciso dalla polizia contro il 26% dei Capuleti.\nCome si fa capire se c’è un pregiudizio verso uno dei due gruppi?Cesario et al. (2019) sostengono che è necessario dividere la frequenza relativa di uccisioni da parte della polizia per la percentuale di criminali ciascun gruppo.\nQuindi, secondo questa logica, il tasso di uccisioni da parte della polizia è di 14/10 = 1.4% per Capuleti e di 26/20 = 1.3% per Montechi.\nQuesto indica una discriminazione contro Capuleti e ci fornisce la risposta alla nostra domanda.\nMa le cose stanno effettivamente così?Forse .\nSe decomponiamo il numero di uccisioni da parte della polizia ciascuna delle modalità della variabile criminalità (ovvero, criminali vs. non criminali), scopriamo che, per Capuleti, 14 morti possono essere suddivisi 5 morti di criminali e 9 morti di non criminali.\nPer Montechi, 26 morti si suddividono 10 morti di criminali e 16 morti di non criminali.\nQuindi, criminali Capuleti vengono uccisi dalla polizia ad un tasso del 5/10 = 50% e criminali Montechi vengono uccisi ad un tasso del 10/20 = 50% – lo stesso tasso nei due gruppi.\nMa non criminali Capuleti vengono uccisi dalla polizia ad un tasso del 9/90 = 10%, mentre non criminali Montechi vengono uccisi ad un tasso del 16/80 = 20%.Ciò significa che criminali di entrambi gruppi hanno la stessa probabilità di essere uccisi dalla polizia, ma non criminali Montechi hanno due volte la probabilità di essere uccisi dalla polizia dei non criminali Capuleti.\nQuesto indica un’enorme discriminazione contro Montechi!\nEppure, l’analisi precedente aveva prodotto il risultato opposto.Lasciando perdere Shakespeare, Ross et al. (2020) hanno dimostrato che questo è esattamente ciò che sta succedendo con dati reali sulle sparatorie della polizia negli Stati Uniti: neri disarmati negli Stati Uniti vengono uccisi tassi molto più alti rispetto ai bianchi disarmati, sebbene tassi siano simili nei due gruppi quando si considerano solo le sparatorie con individui armati; se tuttavia si riassumono dati considerando solo la frequenza totale dei morti scalata per il tasso di criminalità questo fatto viene oscurato.L’articolo di  ci fa vedere come sia necessario stare molto attenti con l’uso della statistica, specialmente quando gli errori statistici possono avere un impatto enorme sulla percezione pubblica – e, nella psicologia, sulla pratica dello psicologo.\nLe analisi statistiche precedenti sono un esempio di ciò che viene chiamato il paradosso di Simpson, ovvero il fatto che, alle volte, quando si riassumono dati un modo apparentemente ragionevole, si finisce per giungere ad una conclusione del tutto sbagliata.\nIl paradosso di Simpson illustra il fatto che non è semplice neppure  dati, figurarsi poi fare delle inferenze!\nQueste considerazioni ci fanno capire che, senza un certo livello di consapevolezza metodologica, lo psicologo (e non solo) si espone al rischio di fare errori gravissimi.Ma c’è un’altra ragione ancora più semplice che dovrebbe farci capire perché la statistica è così importante per la psicologia.\nInfatti, ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione.\nLa psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, certi casi, predire.\nquesto senso, la psicologia è molto diversa dall’ingegneria, per esempio.\nLe proprietà di un determinato ponte, sotto certe condizioni, sono molto simili quelle di un altro ponte, sotto le medesime condizioni.\nQuindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti.\nMa lo stesso non si può dire degli individui: ogni individuo è unico e cambia nel tempo.\nE le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri.\nQuesta è la ragione per cui abbiamo tanto bisogno della statistica psicologia: perché la statistica ci consente di descrivere la variazione e il cambiamento.\nE queste sono appunto le caratteristiche di base dei fenomeni psicologici.Sono sicuro che, leggendo queste righe, molti studenti sarà venuta mente la seguente domanda: perché non chiediamo qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli `tecnici’ della statistica?\nLa risposta questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della teoria statistica.\nMa non possiamo liberarci della statistica anche se non vogliamo diventare dei ricercatori e ci accontentiamo di svolgere la professione di psicologo.\nInfatti, anche questo secondo caso, non possiamo fare meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle nostre conoscenze è infatti richiesto dalla deontologia della professione.\nMa è necessario conoscere un bel po’ di statistica per potere fare questo.\nPer rendersi conto di quanto ciò sia vero basta aprire caso una rivista specialistica di psicologia: gli articoli che riportano risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali.\nE la comprensione della letteratura psicologica è proprio uno dei requisiti minimi del bagaglio professionale dello psicologo.Le considerazioni precedenti cercano di chiarire il seguente punto: la statistica non è qualcosa che, un singolo insegnamento universitario, dobbiamo studiare malincuore, per poi potercela tranquillamente dimenticare.\nNel bene e nel male, gli psicologi usano strumenti statistici tantissime fasi della loro attività professionale: particolare quando costruiscono, somministrano e interpretano test psicometrici.\nÈ dunque chiaro che possedere delle solide basi di statistica è un tassello imprescindibile del bagaglio professionale dello psicologo.","code":""},{"path":"come-studiare.html","id":"come-studiare","chapter":"Come studiare?","heading":"Come studiare?","text":"Il giusto metodo di studio per prepararsi ’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare concetti via via che essi vengono presentati e verificare autonomia le procedure presentate lezione.\nÈ importante fare domande lezione per sviluppare la capacità di mettere relazione tra loro diversi argomenti trattati, prendere parte alle esercitazioni organizzate dai Peer Tutor, utilizzare forum attivi su Moodle e, soprattutto, svolgere gli esercizi proposti su Moodle.\nproblemi forniti su Moodle rappresentano la difficoltà richiesta per superare l’esame e consentono allo studente di capire se le competenze sviluppate risultino essere sufficienti rispetto alle richieste dell’insegnamento.\nIncoraggio inoltre gli studenti venire ricevimento e parlare con per per chiarire ciò che non si è capito appieno.","code":""},{"path":"pacchetti.html","id":"pacchetti","chapter":"Capitolo 1 Pacchetti","heading":"Capitolo 1 Pacchetti","text":"Riporto qui tutti pacchetti che verranno usati queste dispense.","code":"\nsuppressPackageStartupMessages(library(\"here\"))\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nsuppressPackageStartupMessages(library(\"ggpubr\"))\nsuppressPackageStartupMessages(library(\"ggExtra\"))\nsuppressPackageStartupMessages(library(\"car\"))\nsuppressPackageStartupMessages(library(\"cowplot\"))\nsuppressPackageStartupMessages(library(\"tidybayes\"))\nsuppressPackageStartupMessages(library(\"datasauRus\"))\nsuppressPackageStartupMessages(library(\"RColorBrewer\"))\nsuppressPackageStartupMessages(library(\"rio\"))\nsuppressPackageStartupMessages(library(\"papaja\"))\nlibrary(\"patchwork\")\nset.seed(12345)\nsessionInfo()\n#> R version 3.6.3 (2020-02-29)\n#> Platform: x86_64-apple-darwin15.6.0 (64-bit)\n#> Running under: macOS Mojave 10.14.6\n#> \n#> Matrix products: default\n#> BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\n#> \n#> attached base packages:\n#> [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] foreign_0.8-75       psychTools_2.0.8     sjmisc_2.8.6         sjPlot_2.8.7        \n#>  [5] rethinking_2.01      dagitty_0.3-0        rstan_2.21.2         StanHeaders_2.21.0-7\n#>  [9] ggfortify_0.4.11     patchwork_1.1.1      rio_0.5.16           RColorBrewer_1.1-2  \n#> [13] datasauRus_0.1.4     tidybayes_2.3.1      cowplot_1.1.1        car_3.0-10          \n#> [17] carData_3.0-4        ggExtra_0.9          ggpubr_0.4.0         papaja_0.1.0.9997   \n#> [21] here_1.0.1           forcats_0.5.0        stringr_1.4.0        dplyr_1.0.3         \n#> [25] purrr_0.3.4          readr_1.4.0          tidyr_1.1.2          tibble_3.0.5        \n#> [29] ggplot2_3.3.3        tidyverse_1.3.0      goodshirt_0.2.2     \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           splines_3.6.3       \n#>   [5] svUnit_1.0.3         TH.data_1.0-10       inline_0.3.17        digest_0.6.27       \n#>   [9] htmltools_0.5.1.9000 fansi_0.4.2          magrittr_2.0.1       openxlsx_4.2.3      \n#>  [13] modelr_0.1.8         RcppParallel_5.0.2   matrixStats_0.57.0   sandwich_3.0-0      \n#>  [17] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          rappdirs_0.3.1      \n#>  [21] ggdist_2.4.0         haven_2.3.1          xfun_0.20            callr_3.5.1         \n#>  [25] crayon_1.3.4         jsonlite_1.7.2       lme4_1.1-26          survival_3.2-7      \n#>  [29] zoo_1.8-8            glue_1.4.2           gtable_0.3.0         emmeans_1.5.3       \n#>  [33] sjstats_0.18.1       V8_3.4.0             distributional_0.2.1 pkgbuild_1.2.0      \n#>  [37] shape_1.4.5          abind_1.4-5          scales_1.1.1         mvtnorm_1.1-1       \n#>  [41] DBI_1.1.1            ggeffects_1.0.1      rstatix_0.6.0        miniUI_0.1.1.1      \n#>  [45] Rcpp_1.0.6           performance_0.6.1.1  xtable_1.8-4         tmvnsim_1.0-2       \n#>  [49] stats4_3.6.3         httr_1.4.2           arrayhelpers_1.1-0   ellipsis_0.3.1      \n#>  [53] pkgconfig_2.0.3      loo_2.4.1            farver_2.0.3         sass_0.3.0.9000     \n#>  [57] dbplyr_2.0.0         utf8_1.1.4           cowsay_0.8.0         effectsize_0.4.1.2  \n#>  [61] tidyselect_1.1.0     labeling_0.4.2       rlang_0.4.10         later_1.1.0.1       \n#>  [65] munsell_0.5.0        cellranger_1.1.0     tools_3.6.3          fortunes_1.5-4      \n#>  [69] cli_2.2.0            generics_0.1.0       sjlabelled_1.1.7     broom_0.7.3         \n#>  [73] evaluate_0.14        fastmap_1.0.1        yaml_2.2.1           processx_3.4.5      \n#>  [77] knitr_1.30.2         fs_1.5.0             zip_2.1.1            nlme_3.1-151        \n#>  [81] mime_0.9             xml2_1.3.2           compiler_3.6.3       rstudioapi_0.13     \n#>  [85] curl_4.3             ggsignif_0.6.0       reprex_0.3.0         statmod_1.4.35      \n#>  [89] bslib_0.2.3.9000     stringi_1.5.3        parameters_0.10.1.1  highr_0.8           \n#>  [93] ps_1.5.0             lattice_0.20-41      Matrix_1.3-2         psych_2.0.12        \n#>  [97] nloptr_1.2.2.2       vctrs_0.3.6          pillar_1.4.7         lifecycle_0.2.0     \n#> [101] rmsfact_0.0.3        jquerylib_0.1.3      estimability_1.3     insight_0.12.0      \n#> [105] data.table_1.13.6    httpuv_1.5.5         R6_2.5.0             bookdown_0.21.6     \n#> [109] promises_1.1.1       gridExtra_2.3        codetools_0.2-18     boot_1.3-25         \n#> [113] MASS_7.3-53          assertthat_0.2.1     rprojroot_2.0.2      withr_2.4.0         \n#> [117] mnormt_2.0.2         multcomp_1.4-15      bayestestR_0.8.0.1   mgcv_1.8-33         \n#> [121] hms_1.0.0            grid_3.6.3           minqa_1.2.4          coda_0.19-4         \n#> [125] snakecase_0.11.0     rmarkdown_2.6.4      downlit_0.2.1        shiny_1.5.0         \n#> [129] lubridate_1.7.9.2"},{"path":"per-cominciare.html","id":"per-cominciare","chapter":"Capitolo 2 Per cominciare","heading":"Capitolo 2 Per cominciare","text":"Al fine di utilizzare R è necessario eseguire le seguenti tre operazioni\nnell’ordine dato:Installare R;Installare RStudio;Installare R-Packages (se necessario).Vedremo qui come installare  R e RStudio.","code":""},{"path":"per-cominciare.html","id":"installare-r-e-rstudio","chapter":"Capitolo 2 Per cominciare","heading":"2.1 Installare R e RStudio","text":"R è disponibile gratuitamente ed è scaricabile dal sito\nhttp://www.rproject.org/. Dalla pagina principale del sito\nr-project.org andiamo sulla sezione Download e scegliamo un server \npiacimento per scaricare il software d’installazione. Una volta\nscaricato l’installer, lo installiamo come un qualsiasi software,\ncliccando due volte sul file d’istallazione. Esistono versioni di R  per\ntutti più diffusi sistemi operativi (Windows, Mac OS X e Linux).Il R Core Development Team lavora continuamente per migliorare le\nprestazioni di R, per correggere errori e per consentire l’uso di   con\nnuove tecnologie. Di conseguenza, periodicamente vengono rilasciate\nnuove versioni di R. Informazioni questo proposito sono fornite sulla\npagina web https://www.r-project.org/. Per installare una nuova\nversione di R si segue la stessa procedura che è stata seguita per la\nprima installazione.Insieme al software si possono scaricare dal sito principale sia manuali d’uso che numerose dispense per approfondire diversi aspetti di R. particolare, nel sito http://cran.r-project.org/-docs.html si possono trovare anche numerose dispense italiano (sezione “languages”).Dopo avere installato R è opportuno installare anche RStudio. RStudio si\npuò scaricare da https://www.rstudio.com/. Anche RStudio è disponibile\nper tutti più diffusi sistemi operativi.","code":""},{"path":"per-cominciare.html","id":"utilizzare-rstudio-per-semplificare-il-lavoro","chapter":"Capitolo 2 Per cominciare","heading":"2.2 Utilizzare RStudio per semplificare il lavoro","text":"Possiamo pensare ad R come al motore di un automobile e RStudio come\nal cruscotto di un automobile. Più precisamente, R è un linguaggio di\nprogrammazione che esegue calcoli mentre RStudio è un ambiente di\nsviluppo integrato (IDE) che fornisce un’interfaccia grafica aggiungendo\nuna serie di strumenti che facilitano la fase di sviluppo e di\nesecuzione del codice. Utilizzeremo dunque R mediante RStudio. altre\nparole,non apriteaprite inveceL’ambiente di lavoro di RStudio è costituito da quattro finestre: la finestra del codice (scrivere-eseguire script), la finestra della console (riga di comando -\noutput), la finestra degli oggetti (elenco oggetti-cronologia dei\ncomandi) e la finestra dei pacchetti-dei grafici-dell’aiuto linea.La console di RStudio.","code":""},{"path":"per-cominciare.html","id":"eseguire-il-codice","chapter":"Capitolo 2 Per cominciare","heading":"2.2.1 Eseguire il codice","text":"Mediante il menu tendina di RStudio, scegliendo il percorsooppurel’utente può aprire nella finestra del codice (alto destra) un R Notebook o un R script dove inserire le istruzioni da eseguire.un R script, un blocco di codice viene eseguito selezionando un\ninsieme di righe di istruzioni e digitando la sequenza di tasti\nCommand + Invio sul Mac, oppure Control + Invio su Windows. \nun R Notebook, un blocco di codice viene eseguito schiacciando il\nbottone con l’icona \\(\\color{red}\\blacktriangleright\\) (“Run current\nchunk”) posizionata destra rispetto al codice.","code":"File > New File > R NotebookFile > New File > R Script"},{"path":"sintassi-di-base.html","id":"sintassi-di-base","chapter":"Capitolo 3 Sintassi di base","heading":"Capitolo 3 Sintassi di base","text":"R è un linguaggio di programmazione orientato ’analisi dei dati, il\ncalcolo e la visualizzazione grafica. È disponibile su Internet una\nvasta gamma di materiali utile per avvicinarsi ’ambiente R e aiutare\nl’utente nell’apprendimento di questo software statistico. Cercheremo\nqui di fornire alcune indicazioni e una breve descrizione delle risorse\ndi base di R.","code":""},{"path":"sintassi-di-base.html","id":"utilizzare-la-console-r-come-calcolatrice","chapter":"Capitolo 3 Sintassi di base","heading":"3.1 Utilizzare la console R come calcolatrice","text":"La console di RStudio contiene un cursore rappresentato dal simbolo “>”\n(linea di comando) dove si possono inserire comandi e le funzioni –\nrealtà è sempre meglio utilizzare un R  Notebook anziché la console,\nma per ora esaminiamo il funzionamento di quest’ultima.La console di RStudio può essere utilizzata come semplice calcolatrice.\ncomandi elementari consistono di espressioni o di assegnazioni. Le\noperazioni aritmetiche vengono eseguite mediante simboli “standard:” +,\n*, -, /, sqrt(), log(), exp(), …comandi sono separati da un carattere di nuova linea (si immette un\ncarattere di nuova linea digitando il tasto Invio). Se un comando non\nè completo alla fine della linea, R darà un prompt differente che per\ndefault è il carattere + sulla linea seguente e continuerà leggere\nl’input finché il comando non è sintatticamente completo. Ad esempio,R è un ambiente interattivo, ossia comandi producono una risposta\nimmediata. Se scriviamo 2 + 2 e premiamo il tasto di invio, comparirà\nnella riga successiva il risultato:Il risultato è preceduto da [1], il che significa che il risultato\ndell’operazione che abbiamo appena eseguito è il primo valore di questa\nlinea. Alcune funzioni ritornano più di un singolo numero e, quel\ncaso, l’informazione fornita da R  è più utile. Per esempio, l’istruzione\n100:130 ritorna \\(31\\) valori, ovvero numeri da \\(100\\) \\(130\\):questo caso, [24] indica che il valore \\(123\\) è il ventiquattresimo numero\nche è stato stampato sulla console.R  è un linguaggio di programmazione oggetti, quindi si basa sulla\ncreazione di oggetti e sulla possibilità di salvarli nella memoria del\nprogramma. Quando creiamo un oggetto gli attribuiamo un nome usando il\nsimbolo <-. Ad esempio, per creare un oggetto che contiene il\nrisultato dell’operazione 2 + 2 procediamo nel modo seguente:L’operazione di assegnazione (<-) copia il contenuto dell’operando\ndestro (detto r-value) nell’operando sinistro detto (l-value). Il\nvalore dell’espressione assegnazione è r-value. Nell’esempio\nprecedente, res_sum (l-value) assume il valore di \\(4\\).La console di RStudio fornisce la possibilità di richiamare e rieseguire\ncomandi. tasti freccia verticale, \\(\\uparrow\\) e \\(\\downarrow\\), sulla\ntastiera possono essere utilizzati per scorrere avanti e indietro \ncomandi già immessi. Appena trovato il comando che interessa, lo si può\nmodificare, ad esempio, con tasti freccia orizzontali, immettendo\nnuovi caratteri o cancellandone altri.Se viene digitato un comando che R non riconosce, sulla console viene\nvisualizzato un messaggio di errore; ad esempio,","code":"\n4 -\n+ \n+ 1\n#> [1] 3\n2 + 2\n#> [1] 4\n100:130\n#>  [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n#> [23] 122 123 124 125 126 127 128 129 130\nres_sum <- 2 + 2\nres_sum\n#> [1] 43 % 9\nErrore: unexpected input in \"3 % 9\""},{"path":"sintassi-di-base.html","id":"le-parentesi","chapter":"Capitolo 3 Sintassi di base","heading":"3.2 Le parentesi","text":"Le parentesi R  (come generale ogni linguaggio di programmazione) assegnano un significato diverso alle porzioni di codice che delimitano.Le parentesi tonde funzionano come nell’algebra. Per esempionon è equivalente aLe due istruzioni precedenti producono risultati diversi perché, se\nla sequenza delle operazioni algebriche non viene specificata dalle\nparentesi, R  assegna alle operazioni algebriche il seguente ordine\ndi priorità decrescente: esponenziazione, moltiplicazione /\ndivisione, addizione / sottrazione, confronti logici\n(<, >, <=, >=, ==, !=). È sempre una buona idea rendere esplicito\nl’ordine delle operazioni algebriche che si vuole eseguire mediante\nl’uso delle parentesi tonde.\nLe parentesi tonde vengono anche utilizzate per le funzioni, come\nvedremo nei prossimi paragrafi. Tra le parentesi tonde avremo dunque\nl’oggetto cui vogliamo applicare la funzione e gli argomenti\npassati alla funzione.Le parentesi graffe sono destinate alla programmazione. Un blocco\ntra le parentesi graffe viene letto come un oggetto unico che può\ncontenere una o più istruzioni.Le parentesi graffe sono destinate alla programmazione. Un blocco\ntra le parentesi graffe viene letto come un oggetto unico che può\ncontenere una o più istruzioni.Le parentesi quadre vengono utilizzate per selezionare degli\nelementi, per esempio ’interno di un vettore, o di una matrice, o\ndi un data.frame. L’argomento entro le parentesi quadre può essere\ngenerato da espressioni logiche.Le parentesi quadre vengono utilizzate per selezionare degli\nelementi, per esempio ’interno di un vettore, o di una matrice, o\ndi un data.frame. L’argomento entro le parentesi quadre può essere\ngenerato da espressioni logiche.","code":"\n2 + 3 * 4\n#> [1] 14\n(2 + 3) * 4\n#> [1] 20"},{"path":"sintassi-di-base.html","id":"i-nomi-degli-oggetti","chapter":"Capitolo 3 Sintassi di base","heading":"3.3 I nomi degli oggetti","text":"Le entità create e manipolate da R si chiamano ‘oggetti.’ Tali oggetti\npossono essere variabili, array di numeri, caratteri, stringhe,\nfunzioni, o più generale strutture costruite partire da tali\ncomponenti. Durante una sessione di  R gli oggetti sono creati e\nmemorizzati attraverso opportuni nomi.nomi possono contenere un qualunque carattere alfanumerico e come\ncarattere speciale il trattino basso (_) o il punto.  R fornisce \nseguenti vincoli per nomi degli oggetti: nomi degli oggetti non\npossono mai iniziare con un carattere numerico e non possono contenere \nseguenti simboli: $, @, !, ^, +, -, /, *. È buona\npratica usare nomi come ratio_of_sums. È fortemente sconsigliato\nutilizzare nei nomi degli oggetti caratteri accentati o, ancora peggio,\napostrofi. Per questa ragione è sensato creare nomi degli oggetti\nutilizzando la lingua inglese. È anche bene che nomi degli oggetti non\ncoincidano con nomi di funzioni. Si noti che R  è case sensitive, cioè\ne sono due simboli diversi e identificano due oggetti\ndifferenti.","code":""},{"path":"sintassi-di-base.html","id":"permanenza-dei-dati-e-rimozione-di-oggetti","chapter":"Capitolo 3 Sintassi di base","heading":"3.4 Permanenza dei dati e rimozione di oggetti","text":"Gli oggetti vengono salvati nello “spazio di lavoro” (workspace). Il\ncomando ls() può essere utilizzato per visualizzare nomi degli\noggetti che sono quel momento memorizzati R.Per eliminare oggetti dallo spazio di lavoro è disponibile la funzione\nrm(); ad esempiocancella tutti gli oggetti indicati entro parentesi. Per eliminare tutti\ngli oggetti presenti nello spazio di lavoro si può utilizzare la\nseguente istruzione:","code":"\nrm(x, y, z, ink, junk, temp, foo, bar)\nrm(list = ls())"},{"path":"sintassi-di-base.html","id":"chiudere-r","chapter":"Capitolo 3 Sintassi di base","heading":"3.5 Chiudere R","text":"Quando si chiude RStudio il programma ci chiederà se si desidera salvare\nl’area di lavoro sul computer. Tale operazione è da evitare quanto\ngli oggetti così salvati andranno ad interferire con gli oggetti creati\nun lavoro futuro. Si consiglia dunque di rispondere negativamente \nquesta domanda.RStudio, selezionare Preferences dal menu tendina e, \nR General Workspace, deselezionare l’opzione\nRestore .RData workspace start- e scegliere l’opzione\nNever nella finestra di dialogo Save workspace \n.RData exit.RStudio, selezionare Preferences dal menu tendina e, \nR General Workspace, deselezionare l’opzione\nRestore .RData workspace start- e scegliere l’opzione\nNever nella finestra di dialogo Save workspace \n.RData exit.R, selezionare Preferences dal menu tendina e, Startup,\nselezionare l’opzione corrispondenza dell’item\nSave workspace exit R.R, selezionare Preferences dal menu tendina e, Startup,\nselezionare l’opzione corrispondenza dell’item\nSave workspace exit R.","code":""},{"path":"sintassi-di-base.html","id":"sec:editor","chapter":"Capitolo 3 Sintassi di base","heading":"3.6 Creare ed eseguire uno script R con un editore","text":"È molto più facile interagire con   manipolando uno script con un\neditore piuttosto che inserendo direttamente le istruzioni nella\nconsole. R  fornisce il Text Editor dove è possibile inserire il codice\n(File \\(\\\\) New Script). Per salvare il file basta utilizzare l’apposito\nmenù tendina (estensione .R). Tale file potrà poi essere riaperto ed\nutilizzato un momento successivo.L’editore comunica con  R nel modo seguente: dopo avere selezionato la\nporzione di codice che si vuole eseguire, si digita un’apposita sequenza\ndi tasti (Command + Enter su Mac OS X e ctrl + r Windows).\nctrl + r significa premere il tasto ctrl e, tenendolo premuto, premere il tasto r della tastiera.\nCosì facendo, R eseguirà le istruzioni selezionate e l’output verrà\nstampato sulla console. Il Text Editor fornito da R è piuttosto\nprimitivo: è fortemente consigliato utilizzare RStudio.","code":""},{"path":"sintassi-di-base.html","id":"commentare-il-codice","chapter":"Capitolo 3 Sintassi di base","heading":"3.6.1 Commentare il codice","text":"commenti sono parole linguaggio naturale (nel nostro caso l’italiano), che permettono agli utilizzatori di capire il flusso logico del codice e chi lo ha scritto di ricordare il perché di determinate istruzioni.  R, le parole dopo il simbolo # sono considerate\ncommenti e sono ignorate; ad esempio:","code":"\n# Questo e' un commento"},{"path":"sintassi-di-base.html","id":"sec:change_dir","chapter":"Capitolo 3 Sintassi di base","heading":"3.7 Cambiare la cartella di lavoro","text":"Quando si inizia una sessione di lavoro,   sceglie una cartella quale\n“working directory.” Sarà tale cartella che andrà cercare gli\nscript definiti dall’utilizzatore e file dei dati. È possibile\ndeterminare quale sia la corrente “working directory” digitando sulla\nconsole di RStudio l’istruzione:Per cambiare la cartella di lavoro (maniera tale che corrisponda alla\ncartella nella quale sono stati salvati dati e gli script da eseguire)\nsi sceglie la voce Set Working Directory sul menù tendina di RStudio\ne si selezione la voce Choose Directory… Nella finestra che compare,\nsi cambia la cartella con quella che si vuole.","code":"\ngetwd()"},{"path":"sintassi-di-base.html","id":"loggetto-base-di-il-vettore","chapter":"Capitolo 3 Sintassi di base","heading":"3.8 L’oggetto base di : il vettore","text":"R  opera su strutture di dati; la più semplice di tali strutture è il\nvettore numerico, che consiste un insieme ordinato di numeri; ad\nesempio:Nell’istruzione precedente, c() è una funzione.   gli argomenti\nsono passati alle funzioni inserendoli ’interno delle parentesi\ntonde. Si noti che gli argomenti (questo caso, numeri\n\\(7.0, 10.2, -2.9, 21.4\\)) sono separati virgole. La funzione c() può\nprendere un numero arbitrario di argomenti e genera un vettore\nconcatenando suoi argomenti. L’operatore <- assegna un nome al\nvettore che è stato creato. Nel caso presente, digitando x possiamo\nvisualizzare il vettore che abbiamo creato:Se invece eseguiamo l’istruzionesenza assegnazione, il valore dell’espressione sarà visualizzato nella\nconsole, ma il vettore non potrà essere utilizzato nessun altro modo.","code":"\nx <- c(7.0, 10.2, -2.9, 21.4)\nx\n#> [1]  7.0 10.2 -2.9 21.4\nc(7.0, 10.2, -2.9, 21.4)\n#> [1]  7.0 10.2 -2.9 21.4"},{"path":"sintassi-di-base.html","id":"operazioni-vettorializzate","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.1 Operazioni vettorializzate","text":"Molte operazioni   sono vettorializzate, il che significa che esse\nsono eseguite parallelo determinati oggetti . Ciò consente di\nscrivere codice che sia efficiente, conciso e più facile da leggere\nrispetto al codice che contiene istruzioni non vettorializzate.","code":""},{"path":"sintassi-di-base.html","id":"vettori-aritmetici","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.2 Vettori aritmetici","text":"L’esempio più semplice che illustra come si svolgono le operazioni\nvettorializzate riguarda le operazioni algebriche applicate ai vettori.\nvettori, infatti, possono essere utilizzati espressioni numeriche\nnelle quali le operazioni algebriche vengono eseguite “elemento per\nelemento.”Per illustrare questo concetto, definiamo il vettore die che contiene\npossibili risultati del lancio di un dado:Supponiamo di volere sommare \\(10\\) ciascun elemento del vettore die.\nDato che le operazioni sui vettori sono eseguite elemento per elemento,\nper ottenere questo risultato è sufficiente eseguire l’istruzione:Si noti come la costante \\(10\\) sia stata sommata ciascun elemento del\nvettore. maniera corrispondente, l’istruzionesottrarrà un’unità da ciascuno degli elementi del vettore die.Se l’operazione aritmetica coinvolge due o più vettori,   allinea \nvettori ed esegue una sequenza di operazioni elemento per elemento. Per\nesempio, l’istruzionefa sì che due vettori vengano disposti l’uno di fianco ’altro per\npoi moltiplicare gli elementi corrispondenti: il primo elemento del\nprimo vettore per il primo elemento del secondo vettore e così via. Il\nvettore risultante avrà la stessa dimensione dei due vettori che sono\nstati moltiplicati, come indicato qui sotto:\\[\\begin{array}{ccccc}\n1 & \\times & 1 & \\& 1 \\\\\n2 & \\times & 2 & \\& 4 \\\\\n3 & \\times & 3 & \\& 9 \\\\\n4 & \\times & 4 & \\& 16 \\\\\n5 & \\times & 5 & \\& 25 \\\\\n6 & \\times & 6 & \\& 36 \\\\\n\\hline\n\\verb+die+ & * & \\verb+die+ & = & \n\\end{array}\\]Oltre agli operatori aritmetici elementari +, -, *, /, e ^ per\nl’elevamento potenza, sono disponibili le più comuni funzioni\nmatematiche: log(), exp(), sin(), cos(), tan(), sqrt(),\nmax(), min() e così via. Altre funzioni di uso comune sono:\nrange() che restituisce un vettore c(min(x), max(x)); sort() che\nrestituisce un vettore ordinato; length(x) che restituisce il numero\ndi elementi di x; sum(x) che dà la somma degli elementi di x,\nmentre prod(x) dà il loro prodotto. Due funzioni statistiche di uso\ncomune sono mean(x), la media aritmetica, e var(x), la varianza.","code":"\ndie <- c(1, 2, 3, 4, 5, 6)\ndie\n#> [1] 1 2 3 4 5 6\ndie + 10\n#> [1] 11 12 13 14 15 16\ndie - 1\n#> [1] 0 1 2 3 4 5\ndie * die\n#> [1]  1  4  9 16 25 36"},{"path":"sintassi-di-base.html","id":"generazione-di-sequenze-regolari","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.3 Generazione di sequenze regolari","text":"R  possiede un ampio numero di funzioni per generare sequenze di numeri.\nAd esempio, c(1:10) è il vettore c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10).\nL’espressione c(30:1) può essere utilizzata per generare una sequenza\n’indietro.La funzione seq() genera un vettore che contiene una sequenza regolare\ndi numeri, generata base determinate regole. Può avere 5 argomenti:\nprimi due rappresentano l’inizio () e la fine () della\nsequenza, il terzo specifica l’ampiezza del passo (), il quarto la\nlunghezza della sequenza (length.) e infine il quinto\n(along.), che se utilizzato deve essere l’unico parametro\npresente, è il nome di un vettore, ad esempio x, creando tal modo\nla sequenza 1, 2, …, length(x). Esempi di utilizzo della funzione\nseq() sono seguenti:Altra funzione utilizzata per generare sequenze è rep() che può essere\nutilizzata per replicare un oggetto vari modi. Ad esempio:metterà tre copie di die nell’oggetto die3.","code":"\nseq(from = 1, to = 10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\nseq(-5, 5, by = 2.5)\n#> [1] -5.0 -2.5  0.0  2.5  5.0\nseq(from = 1, to = 7, length.out = 4)\n#> [1] 1 3 5 7\nseq(along.with = die)\n#> [1] 1 2 3 4 5 6\ndie3 <- rep(die, times = 3)\ndie3\n#>  [1] 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6"},{"path":"sintassi-di-base.html","id":"sec:gen_rand_numbers","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.4 Generazione di numeri casuali","text":"La funzione sample() è una delle tante funzioni che possono essere\nusate per generare numeri casuali. Per esempio, la seguente istruzione\nsimula dieci lanci di un dado sei facce:Il primo argomento di sample() è il vettore da cui la funzione\nestrarrà degli elementi caso; il secondo argomento specifica che\ndovranno essere effettuate 10 estrazioni casuali; il terzo argomento\nspecifica che le estrazioni sono con rimessa (cioè, lo stesso elemento\npuò essere estratto più di una volta).Scegliere un elemento caso dal vettore \\(\\{1, 2, 3, 4, 5, 6\\}\\) è\nequivalente lanciare un dado e osservare la faccia che si presenta.\nL’istruzione precedente corrisponde dunque alla simulazione di dieci\nlanci di un dado sei facce.","code":"\nroll <- sample(1:6, 10, replace = TRUE)\nroll\n#>  [1] 5 4 5 6 4 6 1 5 5 5"},{"path":"sintassi-di-base.html","id":"vettori-logici","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.5 Vettori logici","text":"Quando si manipolano vettori, talvolta si vogliono trovare gli\nelementi che soddisfano determinate condizioni logiche. Per esempio, \ndieci lanci di un dado, quante volte è uscito \\(5\\)? Per rispondere \nquesta domanda si possono usare gli operatori logici <, > e == per\nle operazioni di “minore di,” “maggiore di” e “uguale .” Se scriviamocreiamo un vettore costituito da elementi TRUE/FALSE quali\nidentificano gli elementi del vettore che soddisfano la condizione\nlogica specificata.Possiamo trattare tale vettore come se fosse costituito da elementi di\nvalore \\(0\\) e \\(1\\). Sommando gli elementi di tale vettore, infatti,\npossiamo contare il numero di “5”:","code":"\nroll == 5\n#>  [1]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\nsum(roll == 5)\n#> [1] 5"},{"path":"sintassi-di-base.html","id":"dati-mancanti","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.6 Dati mancanti","text":"Quando si è presenza di un dato mancante,   assegna il valore\nspeciale NA, che sta per Available. generale, un’operazione\nsu un NA dà come risultato un NA. Nell’uso delle funzioni che\noperano sui dati sarà dunque necessario specificare che, qualunque\noperazione venga effettuata, gli NA devono essere esclusi.","code":""},{"path":"sintassi-di-base.html","id":"vettori-di-caratteri-e-fattori","chapter":"Capitolo 3 Sintassi di base","heading":"3.8.7 Vettori di caratteri e fattori","text":"vettori di caratteri si creano formando una sequenza di caratteri\ndelimitati da doppie virgolette e possono essere concatenati un\nvettore attraverso la funzione c(). Successivamente, si può applicare\nla funzione factor(), che definisce automaticamente le modalità della\nvariabile categoriale. Ad esempio,Talvolta l’ordine dei livelli del fattore non importa, mentre altre\nvolte l’ordine è importante, per esempio, quando una variable\ncategoriale viene rappresentata un grafico. Per specificare l’ordine\ndei livelli del fattore si usa la seguente sintassi:","code":"\nsoc_status <- factor(\n  c(\"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\", \"high\")\n)\nlevels(soc_status)\n#> [1] \"high\"   \"low\"    \"medium\"\nsoc_status <- \n  factor(soc_status, levels = c(\"low\", \"medium\", \"high\"))\nlevels(soc_status)\n#> [1] \"low\"    \"medium\" \"high\""},{"path":"sintassi-di-base.html","id":"cap:sintassi_funzioni","chapter":"Capitolo 3 Sintassi di base","heading":"3.9 Funzioni","text":"  offre la possibilità di utilizzare un’enorme libreria di funzioni che\npermettono di svolgere operazioni complicate, quali ad esempio, il\ncampionamento casuale. Esaminiamo ora con più attenzione le proprietà\ndelle funzioni di R utilizzando ancora l’esempio del lancio di un dado.\nAbbiamo visto precedenza come il lancio di un dado possa essere\nsimulato da R  con la funzione sample(). La funzione sample() prende\ntre argomenti: il nome di un vettore, un numero chiamato size e un\nargomento chiamato replace. La funzione sample() ritorna un numero\ndi elementi del vettore pari size. Ad esempioAssegnando TRUE ’argomento replace specifichiamo che vogliamo un\ncampionamento con rimessa.Se volgiamo eseguire una serie di lanci indipendenti di un dado,\neseguiamo ripetutamente la funzione sample() ponendo size uguale \n1:Come si fa sapere quanti e quali argomenti sono richiesti da una\nfunzione? Tale informazione viene fornita dalla funzione args(). Nel\nnostro casoci informa che il primo argomento è un vettore chiamato x, il secondo\nargomento è chiamato size ed ha il significato descritto sopra, il\nterzo argomento, replace, specifica se il campionamento è eseguito con\no senza reimmissione, e il quarto argomento, prob, assegna delle\nprobabilità agli elementi del vettore. Il significato degli argomenti\nviene spiegato nel file di help della funzione. Si noti che agli ultimi\ndue argomenti sono stati assegnati dei valori, detti di default. Ciò\nsignifica che, se l’utilizzatore non li cambia, verranno usati da . La\nspecificazione replace = FALSE significa che il campionamento viene\neseguito senza reimmissione. Se desideriamo un campionamento con\nreimmissione, basta specificare replace = TRUE (nel caso di una\nsingola estrazione è ovviamente irrilevante). Ad esempio, l’istruzione\nseguente simula risultati di 10 lanci indipendenti di un dado:Infine, prob = NULL specifica che non viene alterata la probabilità di\nestrazione degli elementi del vettore. generale, gli argomenti di una\nfunzione possono essere oggetti come vettori, matrici, altre funzioni,\nparametri o operatori logici.R  ha un sistema di help interno formato HTML che si richiama con\nhelp.start(). Per avere informazioni su qualche funzione specifica,\nper esempio la funzione sample(), il comando da utilizzare è\nhelp(sample) oppure ?sample.","code":"\nsample(die, 2, replace = TRUE)\n#> [1] 2 6\nsample(die, 1, replace = TRUE)\n#> [1] 3\nsample(die, 1, replace = TRUE)\n#> [1] 6\nsample(die, 1, replace = TRUE)\n#> [1] 3\nargs(sample)\n#> function (x, size, replace = FALSE, prob = NULL) \n#> NULL\nsample(die, 10, replace = TRUE)\n#>  [1] 5 5 4 5 2 6 4 2 3 1"},{"path":"sintassi-di-base.html","id":"scrivere-proprie-funzioni","chapter":"Capitolo 3 Sintassi di base","heading":"3.9.1 Scrivere proprie funzioni","text":"Abbiamo visto precedenza come sia possibile simulare risultati\nprodotti da dieci lanci di un dado o, maniera equivalente, dal\nsingolo lancio di dieci dadi. Possiamo replicare questo processo\ndigitando ripetutamente le stesse istruzioni nella console. Otterremo\nogni volta risultati diversi perché, ad ogni ripetizione, il generatore\ndi numeri pseudo-casuali di   dipende dal valore ottenuto dal clock\ninterno della macchina. La funzione set.seed() ci permette di\nreplicare esattamente risultati della generazione di numeri casuali.\nPer ottenere questo risultato, basta assegnare al seed un numero\narbitrario, es. set.seed(12345). Tuttavia, questa procedura è\npraticamente difficile da perseguire se il numero di ripetizioni è alto.\ntal caso è vantaggioso scrivere una funzione contenente il codice che\nspecifica il numero di ripetizioni. questo modo, per trovare il\nrisultato cercato basterà chiamare la funzione una sola volta.Le funzioni di R  sono costituite da tre elementi: il nome, il blocco del\ncodice e una serie di argomenti. Per creare una funzione è necessario\nimmagazzinare R  questi tre elementi e function() consente di\nottenere tale risultato usando la sintassi seguente:Una chiamata di funzione è poi eseguita nel seguente modo:Per potere essere utilizzata, una funzione deve essere presente nella\nmemoria di lavoro di R. Le funzioni salvate un file possono essere\nrichiamate utilizzando la funzione source(), ad esempio,\nsource(\"file_funzioni.R\").Consideriamo ora la funzione two_rolls() che ritorna la somma dei\npunti prodotti dal lancio di due dadi non truccati:La funzione two_rolls() inizia con il creare il vettore die che\ncontiene sei elementi: numeri da \\(1\\) \\(6\\). Viene poi utilizzata la\nfunzione sample() con gli gli argomenti, die, size = 2 e\nreplace = TRUE. Tale funzione restituisce il risultato del lancio di\ndue dadi. Il risultato fornito da sample(die, size = 2, replace = TRUE) viene assegnato ’oggetto res. L’oggetto res corrisponde dunque ad un vettore di due elementi.\nL’istruzione sum(res) somma gli elementi del vettore res e\nattribuisce il risultato di questa operazione sum_res. Infine, la\nfunzione return() ritorna il contenuto dell’oggetto sum_res.\nInvocando la funzione two_rolls() si ottiene dunque la somma del\nlancio di due dadi. generale, la funzione two_rolls() produrrà un\nrisultato diverso ogni volta che viene usata:La formattazione del codice mediante l’uso di spazi e rientri non è\nnecessaria ma è altamente raccomandata per minimizzare la probabilità di\ncompiere errori.","code":"\nnome_funzione <- function(arg1, arg2, ...) {\n  espressione1\n  espressione2\n  return(risultato)\n} \nnome_funzione(arg1, arg2, ...)\ntwo_rolls <- function() {\n  die <- 1:6\n  res <- sample(die, size = 2, replace = TRUE)\n  sum_res <- sum(res)\n  return(sum_res)\n}\ntwo_rolls()   \n#> [1] 9\ntwo_rolls()\n#> [1] 11\ntwo_rolls()\n#> [1] 7"},{"path":"sintassi-di-base.html","id":"pacchetti-1","chapter":"Capitolo 3 Sintassi di base","heading":"3.10 Pacchetti","text":"Le funzioni di  R sono organizzate pacchetti, più importanti dei\nquali sono già disponibili quando si accede al programma.","code":""},{"path":"sintassi-di-base.html","id":"istallazione-e-upgrade-dei-pacchetti","chapter":"Capitolo 3 Sintassi di base","heading":"3.10.1 Istallazione e upgrade dei pacchetti","text":"Alcuni pacchetti non sono presenti nella release di base di R. Per\ninstallare un pacchetto non presente è sufficiente scrivere nella\nconsole:Ad esempio,La prima volta che si usa questa funzione durante una sessione di lavoro\nsi dovrà anche selezionare da una lista il sito mirror da cui\nscaricare il pacchetto.Gli autori dei pacchetti periodicamente rilasciano nuove versioni dei\nloro pacchetti che contengono miglioramenti di varia natura. Per\neseguire l’upgrade dei pacchetti ggplot2 e dplyr, ad esempio, si usa\nla seguente istruzione:Per eseguire l’upgrade di tutti pacchetti l’istruzione è","code":"\ninstall.packages(\"nome_pacchetto\")\ninstall.packages(\"ggplot2\")\nupdate.packages(c(\"ggplot2\", \"dplyr\"))\nupdate.packages()"},{"path":"sintassi-di-base.html","id":"caricare-un-pacchetto-in-r","chapter":"Capitolo 3 Sintassi di base","heading":"3.10.2 Caricare un pacchetto in R","text":"L’istallazione dei pacchetti non rende immediatamente disponibili le\nfunzioni essi contenute. L’istallazione di un pacchetto semplicemente\ncopia il codice sul disco rigido della macchina uso. Per potere usare\nle funzioni contenute un pacchetto installato è necessario caricare\nil pacchetto . Ciò si ottiene con il comando:se si vuole caricare il pacchetto ggplot2. questo punto diventa\npossibile usare le funzioni contenute ggplot2. Queste operazioni si\npossono anche eseguire usando dal menu tendina di RStudio.Per sapere quali sono pacchetti già presenti nella release di  R con\ncui si sta lavorando, basta scrivere:","code":"\nlibrary(\"ggplot2\")\nlibrary()"},{"path":"strutture-di-dati.html","id":"strutture-di-dati","chapter":"Capitolo 4 Strutture di dati","heading":"Capitolo 4 Strutture di dati","text":"Solitamente gli psicologi raccolgono grandi quantità di dati. Tali dati\nvengono codificati R  ’interno di oggetti aventi proprietà\ndiverse. Intuitivamente, R un oggetto è qualsiasi cosa cui è\npossibile assegnare un valore. dati possono essere di tipo numerico o\nalfanumerico. Di conseguenza, R  distingue tra oggetti aventi modi\ndiversi. Inoltre, dati possono essere organizzati righe e colonne\nbase diversi tipi di strutture che R  chiama classi.","code":""},{"path":"strutture-di-dati.html","id":"classi-e-modi-degli-oggetti","chapter":"Capitolo 4 Strutture di dati","heading":"4.1 Classi e modi degli oggetti","text":"Gli oggetti R  si distinguono seconda della loro classe (class) e\ndel loro modo (mode). La classe definisce il tipo di oggetto. R,\nvengono utilizzate cinque strutture di dati che corrispondono cinque\nclassi differenti: vector, matrix, array, list e data.frame.\nUn’altra classe di oggetti R  è function (ad essa appartengono le\nfunzioni).La classe di appartenenza di un oggetto si stabilisce usando le funzioni\nclass(), oppure .list(), .function(), .logical(), e così\nvia. Queste funzioni restituisco TRUE e FALSE base\n’appartenenza o meno dell’argomento quella determinata classe.Gli oggetti R  possono anche essere classificati base al loro ‘modo.’\nmodi ‘atomici’ degli oggetti sono: numeric, complex, character e\nlogical. Per esempio,Nel seguito verranno esaminate le cinque strutture di dati utilizzate da\nR.","code":"\nx <- c(4, 9)\nmode(x)\n#> [1] \"numeric\"\ncards <- c(\"9 of clubs\", \"10 of hearts\", \"jack of hearts\") \nmode(cards)\n#> [1] \"character\""},{"path":"strutture-di-dati.html","id":"sec:vettori","chapter":"Capitolo 4 Strutture di dati","heading":"4.1.1 Vettori","text":"vettori sono la classe di oggetto più importante R. Un vettore può\nessere creato usando la funzione c():Le dimensioni di un vettore presente nella memoria di lavoro possono\nessere trovare con la funzione length(); ad esempio,ci dice che y è un vettore costituito da cinque elementi. La somma, il\nminimo e il massimo degli elementi contenuti un vettore si trovano\ncon le seguenti istruzioni:Mentre ci sono sei ‘tipi’ di vettori ‘atomici’ R, noi ci\nfocalizzeremo sui tipi seguenti: ‘numeric’ (‘integer’: e.g., 5;\n‘double’: e.g., 5.5), ‘character’ (e.g., ‘pippo’) e ‘logical’\n(e.g., TRUE, FALSE). Usiamo la funzione typeof() per determinare\nil ‘tipo’ di un vettore atomico. Tutti gli elementi di un vettore\natomico devono essere dello stesso tipo. La funzione str() rende\nvisibile maniera compatta la struttura interna di un oggetto.","code":"\ny <- c(2, 1, 6, -3, 9)\ny\n#> [1]  2  1  6 -3  9\nlength(y)\n#> [1] 5\nsum(y)\n#> [1] 15\nmin(y)\n#> [1] -3\nmax(y)\n#> [1] 9"},{"path":"strutture-di-dati.html","id":"matrici","chapter":"Capitolo 4 Strutture di dati","heading":"4.2 Matrici","text":"Una matrice è una collezione di vettori. Il comando per generare una\nmatrice è matrix():Il primo argomento è il vettore cui elementi andranno disporsi\n’interno della matrice. È poi necessario specificare le dimensioni\ndella matrice e il modo cui R  dovrà riempire la matrice. Date le\ndimensioni del vettore, la specificazione del numero di righe (secondo\nargomento) è sufficiente per determinare le dimensioni della matrice.\nL’argomento byrow = FALSE è il default. tal caso, R  riempie la\nmatrice per colonne. Se vogliamo che R  riempia la matrice per righe,\nusiamo byrow = TRUE:Le dimensioni di una matrice presente nella memoria di lavoro possono\nessere trovare con la funzione dim(); ad esempio,ci dice che Y è una matrice con quattro righe e cinque colonne.","code":"\nX <- matrix(1:20, nrow = 4, byrow = FALSE)\nX\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1    5    9   13   17\n#> [2,]    2    6   10   14   18\n#> [3,]    3    7   11   15   19\n#> [4,]    4    8   12   16   20\nY <- matrix(1:20, nrow = 4, byrow = TRUE)\nY\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1    2    3    4    5\n#> [2,]    6    7    8    9   10\n#> [3,]   11   12   13   14   15\n#> [4,]   16   17   18   19   20\ndim(Y)\n#> [1] 4 5"},{"path":"strutture-di-dati.html","id":"array","chapter":"Capitolo 4 Strutture di dati","heading":"4.3 Array","text":"Un array è una collezione di matrici (si veda la\nFigura 1.1). Per costruire un array con la\nfunzione array() è necessario specificare un vettore come primo\nargomento e un vettore di dimensioni, chiamato dim, quale secondo\nargomento:Un sottoinsieme di questi dati può essere selezionato, per esempio, nel\nmodo seguente:","code":"\nar <- array(\n  c(11:14, 21:24, 31:34), \n  dim = c(2, 2, 3)\n)\nar[, , 3]\n#>      [,1] [,2]\n#> [1,]   31   33\n#> [2,]   32   34"},{"path":"strutture-di-dati.html","id":"operazioni-aritmetiche-su-vettori-matrici-e-array","chapter":"Capitolo 4 Strutture di dati","heading":"4.4 Operazioni aritmetiche su vettori, matrici e array","text":"","code":""},{"path":"strutture-di-dati.html","id":"operazioni-aritmetiche-su-vettori","chapter":"Capitolo 4 Strutture di dati","heading":"4.4.1 Operazioni aritmetiche su vettori","text":"vettori e le matrici (o gli array) possono essere utilizzati \nespressioni aritmetiche. Il risultato è un vettore o una matrice (o un\narray) formato dalle operazioni fatte elemento per elemento sui vettori\no sulle matrici. Ad esempio,restituisce un vettore di dimensioni uguali alle dimensioni di y, \ncui elementi sono dati dalla somma tra ciascuno degli elementi originari\ndi y e la costante “3.”Ovviamente, ad un vettore possono essere applicate tutte le altre\noperazioni algebriche, sempre elemento per elemento. Ad esempio,restituisce un vettore cui elementi sono uguali agli elementi di y\nmoltiplicati per 3.Se sono costituiti dallo stesso numero di elementi, due vettori possono\nessere sommati, sottratti, moltiplicati e divisi, laddove queste\noperazioni algebriche vengono eseguite elemento per elemento. Per\nesempio,","code":"\ny + 3\n#> [1]  5  4  9  0 12\n3 * y\n#> [1]  6  3 18 -9 27\nx <- c(1, 1, 2, 1, 3)\ny <- c(2, 1, 6, 3, 9)\nx + y\n#> [1]  3  2  8  4 12\nx - y\n#> [1] -1  0 -4 -2 -6\nx * y\n#> [1]  2  1 12  3 27\nx / y\n#> [1] 0.5000000 1.0000000 0.3333333 0.3333333 0.3333333"},{"path":"strutture-di-dati.html","id":"operazioni-aritmetiche-su-matrici","chapter":"Capitolo 4 Strutture di dati","heading":"4.4.2 Operazioni aritmetiche su matrici","text":"Le operazioni algebriche elemento per elemento si possono estendere al\ncaso delle matrici. Per esempio, se X, Y sono entrambe matrici di\ndimensioni \\(4 \\times 5\\), allora la seguente operazionecrea una matrice D anch’essa di dimensioni \\(4 \\times 5\\) cui elementi\nsono ottenuti dalle operazioni fatte elemento per elemento sulle matrici\ne sugli scalari:","code":"\nM <- 2 * (X + Y) - 3 \nM\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1   11   21   31   41\n#> [2,]   13   23   33   43   53\n#> [3,]   25   35   45   55   65\n#> [4,]   37   47   57   67   77"},{"path":"strutture-di-dati.html","id":"operazioni-aritmetiche-su-array","chapter":"Capitolo 4 Strutture di dati","heading":"4.4.3 Operazioni aritmetiche su array","text":"Le stesse considerazioni si estendono al caso degli array.","code":""},{"path":"strutture-di-dati.html","id":"liste","chapter":"Capitolo 4 Strutture di dati","heading":"4.5 Liste","text":"Le liste assomigliano ai vettori perché raggruppano dati un insieme\nunidimensionale. Tuttavia, le liste non raggruppano elementi individuali\nma bensì oggetti di R, quali vettori e altre liste. Per esempio,Le doppie parentesi quadre identificano l’elemento della lista cui\nvogliamo fare riferimento. Per esempio,","code":"\nlist1 <- list(\"R\", list(TRUE, FALSE), 20:24)\nlist1\n#> [[1]]\n#> [1] \"R\"\n#> \n#> [[2]]\n#> [[2]][[1]]\n#> [1] TRUE\n#> \n#> [[2]][[2]]\n#> [1] FALSE\n#> \n#> \n#> [[3]]\n#> [1] 20 21 22 23 24\nlist1[[3]]\n#> [1] 20 21 22 23 24\nlist1[[3]][2]\n#> [1] 21"},{"path":"strutture-di-dati.html","id":"data-frame","chapter":"Capitolo 4 Strutture di dati","heading":"4.6 Data frame","text":"data.frame sono strutture tipo matrice, cui le colonne possono\nessere vettori di tipi differenti. La funzione usata per generare un\ndata frame è data.frame(), che permette di unire più vettori di uguale\nlunghezza come colonne del data frame, ognuno dei quali si riferisce ad\nuna diversa variabile. Ad esempio,L’estrazione di dati da un data.frame può essere effettuata maniera\nsimile quanto avviene per vettori. Ad esempio, per estrarre la\nvariabile value dal data.frame df si può indicare l’indice della\nterza colonna:Dal momento che le colonne sono delle variabili, è possibile estrarle\nanche indicando nome della variabile, scrivendo\nnome_data_frame$nome_variabile:Per fare un esempio, creiamo un data.frame che contenga tutte le informazioni di un mazzo di carte da poker (Grolemund, 2014). tale data.frame, ciascuna riga\ncorrisponde ad una carta – un mazzo da poker ci sono 52 carte,\nperciò il data.frame avrà 52 righe. Il vettore face indica con una\nstringa di caratteri il valore di ciascuna carta, il vettore suit\nindica il seme e il vettore value indica con un numero intero il\nvalore di ciascuna carta. Quindi, il data.frame avrà 3 colonne.Avendo salvato tutte queste informazioni nell’oggetto deck, possiamo\nstamparle sullo schermo semplicemente digitando il nome dell’oggetto che\nle contiene:Si noti che, schermo, R  stampa un numero progressivo che corrisponde\nal numero della riga.","code":"\ndf <- data.frame(\n  face = c(\"ace\", \"two\", \"six\"),\n  suit = c(\"clubs\", \"clubs\", \"clubs\"), \n  value = c(1, 2, 3)\n)\ndf\n#>   face  suit value\n#> 1  ace clubs     1\n#> 2  two clubs     2\n#> 3  six clubs     3\ndf[, 3]\n#> [1] 1 2 3\ndf$value\n#> [1] 1 2 3\ndeck <- data.frame(\n  face = c(\"king\", \"queen\", \"jack\", \"ten\", \"nine\", \"eight\",\n  \"seven\", \"six\", \"five\", \"four\", \"three\", \"two\", \"ace\", \n  \"king\", \"queen\", \"jack\", \"ten\", \"nine\", \"eight\", \"seven\", \n  \"six\", \"five\", \"four\", \"three\", \"two\", \"ace\", \"king\", \n  \"queen\", \"jack\", \"ten\", \"nine\", \"eight\", \"seven\", \"six\", \n  \"five\", \"four\", \"three\", \"two\", \"ace\", \"king\", \"queen\", \n  \"jack\", \"ten\", \"nine\", \"eight\", \"seven\", \"six\", \"five\", \n  \"four\", \"three\", \"two\", \"ace\"), \n  suit = c(\"spades\", \"spades\", \"spades\", \"spades\", \n  \"spades\", \"spades\", \"spades\", \"spades\", \"spades\", \n  \"spades\", \"spades\", \"spades\", \"spades\", \"clubs\", \"clubs\", \n  \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"clubs\", \n  \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"diamonds\", \n  \"diamonds\", \"diamonds\", \"diamonds\", \"diamonds\", \n  \"diamonds\", \"diamonds\", \"diamonds\", \"diamonds\", \n  \"diamonds\", \"diamonds\", \"diamonds\", \"diamonds\", \"hearts\", \n  \"hearts\", \"hearts\", \"hearts\", \"hearts\", \"hearts\", \n  \"hearts\", \"hearts\", \"hearts\", \"hearts\", \"hearts\", \n  \"hearts\", \"hearts\"), \n  value = c(13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n)\ndeck\n#>     face     suit value\n#> 1   king   spades    13\n#> 2  queen   spades    12\n#> 3   jack   spades    11\n#> 4    ten   spades    10\n#> 5   nine   spades     9\n#> 6  eight   spades     8\n#> 7  seven   spades     7\n#> 8    six   spades     6\n#> 9   five   spades     5\n#> 10  four   spades     4\n#> 11 three   spades     3\n#> 12   two   spades     2\n#> 13   ace   spades     1\n#> 14  king    clubs    13\n#> 15 queen    clubs    12\n#> 16  jack    clubs    11\n#> 17   ten    clubs    10\n#> 18  nine    clubs     9\n#> 19 eight    clubs     8\n#> 20 seven    clubs     7\n#> 21   six    clubs     6\n#> 22  five    clubs     5\n#> 23  four    clubs     4\n#> 24 three    clubs     3\n#> 25   two    clubs     2\n#> 26   ace    clubs     1\n#> 27  king diamonds    13\n#> 28 queen diamonds    12\n#> 29  jack diamonds    11\n#> 30   ten diamonds    10\n#> 31  nine diamonds     9\n#> 32 eight diamonds     8\n#> 33 seven diamonds     7\n#> 34   six diamonds     6\n#> 35  five diamonds     5\n#> 36  four diamonds     4\n#> 37 three diamonds     3\n#> 38   two diamonds     2\n#> 39   ace diamonds     1\n#> 40  king   hearts    13\n#> 41 queen   hearts    12\n#> 42  jack   hearts    11\n#> 43   ten   hearts    10\n#> 44  nine   hearts     9\n#> 45 eight   hearts     8\n#> 46 seven   hearts     7\n#> 47   six   hearts     6\n#> 48  five   hearts     5\n#> 49  four   hearts     4\n#> 50 three   hearts     3\n#> 51   two   hearts     2\n#> 52   ace   hearts     1"},{"path":"strutture-di-dati.html","id":"selezione-di-elementi","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1 Selezione di elementi","text":"Una volta creato un data.frame, ad esempio quello che contiene un mazzo\nvirtuale di carte (si veda\nl’esempio \\[exmp:deck_of_cards\\]), è necessario sapere come manipolarlo.\nLa funzione head() mostra le prime sei righe del data.frame:Poniamoci ora il problema di mescolare il mazzo di carte e di estrarre\nalcune carte dal mazzo. Queste operazioni possono essere eseguite usando\nil sistema notazionale di R.Il sistema di notazione di R  consente di estrarre singoli elementi\ndagli oggetti definiti da R. Per estrarre un valore da un data.frame,\nper esempio, dobbiamo scrivere il nome del data.frame seguito da una\ncoppia di parentesi quadre:’interno delle parentesi quadre ci sono due indici separati da una\nvirgola. R  usa il primo indice per selezionare un sottoinsieme di righe\ndel data.frame e il secondo indice per selezionare un sottoinsieme di\ncolonne. Per esempio,restituisce l’elemento che si trova nella nella nona riga della seconda\ncolonna di deck.R  ci sono sei modi diversi per specificare gli indici di un oggetto:\ninteri positivi, interi negativi, zero, spazi vuoti, valori logici e\nnomi. Esaminiamoli qui di seguito.","code":"\nhead(deck)\n#>    face   suit value\n#> 1  king spades    13\n#> 2 queen spades    12\n#> 3  jack spades    11\n#> 4   ten spades    10\n#> 5  nine spades     9\n#> 6 eight spades     8\ndeck[, ]\ndeck[9, 2]\n#> [1] spades\n#> Levels: clubs diamonds hearts spades"},{"path":"strutture-di-dati.html","id":"interi-positivi","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.1 Interi positivi","text":"Gli indici \\(, j\\) possono essere degli interi positivi che identificano\nl’elemento nella \\(\\)-esima riga e nella \\(j\\)-esima colonna del\ndata.frame. Per l’esempio relativo al mazzo di carte, l’istruzioneritorna il valore nella prima riga e nella prima colonna. Per estrarre\npiù di un valore, usiamo un vettore di interi positivi. Per esempio, la\nprima riga di deck si trova conTale sistema notazionale non si applica solo ai data.frame ma può essere\nusato anche per gli altri oggetti di R.L’indice usato da R  inizia da 1. altri linguaggi di programmazione,\nper esempio C, inizia da 0.","code":"\ndeck[1, 1]\n#> [1] king\n#> Levels: ace eight five four jack king nine queen seven six ten three two\ndeck[1, c(1:3)]\n#>   face   suit value\n#> 1 king spades    13"},{"path":"strutture-di-dati.html","id":"interi-negativi","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.2 Interi negativi","text":"Gli interi negativi fanno l’esatto contrario degli interi positivi: R \nritornerà tutti gli elementi tranne quelli specificati dagli interi\nnegativi. Per esempio, la prima riga del data.frame può essere\nspecificata nel modo seguenteovvero, escludendo tutte le righe seguenti.","code":"\ndeck[-(2:52), 1:3]\n#>   face   suit value\n#> 1 king spades    13"},{"path":"strutture-di-dati.html","id":"zero","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.3 Zero","text":"Quando lo zero viene usato come indice, R  non ritorna nulla dalla\ndimensione cui lo zero si riferisce. L’istruzioneritorna un data.frame vuoto. Non molto utile.","code":"\ndeck[0, 0]\n#> data frame with 0 columns and 0 rows"},{"path":"strutture-di-dati.html","id":"spazio","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.4 Spazio ’ ’","text":"Uno spazio viene usato quale indice per comunicare R  di estrarre\ntutti valori quella dimensione. Questo è utile per estrarre intere\ncolonne o intere righe da un data.frame. Per esempio, l’istruzioneritorna la terza riga del data.frame deck.","code":"\ndeck[3, ]\n#>   face   suit value\n#> 3 jack spades    11"},{"path":"strutture-di-dati.html","id":"valori-booleani","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.5 Valori booleani","text":"Se viene fornito un vettore di stringhe TRUE, FALSE, R  selezionerà\ngli elementi riga o colonna corrispondenti ai valori booleani TRUE\nusati quali indici. Per esempio, l’istruzioneritorna valori delle prime due colonne della terza riga di deck.","code":"\ndeck[3, c(TRUE, TRUE, FALSE)]\n#>   face   suit\n#> 3 jack spades"},{"path":"strutture-di-dati.html","id":"nomi","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.1.6 Nomi","text":"È possibile selezionare gli elementi del data.frame usando loro nomi.\nPer esempio,","code":"\ndeck[1, c(\"face\", \"suit\", \"value\")]\n#>   face   suit value\n#> 1 king spades    13\ndeck[, \"value\"]\n#>  [1] 13 12 11 10  9  8  7  6  5  4  3  2  1 13 12 11 10  9  8  7  6  5  4  3  2  1 13 12 11\n#> [30] 10  9  8  7  6  5  4  3  2  1 13 12 11 10  9  8  7  6  5  4  3  2  1"},{"path":"strutture-di-dati.html","id":"giochi-di-carte","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.2 Giochi di carte","text":"Avendo presentato le nozioni base del sistema di notazione di R,\nutilizziamo tali conoscenze per manipolare il data.frame. L’istruzioneritorna tutte le righe e tutte e le colonne del data.frame deck. Le\nrighe sono identificate dal primo indice, che va da 1 52. Permutare \nmodo casuale l’indice delle righe equivale mescolare il mazzo di\ncarte. Per fare questo, utilizziamo la funzione sample() ponendo replace=FALSE e size\nuguale alla dimensione del vettore che contiene gli indici da 1 52:Utilizzando il vettore random di indici permutati otteniamo il\nrisultato cercato:Possiamo ora scrivere una funzione che include le precedenti istruzioni:Invocando la funzione shuffle() possiamo generare un data.frame che\nrappresenta un mazzo di carte mescolato:Se immaginiamo di distribuire le carte di questo mazzo due giocatori\ndi poker, per il primo giocatore avremo:e per il secondo:","code":"\ndeck[1:52, ]\nrandom <- sample(1:52, size = 52, replace = FALSE)\nrandom\n#>  [1] 45 23 12 37 38 47 31  4 48  9  5 42 24 15 13  2 41 21 34  6 46 32 52 16 49  3 25 22 11\n#> [30]  7 35 33 43 10 28 17 30  8 26  1 14 18 27 44 20 51 40 36 29 39 50 19\ndeck_shuffled <- deck[random, ]\nhead(deck_shuffled)\n#>     face     suit value\n#> 45 eight   hearts     8\n#> 23  four    clubs     4\n#> 12   two   spades     2\n#> 37 three diamonds     3\n#> 38   two diamonds     2\n#> 47   six   hearts     6\nshuffle <- function(cards) {\n  random <- sample(1:52, size = 52, replace = FALSE) \n  return(cards[random, ])\n}\ndeck_shuffled <- shuffle(deck)\ndeck_shuffled[c(1, 3, 5, 7, 9), ]\n#>     face     suit value\n#> 19 eight    clubs     8\n#> 51   two   hearts     2\n#> 11 three   spades     3\n#> 35  five diamonds     5\n#> 23  four    clubs     4\ndeck_shuffled[c(2, 4, 6, 8, 10), ]\n#>     face     suit value\n#> 28 queen diamonds    12\n#> 48  five   hearts     5\n#> 26   ace    clubs     1\n#> 2  queen   spades    12\n#> 42  jack   hearts    11"},{"path":"strutture-di-dati.html","id":"variabili-locali","chapter":"Capitolo 4 Strutture di dati","heading":"4.6.3 Variabili locali","text":"Si noti che, nell’esempio precedente, abbiamo passato l’argomento deck\nalla funzione shuffle(), perché questo è il nome del data.frame che\nvolevamo manipolare. Nella definizione della funzione shuffle(), però,\nl’argomento della funzione era chiamato cards. Il nome degli argomenti\nè diverso nei due casi. Allora perché l’istruzione shuffle(deck) non\ndà un messaggio d’errore?La risposta questa domanda è che nelle funzioni le variabili nascono\nquando la funzione entra esecuzione e muoiono al termine\ndell’esecuzione della funzione. Per questa ragione, sono dette ‘locali.’\nLa variabile cards, questo esempio, esiste soltanto ’interno\ndella funzione. Dunque non deve (necessariamente) avere lo stesso nome\ndi un altro oggetto che esiste al di fuori della funzione, nello spazio\ndi lavoro di R  (anzi, è meglio se il nome degli oggetti usati\n’interno delle funzioni è diverso da quello degli oggetti che\nesistono fuori dalle funzioni). R  sa che l’oggetto deck passato \nshuffle() corrisponde cards ’interno della funzione perché\nassegna il nome cards qualunque oggetto venga passato alla funzione\nshuffle() come primo (e, questo caso, unico) argomento.","code":""},{"path":"strutture-di-controllo.html","id":"strutture-di-controllo","chapter":"Capitolo 5 Strutture di controllo","heading":"Capitolo 5 Strutture di controllo","text":" R esistono strutture di controllo specifiche per regolare il flusso\ndi esecuzione di un programma. loop permettono di ripetere\nciclicamente blocchi di istruzioni per un numero prefissato di volte o\nfino che una determinata condizione logica viene soddisfatta. Questo\nli rende utili per la programmazione di simulazioni.","code":""},{"path":"strutture-di-controllo.html","id":"il-ciclo-for","chapter":"Capitolo 5 Strutture di controllo","heading":"5.1 Il ciclo for","text":"Il ciclo è una struttura di controllo iterativa che determina\nl’esecuzione di una porzione di programma ripetuta per un certo numero\nnoto di volte. Il linguaggio R  usa la seguente sintassi per il ciclo\n:(indice valori_indice) { operazioni }il che significa “esegui le operazioni operazioni per diversi valori\ndi indice compresi nel vettore valori_indice.” Per esempio, il\nseguente ciclo non fa altro che stampare il valore della variabile\ncontatore ciascuna esecuzione del ciclo:Un esempio (leggermente) più complicato è il seguente:Per esempio, quanti numeri pari sono contenuti un vettore? La\nrisposta questa domanda viene fornita dalla funzione\ncountEvenNumbers() che possiamo definire come indicato qui sotto:Nella funzione countEvenNumbers() abbiamo inizializzato la variabile\ncount zero. Prima dell’esecuzione del ciclo , dunque, count\nvale zero. Il ciclo viene eseguito tante volte quanti sono gli\nelementi che costituiscono il vettore x. L’indice dunque assume\nvalori compresi tra 1 e il valore che corrisponde al numero di elementi\ndi x. L’operazione modulo, indicato con %% dà come risultato il\nresto della divisione euclidea del primo numero per il secondo. Per\nesempio, 9 %% 2 dà come risultato \\(1\\) perché questo è il resto della\ndivisione \\(9/2\\). L’operazione modulo dà come risultato \\(0\\) per tutti \nnumeri pari. ciascuna esecuzione del ciclo l’operazione modulo\nviene eseguita, successivamente, su uno degli elementi di x. Se\nl’operazione modulo dà \\(0\\) come risultato, ovvero se il valore\nconsiderato è un numero pari, allora la variabile count viene\nincrementata di un’unità. L’istruzione return() ritorna dunque il\nnumero di valori pari contenuti nel vettore di input alla funzione. Per\nesempio:","code":"\nfor (i in 1:3) {\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\nx_list <- seq(1, 9, by = 2)\nx_list\n#> [1] 1 3 5 7 9\nsum_x <- 0\nfor (x in x_list) {\n  sum_x <- sum_x + x\n  cat(\"L'indice corrente e'\", x, \"\\n\")\n  cat(\"La frequenza cumulata e'\", sum_x, \"\\n\") \n}\n#> L'indice corrente e' 1 \n#> La frequenza cumulata e' 1 \n#> L'indice corrente e' 3 \n#> La frequenza cumulata e' 4 \n#> L'indice corrente e' 5 \n#> La frequenza cumulata e' 9 \n#> L'indice corrente e' 7 \n#> La frequenza cumulata e' 16 \n#> L'indice corrente e' 9 \n#> La frequenza cumulata e' 25\ncountEvenNumbers <- function(x) {\n  count <- 0\n  for (i in 1:length(x)) {\n    if (x[i] %% 2 == 0)  \n      count = count + 1\n  }\n  count\n}\nx <- c(1, 2, 1, 4, 6, 3, 9, 12)\ncountEvenNumbers(x)\n#> [1] 4"},{"path":"inputoutput.html","id":"inputoutput","chapter":"Capitolo 6 Input/Output","heading":"Capitolo 6 Input/Output","text":"dati raccolti dallo psicologo sono contenuti file aventi formati\ndiversi: solo testo, CSV, Excel, eccetera. R  prevede diverse funzioni\ndi importazione dei dati. Esamineremo qui la funzione read.table() per\nl’importazione di dati formato solo testo, ma funzioni analoghe\npossono essere usate per molti altri formati possibili.","code":""},{"path":"inputoutput.html","id":"la-funzione-read.table","chapter":"Capitolo 6 Input/Output","heading":"6.1 La funzione read.table()","text":"Ci sono tanti modi per importare un file dal nostro computer. R \npermette di utilizzare delle funzioni che sono già nella libreria di\nbase, oppure possiamo utilizzare delle funzioni specifiche, seconda\ndel tipo di file da importare, che sono contenute pacchetti\naggiuntivi. Per leggere dati da file R  è conveniente\npreliminarmente generare un file di dati formato ASCII, disponendoli\ncome si farebbe una matrice di dati, e mettere questo file nella\ncartella di lavoro corrente. Fatto questo, si può utilizzare la funzione\nread.table() presente nella libreria di base per leggere l’intero\ndataset. Se la prima riga del file contiene l’intestazione delle\nvariabili, allora read.table(\"my_file.txt\", header = TRUE)\ninterpreterà la prima riga del file come una riga dove sono contenuti \nnomi delle variabili, assegnando ciascun nome alle variabili del data\nframe:alternativa, si può impiegare la funzione read.csv(), che è adatta\nleggere dati salvati .csv. Utilizzando altre funzioni, si possono\nleggere R  dati contenuti file aventi formati diversi da quelli\nconsiderati qui, quali Excel, SPSS, ecc.","code":"mydata <- read.table(\"my_file.txt\", header = TRUE)"},{"path":"inputoutput.html","id":"file-di-dati-forniti-da-r","chapter":"Capitolo 6 Input/Output","heading":"6.2 File di dati forniti da R","text":"R  esistono comunque oltre 50 insiemi di dati contenuti nel package\nbase e altri sono disponibili altri packages. Per vedere l’elenco\ndegli insiemi di dati disponibili nel package base basta usare\nl’istruzione data(); per caricare un particolare insieme di dati, ad\nesempio cars, basta utilizzare l’istruzioneNella maggior parte dei casi questo corrisponde caricare un oggetto,\nsolitamente un data.frame dello stesso nome: per l’esempio considerato\nsi avrebbe un data frame di nome cars.","code":"data(cars)"},{"path":"inputoutput.html","id":"esportazione-di-un-file","chapter":"Capitolo 6 Input/Output","heading":"6.3 Esportazione di un file","text":"Per esportare un data.frame formato .csv possiamo scrivere il\nseguente codicedove df_esempio è il data.frame da salvare e esempio.csv è il file\nche verrà salvato ’interno della nostra cartellla di lavoro.","code":"write.csv(df_esempio, file = \"esempio.csv\", row.names = FALSE)"},{"path":"inputoutput.html","id":"pacchetto-rio","chapter":"Capitolo 6 Input/Output","heading":"6.4 Pacchetto rio","text":"Un’alternativa più semplice è fornita dalle funzioni fornite dal pacchetto rio. Per importare dati da un file qualsiasi formato si usaPer esportare dati un file avente qualsiasi formato si usa invece","code":"\nmy_data_frame <- rio::import(\"my_file.csv\")\nrio::export(my_data_frame, \"my_file.csv\")"},{"path":"inputoutput.html","id":"dove-sono-i-miei-file","chapter":"Capitolo 6 Input/Output","heading":"6.5 Dove sono i miei file?","text":"Quello che abbiamo detto finora, proposito dell’importazione ed esportazione dei file, si riferisce file che si trovano nella cartella di lavoro (working directory). Ma non sempre ci troviamo questa situazione, il che è anche una buona cosa, perché se dobbiamo gestire un progetto anche leggermente complesso è sempre una buona idea salvare file che usiamo cartelle diverse. Per esempio, possiamo usare una cartella chiamata psicometria dove salviamo tutto il materiale di questo insegnamento. Nella cartella psicometria ci potrà essere una cartella chiamata scripts dove salveremo gli script con il codice R utilizzato per vari esercizi, e una cartella chiamata data dove possiamo salvare dati. Questa organizzazione minimale ci pone, però, difronte ad un problema: dati che vogliamo caricare R non si trovano più nella cartella dove sono contenuti gli script. Quando importiamo un file di dati dobbiamo dunque specificare il percorso che identifica la posizione sul nostro computer del file che ci interessa.Questo problema può essere risolto due modi: speficicando l’inridizzo del file modo assoluto o relativo. Specificare l’indirizzo di un file modo assoluto ha una serie di limiti. Il più grande è che non sarà possibile utilizzare quell’istruzione su una macchina diversa. Dunque, è molto più conveniente specificare l’indirizzo dei file modo relativo. Ma relativo rispetto cosa? Rispetto alla working directory che definirà l’origine del nostro percorso.Ma è facile immaginare che progetti diversi possano avere diverse working directory. Infatti le cose stanno proprio questo modo: per ciascun progetto dobbiamo specificare una diversa working directory. Per esempio, potremmo avere un progetto relativo ’insegnamento di Psicometria e un progetto relativo alla prova finale.Per organizzaere il lavoro questo modo, si procede come segue. Supponiamo di creare una cartella chiamata psicometria che contiene, al suo interno, le cartelle scripts e data:Queste cartelle conterranno file che ho specificato sopra.Chiudiamo RStudio, se è aperto e lo riapriamo di nuovo. Dal menu selezioniamo File -> New Project… Questo aprirà un altro menu che ci chiederà, tra le altre cose se vogliamo creare un nuovo progetto (New project). Selezioniamo quell’opzione e navighiamo fino alla cartella psicometria e selezioniamo open. Questo creerà un file chiamato psicometria.Rproj nella cartella psicometria.Chiudiamo RStudio. Se vogliamo accedere al progetto “psicometria” dobbiamo cliccare sul file psicometria.Rproj. Questo aprirà RStudio e farà modo che la working directory coincida con la cartalla psicometria. Ogni volta che vogliamo lavorare sui dati del progetto “psicometria” dobbiamo chiudere RStudio (se è già aperto) e riaprirlo cliccando sul file psicometria.Rproj.questo punto possiamo definire l’indirizzo dei file modo relativo – relativo alla cartella psicometria. Per fare questo usiamo le funzionalità del pacchetto . Supponiamo di volere caricare un file di dati che si chiama dati_depressione.txt e si trova nella cartella data contenuta nella cartella psicometria. Per importare questi dati (dopo avere caricato pacchetti rio e ) useremo l’istruzione seguente:altre parole, così facendo specifichiamo il percorso relativo del file dati_depressione.txt. L’istruzione precedente significa che, partendo dalla cartella che coincide con la working directory dobbiamo spostarci nella cartella data e lì dentro troviamo il file chiamato dati_depressione.txt.","code":"psicometria/\n  ├── data\n  ├── scripts\nrio::import(here(\"data\", \"dati_depressione.txt\"))"},{"path":"terminologia.html","id":"terminologia","chapter":"Capitolo 7 Terminologia","heading":"Capitolo 7 Terminologia","text":"","code":""},{"path":"terminologia.html","id":"metodi-e-procedure-della-psicologia","chapter":"Capitolo 7 Terminologia","heading":"7.1 Metodi e procedure della psicologia","text":"Una teoria psicologica di un qualche aspetto del comportamento umano o\ndella mente ha le seguenti proprietà:descrive le caratteristiche del comportamento questione,formula predizioni sulle caratteristiche future del comportamento,è sostenuta da evidenze empiriche,deve essere falsificabile (ovvero, linea di principio, deve\npotere fare delle predizioni su aspetti del fenomeno considerato che\nnon sono ancora noti e che, se venissero indagati, potrebbero\nportare rigettare la teoria, se si dimostrassero incompatibili con\nessa).L’analisi dei dati si riferisce al punto 3 indicato sopra e, nelle sue\nfasi distinte, ovverola misurazione,l’analisi descrittiva,l’inferenza causale,ha un ruolo centrale nello sviluppo delle teorie psicologiche. Prima di\naffrontare il primo degli ambiti cui abbiamo articolato l’analisi dei\ndati, ovvero quello della misurazione, prenderemo qui esame la\nterminologia che viene utilizzata.","code":""},{"path":"terminologia.html","id":"variabili-e-costanti","chapter":"Capitolo 7 Terminologia","heading":"7.2 Variabili e costanti","text":"L’analisi dei dati inizia con l’individuazione delle unità portatrici di\ninformazioni circa il fenomeno di interesse. Si dice popolazione (o\nuniverso) l’insieme \\(\\Omega\\) delle entità capaci di fornire\ninformazioni sul fenomeno oggetto dell’indagine statistica. Possiamo\ndunque scrivere\n\\(\\Omega = \\{\\omega_i\\}_{=1, \\dots, n}= \\{\\omega_1, \\omega_2, \\dots, \\omega_n\\}\\)\noppure \\(\\Omega = \\{\\omega_1, \\omega_2, \\dots \\}\\) nel caso di\npopolazioni finite o infinite, rispettivamente. Gli elementi \\(\\omega_i\\)\ndell’insieme \\(\\Omega\\) sono detti unità statistiche. Un sottoinsieme\ndella popolazione viene chiamato campione. Ciascuna unità statistica\n\\(\\omega_i\\) (abbreviata con u.s.) è portatrice dell’informazione che\nverrà rilevata mediante un’operazione di misurazione.Definiamo variabile statistica la proprietà (o grandezza) che è\noggetto di studio nell’analisi dei dati. Una variabile è una proprietà\ndi un fenomeno che può essere espressa più valori sia numerici sia\ncategoriali. Il termine “variabile” si contrappone al termine “costante”\nche descrive una proprietà invariante di tutte le unità statistiche.Si dice modalità ciascuna delle varianti con cui una variabile\nstatistica può presentarsi. Definiamo insieme delle modalità di una\nvariabile statistica l’insieme \\(M\\) di tutte le possibili espressioni con\ncui la variabile può manifestarsi. Le modalità osservate e facenti parte\ndel campione si chiamano dati (si veda la\nTabella 1.1).Proprietà oggetto di studio, variabile e modalità.","code":""},{"path":"terminologia.html","id":"variabili-indipendenti-e-variabili-dipendenti","chapter":"Capitolo 7 Terminologia","heading":"7.3 Variabili indipendenti e variabili dipendenti","text":"È importante distinguere il concetto di variabile indipendente, che\ndescrive ciò che viene manipolato dallo sperimentatore o che è già\npresente nel campione, dalla variabile dipendente, che descrive ciò\nche varia al variare della variabile indipendente e che viene misurato\nnel campione.","code":""},{"path":"terminologia.html","id":"la-matrice-dei-dati","chapter":"Capitolo 7 Terminologia","heading":"7.4 La matrice dei dati","text":"Le realizzazioni delle variabili esaminate una rilevazione statistica\nvengono organizzate una matrice dei dati. Le colonne della matrice\ndei dati contengono gli insiemi dei dati individuali di ciascuna\nvariabile statistica considerata. Ogni riga della matrice contiene tutte\nle informazioni relative alla stessa unità statistica. Una generica\nmatrice dei dati ha l’aspetto seguente: \\[D_{m,n} = \n \\begin{pmatrix}\n  \\omega_1 & a_{1}   & b_{1}   & \\cdots & x_{1} & y_{1}\\\\\n  \\omega_2 & a_{2}   & b_{2}   & \\cdots & x_{2} & y_{2}\\\\\n  \\vdots   & \\vdots  & \\vdots  & \\ddots & \\vdots & \\vdots  \\\\\n \\omega_n  & a_{n}   & b_{n}   & \\cdots & x_{n} & y_{n}\n \\end{pmatrix}\\] dove, nel caso presente, la prima colonna contiene il\nnome delle unità statistiche, la seconda e la terza colonna si\nriferiscono due mutabili statistiche (variabili categoriali; \\(\\) e\n\\(B\\)) e ne presentano le modalità osservate nel campione mentre le ultime\ndue colonne si riferiscono due variabili statistiche (\\(X\\) e \\(Y\\)) e ne\npresentano le modalità osservate nel campione. Generalmente, tra le\nunità statistiche \\(\\omega_i\\) non esiste un ordine progressivo; l’indice\nattribuito alle unità statistiche nella matrice dei dati si riferisce\nsemplicemente alla riga che esse occupano.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"la-misurazione-in-psicologia","chapter":"Capitolo 8 La misurazione in psicologia","heading":"Capitolo 8 La misurazione in psicologia","text":"Le osservazioni empiriche – osservazioni sul campo, sondaggi o\nrisultati di esperimenti – forniscono il materiale che viene utilizzato\nun’indagine statistica e sono chiamate dati. Quando parliamo di\ndati dobbiamo chiederci: che misura dati che sono stati raccolti\nsono grado di rappresentare maniera veritiera le caratteristiche\ndel fenomeno esaminato? C’è un’intera disciplina che cerca di rispondere\nalla domanda precedente: la “teoria della misurazione.” Senza entrare\nnei dettagli, il presente capitolo intende fornire un’introduzione\ngenerale alle tematiche della misurazione psicologia.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"le-scale-di-misura","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1 Le scale di misura","text":"risultati delle misurazioni, ovvero le variabili, devono avere almeno\ndue valori possibili (altrimenti sarebbero delle costanti). È importante\nnotare che le modalità delle variabili sono grado di descrivere\nl’intensità del fenomeno misurato con livelli diversi di precisione. La\ncapacità delle variabili di “catturare” forma numerica le proprietà\ndel fenomeno che esse rappresentano viene descritta dalla teoria delle\nscale di misura di Stevens (1946). Secondo tale teoria, possiamo distinguere\nquattro scale di misura aventi proprietà diverse: le scale nominali (nominal scales), ordinali (ordinal scales), intervalli (interval scales), di\nrapporti (ratio scales).","code":""},{"path":"la-misurazione-in-psicologia.html","id":"scala-nominale","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1.1 Scala nominale","text":"La scala nominale raggruppa dati categorie qualitative mutuamente\nesclusive (cioè nessun dato si può collocare più di una categoria).Esiste la sola relazione di equivalenza tra le misure delle u.s., cioè\nnella scala nominale gli elementi del campione appartenenti classi\ndiverse sono differenti, mentre tutti quelli della stessa classe sono\ntra loro equivalenti: \\(x_i = x_j\\) oppure \\(x_i \\neq x_j\\). È ammessa\nl’operazione del conteggio delle u.s. presenti ogni categoria e il\nconteggio delle classi di equivalenze, dunque la descrizione dei dati\navviene tramite le frequenze assolute e le frequenze relative.partire da una scala nominale è possibile costruire altre scale\nequivalenti trasformando valori della scala di partenza modo tale\nda cambiare nomi delle modalità ma lasciando però inalterata la\nsuddivisione u.s. nelle medesime classi di equivalenza. Questo significa\nche prendendo una variabile misurata su scala nominale e cambiando \nnomi delle sue categorie otteniamo una nuova variabile esattamente\ncorrispondente alla prima.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"scala-ordinale","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1.2 Scala ordinale","text":"La scala ordinale conserva la proprietà della scala nominale di\nclassificare ciascun dato ’interno di una sola categoria, ma alla\nrelazione di equivalenza tra elementi di una stessa classe aggiunge la\nrelazione di ordinamento tra le varie classi di equivalenza. Essendo\nbasata su una relazione d’ordine, una scala ordinale descrive soltanto\nl’ordine di rango tra le modalità, ma non dà alcuna informazione su\nquanto una modalità sia più grande di un’altra. Non ci dice, per\nesempio, se la distanza tra le modalità \\(\\) e \\(b\\) sia uguale, maggiore o\nminore della distanza tra le modalità \\(b\\) e \\(c\\).","code":""},{"path":"la-misurazione-in-psicologia.html","id":"scala-ad-intervalli","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1.3 Scala ad intervalli","text":"La scala ad intervalli include le proprietà di quella nominale e di\nquella ordinale, e più consente di misurare le distanze tra le coppie\ndi u.s. nei termini di un intervallo costante, chiamato unità di\nmisura, cui viene attribuito il valore “1.” La posizione dell’origine\ndella scala, cioè il punto zero, è scelta arbitrariamente, nel senso che\nnon indica l’assenza della quantità che si sta misurando. Avendo uno\nzero arbitrario, questa scala di misura consente valori negativi. Lo\nzero, infatti, non viene attribuito ’u.s. cui la proprietà\nmisurata risulta assente.La scala intervalli equivalenti ci consente di effettuare operazioni\nalgebriche basate sulla differenza tra numeri associati ai diversi\npunti della scala, operazioni algebriche non era possibile eseguire nel\ncaso di misure livello di scala ordinale o nominale. Il limite della\nscala ad intervalli è quello di non consentire il calcolo del rapporto\ntra coppie di misure. Possiamo dire, per esempio, che la distanza tra\n\\(\\) e \\(b\\) è la metà della distanza tra \\(c\\) e \\(d\\). Oppure che la distanza\ntra \\(\\) e \\(b\\) è uguale alla distanza tra \\(c\\) e \\(d\\). Non possiamo dire,\nperò, che \\(\\) possiede la proprietà misurata quantità doppia rispetto\n\\(b\\). Non possiamo cioè stabilire dei rapporti diretti tra le misure\nottenute. Solo per le differenze tra le modalità sono dunque permesse\ntutte le operazioni aritmetiche: le differenze possono essere tra loro\nsommate, elevate potenza oppure divise, determinando così le quantità\nche stanno alla base della statistica inferenziale.Nelle scale ad intervalli equivalenti, l’unità di misura è arbitraria,\novvero può essere cambiata attraverso una dilatazione, operazione che\nconsiste nel moltiplicare tutti valori della scala per una costante\npositiva. Poiché l’aggiunta di una costante non altera le differenze tra\nvalori della scala, è anche ammessa la traslazione, operazione che\nconsiste nel sommare una costante tutti valori della scala. Essendo\nla scala invariate rispetto alla traslazione e alla dilatazione, le\ntrasformazioni ammissibili sono le trasformazioni lineari:\n\\[y' = + , \\quad b > 0.\\]\nL’aspetto che rimane invariante seguito di una trasformazione lineare\nè l’uguaglianza dei rapporti fra intervalli.Esempio 8.2  (Temperatura)  Esempio di scala ad intervalli è la temperatura misurata gradi\nCelsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è\npossibile stabilire se due modalità sono uguali o diverse: 30\\(^\\circ\\)C\n\\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale è possibile mettere due\nmodalità una relazione d’ordine: 30\\(^\\circ\\)C \\(>\\) 20\\(^\\circ\\)C. \naggiunta ai casi precedenti, però, è possibile definire una unità di\nmisura per cui è possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c’è\nuna differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. valori di\ntemperatura, oltre poter essere ordinati secondo l’intensità del\nfenomeno, godono della proprietà che le differenze tra loro sono\ndirettamente confrontabili e quantificabili.Il limite della scala ad intervalli è quello di non consentire il\ncalcolo del rapporto tra coppie di misure. Ad esempio, una temperatura\ndi 80\\(^\\circ\\)C non è il doppio di una di 40\\(^\\circ\\)C. Se infatti\nesprimiamo le stesse temperature nei termini della scala Fahrenheit,\nallora due valori non saranno rapporto di 1 2 tra loro. Infatti,\n20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa\nche la relazione “il doppio di” che avevamo individuato precedenza si\napplicava ai numeri della scala centigrada, ma non alla proprietà\nmisurata (cioè la temperatura). La decisione di che scala usare\n(Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non\ndeve influenzare le inferenze che traiamo dai dati. Queste inferenze,\ninfatti, devono dirci qualcosa proposito della realtà empirica e non\npossono nessun modo essere condizionate dalle nostre scelte\narbitrarie che ci portano scegliere la scala Centigrada piuttosto che\nquella Fahrenheit.Consideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo esame, ad esempio, tre temperature:\n\\(20^\\circ C = 68^\\circ F\\),\n\\(15^\\circ C = 59^\\circ F\\),\n\\(10^\\circ C = 50 ^\\circ F\\).","code":""},{"path":"la-misurazione-in-psicologia.html","id":"scala-di-rapporti","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1.4 Scala di rapporti","text":"Nella scala rapporti equivalenti la posizione dello zero non è\narbitraria, ma corrisponde ’elemento dotato di intensità nulla\nrispetto alla proprietà misurata. Una scala rapporti equivalenti si\ncostruisce associando il numero 0 ’elemento con intensità nulla;\nviene poi scelta un’unità di misura \\(u\\) e, ad ogni elemento, si assegna\nun numero \\(\\) definito come: \\[= \\frac{d}{u}\\] dove \\(d\\) rappresenta la\ndistanza dall’origine. Alle u.s. vengono dunque assegnati dei numeri\ntali per cui le differenze e rapporti tra numeri riflettono le\ndifferenze e rapporti tra le intensità della proprietà misurata.Operazioni aritmetiche sono possibili non solo sulle differenze tra \nvalori della scala (come per la scala intervalli equivalenti), ma\nanche sui valori stessi della scala. L’unica arbitrarietà riguarda\nl’unità di misura che si utilizza. L’unità di misura può cambiare, ma\nqualsiasi unità di misura si scelga, lo zero deve sempre indicare\nl’intensità nulla della proprietà considerata.Le trasformazioni ammissibili questo livello di scala sono dette\ntrasformazioni di similarità: \\[y' = , \\quad b > 0.\\] questo livello\ndi scala, seguito delle trasformazioni ammissibili, rimangono\ninvariati anche rapporti: \\[\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}.\\]","code":""},{"path":"la-misurazione-in-psicologia.html","id":"gerarchia-dei-livelli-di-scala-di-misura","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.1.5 Gerarchia dei livelli di scala di misura","text":"Stevens (1946) parla di livelli di scala poiché quattro tipi di scala di\nmisura stanno una precisa gerarchia: la scala nominale rappresenta il\nlivello più basso della misurazione, la scala rapporti equivalenti è\ninvece il livello più alto.Passando da un livello di misurazione ad uno\npiù alto aumenta il numero di operazioni aritmetiche che possono essere\ncompiute sui valori della scala, come indicato nella figura seguente.\nFigura 8.1: quattro livelli di scala secondo Stevens (1946).\nPer ciò che riguarda le trasformazioni ammissibili, più il livello di\nscala è basso, più le funzioni sono generali (sono minori cioè vincoli\nper passare da una rappresentazione numerica ad un’altra equivalente).\nSalendo la gerarchia, la natura delle funzioni di trasformazione si fa\npiù restrittiva.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"sec:DiscreteVsContinuous","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.2 Variabili discrete vs. variabili continue","text":"Le variabili livello di intervalli e di rapporti possono essere\ndiscrete o continue. Le variabili discrete possono assumere alcuni\nvalori ma non altri. Una volta che l’elenco di valori accettabili è\nstato specificato, non ci sono casi che cadono tra questi valori.\nLe variabili\ndiscrete di solito assumono valori interi.Quando una variabile può assumere qualsiasi valore entro un intervallo\nspecificato, allora si dice che la variabile è continua. teoria, ciò\nsignifica che frazioni e decimali possono essere utilizzati per\nraggiungere un livello di precisione qualsiasi. pratica, un certo\npunto dobbiamo arrotondare numeri, rendendo tecnicamente la variabile\ndiscreta. variabili veramente discrete, tuttavia, non è possibile\naumentare piacimento il livello di precisione della misurazione.\nFigura 8.2: Le variabili continue possono assumere un insieme continuo di valori, al contrario delle variabili discrete, per le quali l’insieme dei possibili valori ha cardinalità al più numerabile.\n","code":""},{"path":"la-misurazione-in-psicologia.html","id":"perché-alcune-misurazioni-sono-migliori-di-altre","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.3 Perché alcune misurazioni sono migliori di altre?","text":"psicologia, ciò che vogliamo misurare non è una caratteristica\nfisica, ma invece è un concetto teorico inosservabile, ovvero un\ncostrutto.Un costrutto rappresenta il risultato di una fondata riflessione scientifica, non è per definizione accessibile ’osservazione diretta, ma viene inferito dall’osservazione di opportuni indicatori (Sartori, 2005).Ad esempio, supponiamo che un docente voglia valutare quanto bene uno studente comprenda la distinzione tra le quattro diverse scale di misura che sono state descritte sopra. Il docente potrebbe predisporre un test costituito da un insieme di domande e potrebbe\ncontare quante domande lo studente risponde correttamente. Questo\ntest, però, può o può non essere una buona misura del costrutto relativo\nalla conoscenza effettiva delle quattro scale di misura. Per esempio, se\nil docente scrive le domande del test modo ambiguo o se usa una\nlinguaggio troppo tecnico che lo studente non conosce, allora \nrisultati del test potrebbero suggerire che lo studente non conosce la\nmateria questione anche se realtà questo non è vero. D’altra\nparte, se il docente prepara un test scelta multipla con risposte\nerrate molto ovvie, allora lo studente può ottenere dei buoni risultati\nal test anche senza essere grado di comprendere adeguatamente le\nproprietà delle quattro scale di misura.generale non è possibile misurare un costrutto senza una certa\nquantità di errore. Poniamoci dunque il problema di determinare che\nmodo una misurazione possa dirsi adeguata.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"sec:accuratezza_precisione","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.3.1 Tipologie di errori","text":"L’errore è, per definizione, la differenza tra il valore vero e il\nvalore misurato della grandezza esame. Gli errori sono classificati\ncome sistematici (o determinati) e casuali (o indeterminati). Gli errori\ncasuali sono fluttuazioni, eccesso o difetto rispetto al valore\nreale, delle singole determinazioni e sono dovuti alle molte variabili\nincontrollabili che influenzano ogni misura psicologica. Gli errori\nsistematici, invece, influiscono sulla misurazione sempre nello stesso\nsenso e, solitamente, per una stessa quantità (possono essere additivi o\nproporzionali).Le differenze tra le due tipologie di errori, sistematici e casuali,\nintroducono concetti di accuratezza e di precisione della misura. Una\nmisura viene definita:accurata, quando vi è un accordo tra la misura effettuata ed il\nvalore reale;precisa quando, ripetendo più volte la misura, risultati\nottenuti sono concordanti, cioè differiscono maniera irrilevante\ntra loro.La metafora del tiro bersaglio illustra la relazione tra precisione e accuratezza.\nFigura 8.3: Illustrazione dei concetti di precisione e accuratezza: () bassa precisione e bassa accuratezza, (b) bassa precisione e alta accuratezza, (c) alta precisione e bassa accuratezza, (d) alta precisione e alta accuratezza.\nPer tenere sotto controllo l’incidenza degli errori, sono stati\nintrodotti psicologia concetti di attendibilità e validità:l’attendibilità esprime il grado di accordo o coerenza tra\nmisurazioni indipendenti dello stesso costrutto psicologico[^1];la validità descrive il grado cui uno strumento misura ciò che\ndice di misurare.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"sec:reliability","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.3.2 Attendibilità","text":"Uno strumento si dice attendibile quando valuta modo coerente e\nstabile la stessa variabile: risultati ottenuti si mantengono costanti\ndopo ripetute somministrazione ed assenza di variazioni psicologiche\ne fisiche dei soggetti sottoposti al test o cambiamenti dell’ambiente \ncui ha luogo la somministrazione.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"sec:validity","chapter":"Capitolo 8 La misurazione in psicologia","heading":"8.3.3 Validità","text":"L’attendibilità di uno strumento non è sufficiente: primo luogo uno\nstrumento di misura deve essere valido, laddove la validità rappresenta\nil grado cui uno strumento misura effettivamente ciò che dovrebbe\nmisurare. genere, si fa riferimento ad almeno quattro tipi di\nvalidità.La validità di costrutto riguarda il grado cui un test misura\nciò per cui è stato costruito. Essa si suddivide : validità\nconvergente e validità divergente. La validità convergente fa\nriferimento alla concordanza tra uno strumento e un altro che misura\nlo stesso costrutto. La validità divergente, al contrario, valuta il\ngrado di discriminazione tra strumenti che misurano costrutti\ndifferenti. Senza validità di costrutto le altre forme di validità\nnon hanno senso.base alla validità di contenuto, un test fornisce una misura\nvalida di un attributo psicologico se il dominio dell’attributo è\nrappresentato maniera adeguata dagli item del test. Un requisito\ndi base della validità di contenuto è la rilevanza e la\nrappresentatività del contenuto degli item riferimento\n’attributo che il test intende misurare.La validità di criterio valuta il grado di concordanza tra \nrisultati dello strumento considerato e risultati ottenuti da\naltri strumenti che misurano lo stesso costrutto, o tra risultati\ndello strumento considerato e un criterio esterno. Nella validità\nconcorrente, costrutto e criterio vengono misurati contestualmente,\nconsentendo un confronto immediato. Nella validità predittiva, il\ncostrutto viene misurato prima e il criterio un momento\nsuccessivo, consentendo la valutazione della capacità dello\nstrumento di predire un evento futuro.Infine, la validità di facciata fa riferimento al grado cui il\ntest appare valido ai soggetti cui esso è diretto. La validità di\nfacciata è importante ambiti particolari, quali ad esempio la\nselezione del personale per una determinata occupazione. questo\ncaso è ovviamente importante che chi si sottopone al test ritenga\nche il test vada misurare quegli aspetti che sono importanti per\nle mansioni lavorative che dovranno essere svolte, piuttosto che\naltre cose. generale, la validità di facciata non è utile, tranne\ncasi particolari.","code":""},{"path":"la-misurazione-in-psicologia.html","id":"conclusioni","chapter":"Capitolo 8 La misurazione in psicologia","heading":"Conclusioni","text":"Una domanda che uno psicologo spesso si pone è: “sulla base delle\nevidenze osservate, possiamo concludere dicendo che l’intervento\npsicologico è efficace nel trattamento e nella cura del disturbo?” Le\nconsiderazioni svolte questo capitolo dovrebbero farci capire che,\nprima di cercare di rispondere questa domanda con l’analisi statistica\ndei dati, devono essere affrontati problemi della validità e\ndell’attendibilità delle misure (oltre stabilire l’appropriato livello\ndi scala di misura delle osservazioni). L’attendibilità è un\nprerequisito della validità. Se gli errori di misurazione sono troppo\ngrandi, dati sono inutili. Inoltre, uno strumento di misurazione può\nessere preciso ma non valido. La validità e l’attendibilità delle\nmisurazioni sono dunque entrambe necessarie.generale, l’attendibilità e la validità delle misure devono essere\nvalutate per capire se dati raccolti da un ricercatore siano adeguati\n(1) per fornire una risposta alla domanda della ricerca, e (2) per\ngiungere alla conclusione proposta dal ricercatore alla luce dei\nrisultati dell’analisi statistica che è stata eseguita. È chiaro che le\ninformazioni fornite questo capitolo si limitano scalfire la\nsuperficie di questi problemi. concetti qui introdotti, però, devono\nsempre essere tenuti mente e costituiscono il fondamento di quanto\nverrà esposto nei capitoli successivi.","code":""},{"path":"statistica-descrittiva.html","id":"statistica-descrittiva","chapter":"Capitolo 9 Statistica descrittiva","heading":"Capitolo 9 Statistica descrittiva","text":"Nel 1907 Francis Galton, cugino di Charles Darwin, matematico e\nstatistico autodidatta, geografo, esploratore, teorico della\ndattiloscopia (ovvero, dell’uso delle impronte digitali fini\nidentificativi) e dell’eugenetica, scrisse una lettera alla rivista\nscientifica Nature sulla sua visita alla Fat Stock Poultry\nExhibition di Plymouth. Lì vide alcuni membri del pubblico partecipare\nad un gioco il cui scopo era quello di indovinare il peso della carcassa\ndi un grande bue che era appena stato scuoiato. Galton si procurò 787\ndei biglietti che erano stati compilati dal pubblico e considerò il\nvalore medio di 547 kg come la “scelta democratica” dei partecipanti, \nquanto “ogni altra stima era stata giudicata troppo alta o troppo bassa\ndalla maggioranza dei votanti.” Il punto interessante è che il peso\ncorretto di 543 kg si dimostrò essere molto simile alla “scelta\ndemocratica” basata sulle stime dei 787 partecipanti. Galton intitolò la\nsua lettera Nature Vox Populi (voce del popolo), ma questo processo\ndecisionale è ora meglio conosciuto come la “saggezza delle folle”\n(wisdom crowds). Possiamo dire che, nel suo articolo del 1907,\nGalton effettuò quello che ora chiamiamo un riepilogo dei dati, ovvero\ncalcolò un indice sintetico partire da un insieme di dati. questo\ncapitolo esamineremo le tecniche che sono state sviluppate nel secolo\nsuccessivo per riassumere le grandi masse di dati con cui sempre più\nspesso ci dobbiamo confrontare. Vedremo come calcolare e interpretare\ngli indici di posizione e di dispersione, discuteremo le distribuzioni\ndi frequenze e le relazioni tra variabili. Vedremo inoltre quali sono le\ntecniche di visualizzazione che ci consentono di rappresentare questi\nsommari dei dati mediante dei grafici. Ma prima di entrare nei dettagli,\nprendiamoci un momento per capire perché abbiamo bisogno della\nstatistica descrittiva.","code":""},{"path":"statistica-descrittiva.html","id":"perché-riassumere-i-dati","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.1 Perché riassumere i dati?","text":"Quando riassumiamo dati, necessariamente buttiamo via delle\ninformazioni. Ma è una buona idea procedere questo modo? Non sarebbe\nmeglio conservare le informazioni specifiche di ciascun soggetto che\npartecipa ad un esperimento psicologico, al di là di ciò che viene\ntrasmesso dagli indici riassuntivi della statistica descrittiva? Che\ndire delle informazioni che descrivono come sono stati raccolti dati,\ncome l’ora del giorno o l’umore del partecipante? Tutte queste\ninformazioni vengono perdute quando riassumiamo dati. La risposta alla\ndomanda che ci siamo posti è che, generale, non è una buona idea\nconservare tutti dettagli di ciò che sappiamo. È molto più utile\nriassumere le informazioni perché la semplificazione risultante consente\nprocessi di generalizzazione.un contesto letterario, l’importanza della generalizzazione è stata\nsottolineata da Jorge Luis Borges nel suo racconto “Funes o della\nmemoria,” che descrive un individuo che perde la capacità di\ndimenticare. Borges si concentra sulla relazione tra generalizzazione e\npensiero:Pensare è dimenticare una differenza, generalizzare, astrarre. Nel mondo troppo pieno di Funes, c’erano solo dettagli.Come possiamo ben capire, la vita di Funes non è facile. Se facciamo\nriferimento alla psicologia possiamo dire che gli psicologi hanno\nstudiato lungo l’utilità della generalizzazione per il pensiero. Un\nesempio è fornito dal fenomeno della formazione dei concetti e lo\npsicologo che viene mente questo proposito è sicuramente Eleanor\nRosch, la quale ha studiato principi di base della categorizzazione. \nconcetti ci forniscono uno strumento potente per organizzare le\nconoscenze. Noi siamo grado di riconoscere facilmente diversi\nesemplare di un concetto – per esempio, “gli uccelli” – anche se \nsingoli esemplari che fanno parte di una categoria sono molto diversi\ntra loro (l’aquila, la gallina, il pettirosso). L’uso dei concetti, cioè\nla generalizzazione, è utile perché ci consente di fare previsioni sulle\nproprietà dei singoli esemplari che appartengono ad una categoria, anche\nse non abbiamo mai avuto esperienza diretta con essi – per esempio,\npossiamo fare la predizione che tutti gli uccelli possono volare e\nmangiare vermi, ma non possono guidare un’automobile o parlare \ninglese. Queste previsioni non sono sempre corrette, ma sono utili.Le statistiche descrittive, un certo senso, ci fornisco l’analogo dei\n“prototipi” che, secondo Eleanor Rosch, stanno alla base del processo\npsicologico di creazione dei concetti. Un prototipo è l’esemplare più\nrappresentativo di una categoria. maniera simile, una statistica\ndescrittiva come la media, ad esempio, potrebbe essere intesa come\nl’osservazione “tipica.”La statistica descrittiva ci fornisce gli strumenti per riassumere \ndati che abbiamo disposizione una forma visiva o numerica. Le\nrappresentazioni grafiche più usate della statistica descrittiva sono\ngli istogrammi, diagrammi dispersione o box-plot, e gli indici\nsintetici più comuni sono la media, la mediana, la varianza e la\ndeviazione standard.","code":""},{"path":"statistica-descrittiva.html","id":"distribuzioni-di-frequenze","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.2 Distribuzioni di frequenze","text":"Per introdurre principali strumenti della statistica descrittiva\nconsidereremo qui dati raccolti da Zetsche et al. (2019). Questi autori\nhanno studiato le aspettative negative le quali sono state evidenziate\ncome un meccanismo chiave nel mantenimento e nella reiterazione della\ndepressione. Zetsche et al. (2019) hanno valutato le aspettative di\nindividui depressi circa il loro umore futuro ed si sono chiesti se\nqueste aspettative fossero accurate oppure distorte negativamente.uno degli studi descritti viene esaminato un campione costituito da 30\nsoggetti con almeno un episodio depressivo maggiore e da 37 controlli\nsani. Gli autori hanno misurato il livello depressivo con il Beck\nDepression Inventory (BDI-II). Ma qual è la la gravità della\ndepressione riportata dai soggetti nel campione esaminato da\nZetsche et al. (2019)?Dei 67 soggetti considerati, uno non ha completato il BDI-II e quindi abbiamo disposizione 66 valori del BDI-II.\ndati sono riportati nella tabella [tab:bdi2_values].\nPer semplicità dati sono stati ordinati \nordine crescente. È chiaro che dati grezzi sono di difficile lettura.\nPoniamoci dunque il problema di creare una rappresentazione sintetica e\ncomprensibile di questo insieme di valori.Uno dei modi che ci consentono di effettuare una sintesi dei dati è\nquello di generare una distribuzione di frequenze.\nUna distribuzione di frequenze è un riepilogo del conteggio della\nfrequenza con cui le modalità osservate un insieme di dati si\nverificano un intervallo di valori.Per creare una distribuzione di frequenze possiamo procedere effettuando\nuna partizione delle modalità della variabile di interesse \\(m\\) classi\n(denotate con \\(\\Delta_i\\)) tra loro disgiunte. tale partizione, la\nclasse \\(\\)-esima coincide con un intervallo di valori aperto destra\n\\([a_i, b_i)\\) o aperto sinistra \\((a_i, b_i]\\). Ad ogni classe\n\\(\\Delta_i\\) avente \\(a_i\\) e \\(b_i\\) come limite inferiore e superiore\nassociamo l’ampiezza \\(b_i - a_i\\) (non necessariamente uguale per ogni\nclasse) e il valore centrale \\(\\bar{x}_i\\). La scelta delle classi è\narbitraria, ma è buona norma non definire classi con un numero troppo\npiccolo (< 5) di osservazioni. Poiché ogni elemento dell’insieme\n\\(\\{x_i\\}_{=1}^n\\) appartiene ad una ed una sola classe \\(\\Delta_i\\),\npossiamo calcolare le quantità elencate di seguito.La frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).La frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).La frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe. Proprietà: \\(f_1+f_2+\\dots+f_m =1\\).La frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe. Proprietà: \\(f_1+f_2+\\dots+f_m =1\\).La frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(\\)-esima compresa: \\(N_i = \\sum_{=1}^m n_i.\\)La frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(\\)-esima compresa: \\(N_i = \\sum_{=1}^m n_i.\\)La frequenza cumulata relativa \\(F_i\\), ovvero\n\\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{=1}^m f_i.\\)La frequenza cumulata relativa \\(F_i\\), ovvero\n\\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{=1}^m f_i.\\)Calcoliamo ora la distribuzione di frequenza assoluta e la distribuzione di frequenza relativa per valori del BDI-II del campione clinico di Zetsche et al. (2019).\nPer costruire una distribuzione di frequenza è innanzitutto necessario scegliere gli\nintervalli delle classi. Facendo riferimento ai cut-usati per l’interpretazione del BDI-II, definiamo seguenti intervalli aperti destra:depressione minima: [0, 13.5),depressione lieve: [13.5, 19.5),depressione moderata: [19.5, 28.5),depressione severa: [28.5, 63).La distribuzione di frequenza della variabile bdi2 è riportata nella\ntabella seguente. Questa distribuzione di frequenza ci aiuta capire meglio cosa sta succedendo. Se consideriamo la frequenza relativa, ad esempio, possiamo notare che ci sono due valori maggiormente ricorrenti e tali valori corrispondono alle due classi più estreme. Questo ha senso nel caso presente, quanto il campione esaminato da Zetsche et al. (2019) includeva due gruppi di\nsoggetti: soggetti sani (con valori BDI-II bassi) e soggetti depressi\n(con valori BDI-II alti). una distribuzione di frequenza tali valori\ntipici vanno sotto il nome di mode della distribuzione.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.2.1 Esercizio con R","text":"Poniamoci ora il problema di costruire la tabella precedente partendo dai dati grezzi messi disposizione da Zetsche et al. (2019).\nLeggiamo dati assumendo che il file data.mood.csv si trovi nella cartella data contenuta nella working directory.C’è un solo valore di depressione per ciascun soggetto ma tale valore viene ripetuto tante volte quante volte sono le righe del data.frame associate ad ogni soggetto (ciascuna riga corrispondente ad una prova diversa). È dunque necessario trasformare il data.frame modo tale da avere un’unica riga per ciascun soggetto, ovvero un unico valore BDI-II per soggetto.Ci sono dunque 66 soggetti quali hanno ottenuto valori sulla scala del BDI-II stampati di seguito (li presento ordinati dal più piccolo al più grande).Calcolo ora le frequenze assolute per seguenti intervalli aperti destra: [0, 13.5), [13.5, 19.5), [19.5, 28.5), [28.5, 63). Esaminando dati, possiamo notare che 36 soggetti cadono nella prima classe. È però necessario eseguire quest’operazione di conteggio utilizzando R.Uno dei modi possibili per calcolare le frequenze assolute è quello di usare la funzione cut(). Mediante tal funzione è possibile dividere il campo di variazione (ovvero, la differenza tra il valore massimo di una distribuzione ed il valore minimo) di una variabile continua x intervalli e codificare ciascun valore x nei termini dell’intervallo cui appartiene. Tale risultato si ottiene nel modo seguente.Possiamo ora usare la funzione table() la quale ritorna un elenco che associa la frequenza assoluta ciascuna modalità della variabile input – ovvero, la distribuzione di frequenza assoluta.Per ottenere la distribuzione di frequenza relativa è sufficiente dividere ciascuna frequenza assoluta per il numero totale di osservazioni:questo modo abbiamo ottenuto le distribuzioni di frequenza assoluta e relativa per valori del BDI-II dei soggetti di Zetsche et al. (2019):","code":"\ndf <- read.csv(\n  here(\"data\", \"data.mood.csv\"), \n  header=TRUE\n) \nbysubj <- df %>% \n  group_by(esm_id) %>% \n  summarise(\n    bdi = mean(bdi)\n  ) %>% \n  na.omit()\nsort(bysubj$bdi)\n#>  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  2  2  2  2\n#> [30]  3  3  3  5  7  9 12 19 22 22 24 25 25 26 26 26 27 27 28 28 30 30 30 31 31 33 33 34 35\n#> [59] 35 35 36 39 41 43 43 44\nbysubj$bdi_level <- cut(\n  bysubj$bdi,\n  breaks = c(0, 13.5, 19.5, 28.5, 63),\n  include.lowest = TRUE,\n  labels = c(\n    \"minimal\", \"mild\", \"moderate\", \"severe\"\n  )\n)\n\nbysubj$bdi_level\n#>  [1] moderate severe   severe   moderate severe   severe   severe   severe   moderate\n#> [10] severe   moderate mild     severe   minimal  minimal  minimal  severe   moderate\n#> [19] minimal  minimal  minimal  minimal  minimal  moderate minimal  minimal  minimal \n#> [28] minimal  minimal  minimal  minimal  severe   minimal  minimal  severe   minimal \n#> [37] moderate minimal  minimal  minimal  severe   minimal  minimal  severe   severe  \n#> [46] moderate severe   severe   minimal  moderate minimal  moderate severe   moderate\n#> [55] moderate minimal  minimal  minimal  minimal  minimal  minimal  minimal  minimal \n#> [64] minimal  minimal  minimal \n#> Levels: minimal mild moderate severe\ntable(bysubj$bdi_level)\n#> \n#>  minimal     mild moderate   severe \n#>       36        1       12       17\ntable(bysubj$bdi_level) / sum(table(bysubj$bdi_level))\n#> \n#>    minimal       mild   moderate     severe \n#> 0.54545455 0.01515152 0.18181818 0.25757576"},{"path":"statistica-descrittiva.html","id":"istogramma","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.3 Istogramma","text":"dati che sono stati sintetizzati una distribuzione di frequenze\npossono essere rappresentati graficamente un istogramma.\nUn istogramma si costruisce riportando sulle ascisse limiti delle\nclassi \\(\\Delta_i\\) e sulle ordinate valori della funzione costante \ntratti\n\\[\\varphi_n(x)= \\frac{f_i}{b_i-a_i}, \\quad x\\\\Delta_i,\\, =1, \\dots, m\\]\nche misura la densità della frequenza relativa della variabile \\(X\\)\nnella classe \\(\\Delta_i\\), ovvero il rapporto fra la frequenza relativa\n\\(f_i\\) e l’ampiezza (\\(b_i - a_i\\)) della classe. questo modo il\nrettangolo dell’istogramma associato alla classe \\(\\Delta_i\\) avrà un’area\nproporzionale alla frequenza relativa \\(f_i\\). Si noti che l’area totale\ndell’istogramma delle frequenze relative è data della somma delle aree\ndei singoli rettangoli e quindi vale 1.0.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-1","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.3.1 Esercizio con R","text":"Poniamoci il problema di costruire un istogramma per dati del BDI-II.\nNell’istogramma viene rappresentata la frequenza relativa delle classi: l’area di ogni barra dell’istogramma è proporzionale alla frequenza relativa della classe che la barra rappresenta.\nCome si trova l’altezza delle barre dell’istogramma? Per la classe [0, 13.5), ad esempio, la frequenza relativa è 36/66. Tale valore corrisponde ’area del rettangolo. Dato che la base del rettangolo è 13.5, l’altezza sarà 36/66 / 13.5, ovvero 0.0404. E così via per le altre barre dell’istogramma.Una rappresentazione grafica dell’istogramma delle frequenze relative si può ottenere con R utilizzando le funzioni di ggplot2. Il pacchetto ggplot2 è un potente strumento per rappresentare graficamente dati. Le iniziali del nome, gg, si riferiscono alla ‘’Grammar Graphics’’, che è un modo di pensare le figure come una serie di layer stratificati. Originariamente descritta da Leland Wilkinson, la grammatica dei grafici è stata aggiornata e applicata R da Hadley Wickham, il creatore del pacchetto. Per chiarezza, precisiamo che la funzione ggplot() utilizza intervalli aperti destra.\nFigura 9.1: Istogramma per valori BDI-II riportati da Zetsche et al. (2019).\nCon quattro intervalli individuati dai cut-del BDI-II otteniamo la\nrappresentazione riportata nella figura 9.1. Nel caso della prima barra dell’istogramma sinistra, l’ampiezza dell’intervallo è pari 13.5 e\nl’area della barra (ovvero, la frequenza relativa) è uguale 36/66.\nDunque l’altezza della barra è uguale (36 / 66) / 13.5 = 0.040. Lo\nstesso procedimento si applica per il calcolo dell’altezza degli altri\nrettangoli.Anche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale.\nQuesto è il caso dell’istogramma seguente il quale è stato generato partire dagli stessi dati.\nFigura 9.2: Una rappresentazione più comune per l’istogramma dei valori BDI-II di Zetsche et al. (2019) nella quale gli intervalli delle classi hanno ampiezze uguali.\n","code":"\np1 <- bysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1) # il valore BDI-II massimo è 44\n  ) +\n  scale_x_continuous(breaks=c(0, 13.5, 19.5, 28.5, 44.1)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  ) +\n  theme_apa()\np1\np2 <- bysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  scale_x_continuous(breaks=c(0.00,  7.35, 14.70, 22.05, 29.40, 36.75, 44.10)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequanza\"\n  ) +\n  theme_apa()\np2"},{"path":"statistica-descrittiva.html","id":"funzione-di-densità-empirica","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.4 Funzione di densità empirica","text":"Il confronto tra le figure 9.1 e 9.2 rende chiaro un limite degli istogrammi. È infatti ovvio che il profilo dell’istogramma è arbitrario: seconda del numero e dei limiti delle classi che vengono scelte,\ncambiano sia il numero che la forma delle barre dell’istogramma. Questo rende difficile fornire un’interpretazione alle informazioni fornite da un istogramma.Il problema precedente può essere alleviato utilizzando una\nrappresentazione alternativa della distribuzione di frequenza, ovvero la\nstima della densità della frequenza dei dati (detta anche stima kernel\ndi densità). Un modo semplice per pensare tale rappresentazione, che\ninglese va sotto il nome di density plot, è quello di immaginare un\ngrande campione di dati, modo che diventi possibile definire un\nenorme numero di classi di equivalenza di ampiezza molto piccola, le\nquali non risultino vuote. tali circostanze, la funzione di densità\nempirica non è altro che il profilo lisciato dell’istogramma. La\nstessa idea si applica anche quando il campione è più piccolo.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-2","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.4.1 Esercizio con R","text":"Nel caso dei dati del BDI-II otteniamo la reppresentazione fornita dalla figura seguente.\nFigura 9.3: Funzione di densità empirica per valori BDI-II di Zetsche et al. (2019).\nChe interpretazione possiamo attribuire alla funzione di densità empirica rappresentata nella figura 9.3?\nLa interpretiamo come abbiamo fatto con gli istogrammi: l’area sottesa al grafico della funzione di densità empirica un certo intervallo rappresenta la proporzione dei casi della distribuzione che hanno valori compresi nell’intervallo considerato.","code":"\np3 <- bysubj %>% \n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..), \n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  geom_density(\n    aes(x = bdi), \n    adjust = 0.5, \n    size = 0.8, \n    fill = \"steelblue3\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  ) +\n  theme_apa()\np3"},{"path":"statistica-descrittiva.html","id":"forma-di-una-distribuzione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.5 Forma di una distribuzione","text":"generale, la forma di una distribuzione descrive come dati si\ndistribuiscono intorno ai valori centrali. Distinguiamo tra\ndistribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali\no multimodali. Un’illustrazione grafica è fornita nella\nfigura seguente.\nFigura 9.4: 1: Asimmetria negativa. 2: Asimmetria positiva. 3: Distribuzione unimodale. 4: Distribuzione bimodale.\nNel pannello 1 la distribuzione è unimodadle con asimmetria negativa; nel pannello 2 la distribuzione è unimodadle con asimmetria positiva; nel pannello 3 la distribuzione è simmetrica e unimodale; nel pannello 4 la distribuzione è bimodale.Se consideriamo nuovamente la figura 9.3 possiamo dire che la distribuzione dei valori del BDI-II nel campione considerato da Zetsche et al. (2019) è bimodale.","code":""},{"path":"statistica-descrittiva.html","id":"indici-di-posizione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.6 Indici di posizione","text":"","code":""},{"path":"statistica-descrittiva.html","id":"quantili","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.6.1 Quantili","text":"La descrizione della distribuzione dei valori BDI-II di\nZetsche et al. (2019) può essere facilitata dalla determinazione di\nalcuni valori caratteristici che sintetizzano le informazioni contenute\nnella distribuzione di frequenze. Si dicono quantili (o frattili)\nquei valori caratteristici che hanno le seguenti proprietà. quartili\nsono quei valori che ripartiscono dati \\(x_i\\) quattro parti\nugualmente numerose (pari ciascuna al 25% del totale). Il primo\nquartile, \\(q_1\\), lascia alla sua sinistra il 25% del campione pensato\ncome una fila ordinata (destra quindi il 75%). Il secondo quartile\n\\(q_2\\) lascia sinistra il 50% del campione (destra quindi il 50%).\nEsso viene anche chiamato mediana. Il terzo quartile lascia sinistra\nil 75% del campione (destra quindi il 25%). Secondo lo stesso\ncriterio, si dicono decili quantili di ordine \\(p\\) multiplo di 0.10 e\npercentili quantili di ordine \\(p\\) multiplo di 0.01.Come si calcolano quantili? Consideriamo la definizione di quantile\nnon interpolato di ordine \\(p\\) \\((0 < p < 1)\\). Si procede innanzitutto\nordinando dati ordine crescente, \\(\\{x_1, x_2, \\dots, x_n\\}\\). Ci\nsono poi due possibilità. Se il valore \\(np\\) non è intero, sia \\(k\\)\nl’intero tale che \\(k < np < k + 1\\) – ovvero, la parte intera di \\(np\\).\nAllora \\(q_p = x_{k+1}.\\) Se \\(np = k\\) con \\(k\\) intero, allora\n\\(q_p = \\frac{1}{2}(x_{k} + x_{k+1}).\\) Se vogliamo calcolare il primo\nquartile \\(q_1\\), ad esempio, utilizziamo \\(p = 0.25\\). Dovendo calcolare\ngli altri quantili basta sostituire \\(p\\) il valore appropriato[^2].Gli indici di posizione, tra le altre cose, hanno un ruolo importante,\novvero vengono utilizzati per creare una rappresentazione grafica di una\ndistribuzione di valori che è molto popolare e può essere usata \nalternativa ad un istogramma (realtà vedremo poi come possa essere\ncombinata con un istogramma). Tale rappresentazione va sotto il nome di\nbox-plot.Per fare un esempio, consideriamo nove soggetti del campione clinico di Zetsche et al. (2019) che hanno riportato un unico episodio di depressione maggiore. Per tali soggetti valori ordinati del BDI-II (per semplicità li chiameremo \\(x\\)) sono seguenti: 19, 26, 27, 28, 28, 33, 33, 41, 43.\nPer il calcolo del secondo quartile (non interpolato), ovvero per il calcolo della mediana, dobbiamo considerare la quantità \\(np = 9 \\cdot 0.5 = 4.5\\), non intero. Quindi, \\(q_1 = x_{4 + 1} = 27\\).\nPer il calcolo del quantile (non interpolato) di ordine \\(p = 2/3\\) dobbiamo considerare la quantità \\(np = 9 \\cdot 2/3 = 6\\), intero. Quindi, \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).","code":""},{"path":"statistica-descrittiva.html","id":"box-plot","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.6.2 Box-plot","text":"Il box-plot (o diagramma scatola) è uno strumento grafico utile al\nfine di ottenere informazioni circa la dispersione e l’eventuale\nsimmetria o asimmetria di una distribuzione. Per costruire un box-plot\nsi rappresenta sul piano cartesiano un rettangolo (cioè la “scatola”) di\naltezza arbitraria la cui base corrisponde alla dist intanza\ninterquartile (IQR = \\(q_{0.75} - q_{0.25}\\)). La linea interna alla\nscatola rappresenta la mediana \\(q_{0.5}\\). Si tracciano poi ai lati della\nscatola due segmenti di retta cui estremi sono detti “valore\nadiacente” inferiore e superiore. Il valore adiacente inferiore è il\nvalore più piccolo tra le osservazioni che risulta maggiore o uguale al\nprimo quartile meno la distanza corrispondente 1.5 volte la distanza\ninterquartile.\nIl valore adiacente superiore è il valore più grande tra le osservazioni che risulta minore o uguale \\(Q_3+1.5\\) IQR. valori esterni ai valori adiacenti (chiamati valori anomali) vengono rappresentati individualmente nel box-plot per meglio evidenziarne la presenza e la posizione.\nFigura 9.5: Box-plot: \\(M\\) è la mediana, \\(\\bar{x}\\) è la media aritmetica e IQR è la distanza interquartile (\\(Q_3 - Q_1\\)).\nConsideriamo ora un caso concreto nel quale viene utilizzato un box-plot.\nNel caso dei dati di Zetsche et al. (2019) ci chiediamo che modo si differenziano le distribuzioni del BDI-II tra due gruppi considerati, ovvero tra il gruppo dei pazienti e il gruppo di controllo.La figura 9.6 fornisce due rappresentazioni grafiche che possono essere utilizzate per rispondere questa domanda.\nFigura 9.6: Due versioni di un violin plot per valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019).\nNella figura 9.6 sinistra sono rappresentati dati grezzi: questa è la pratica migliore quando il numero di osservazioni è piccolo. La linea curva che circonda (simmetricamente) le osservazioni è l’istogramma lisciato che abbiamo descritto precedenza. Nella figura 9.6 destra sono rappresentanti gli stessi dati: la funzione di densità empirica è la stessa di prima, ma al suo interno viene collocato un box-plot. Questa seconda rappresentazione è da preferirsi quando ci sono molte osservazioni e non è utile rappresentare singolarmente ciascun dato. Entrambe le rappresentazioni suggeriscono che la distribuzione dei dati è ’incirca simmetrica nel gruppo clinico (codificato come mdd). Il gruppo di controllo (ctl) mostra invece un’asimmetria positiva, con tre osservazioni evidenziate nel boxplot come dei “valori anomali,” dato che si discostano dalla mediana di una quantità maggiore di 1.5 IQR.","code":"\nbysubj <- df %>% \n  group_by(esm_id, group) %>% \n  summarise(\n    bdi = mean(bdi),\n    nr_of_episodes = mean(nr_of_episodes, na.rm = TRUE)\n  ) %>% \n  na.omit()\n#> `summarise()` has grouped output by 'esm_id'. You can override using the `.groups` argument.\n\nbysubj %>% \n  ggplot(aes(x=group, y=bdi)) + \n  geom_boxplot() +\n  labs(\n    x = \"Gruppo\",\n    y = \"BDI-II\"\n  ) +\n  theme_apa()\nlibrary(\"patchwork\")\n\nbysubj <- df %>% \n  group_by(esm_id, group) %>% \n  summarise(\n    bdi = mean(bdi),\n    nr_of_episodes = mean(nr_of_episodes, na.rm = TRUE)\n  ) %>% \n  na.omit()\n#> `summarise()` has grouped output by 'esm_id'. You can override using the `.groups` argument.\n\np1 <- bysubj %>% \n  ggplot(aes(x=group, y=bdi)) + \n  geom_violin(trim=FALSE) +\n  geom_dotplot(binaxis='y', stackdir='center', dotsize=0.7) +\n  labs(\n    x = \"Gruppo\",\n    y = \"BDI-II\"\n    #, caption = \"Fonte: Zetsche, Buerkner, & Renneberg (2020)\"\n  ) \n\np2 <- bysubj %>% \n  ggplot(aes(x=group, y=bdi)) + \n  geom_violin(trim=FALSE) +\n  geom_boxplot(width=0.05) +\n  labs(\n    x = \"Gruppo\",\n    y = \"BDI-II\"\n    #, caption = \"Fonte: Zetsche, Buerkner, & Renneberg (2020)\"\n  ) \n\np1 + p2\n#> `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"statistica-descrittiva.html","id":"leccellenza-grafica","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.6.3 L’eccellenza grafica","text":"Non c’è un modo “corretto” per rappresentare forma grafica un insieme\ndi dati. Ciascuno dei grafici che abbiamo discusso ha suoi pregi e \nsuoi difetti. Un ricercatore che ha influenzato molto il modo cui\nviene realizzata la visualizzazione dei dati scientifici è Edward Tufte,\nsoprannominato dal New York Times il “Leonardo da Vinci dei dati.”\nSecondo Tufte, “l’eccellenza nella grafica consiste nel comunicare idee\ncomplesse modo chiaro, preciso ed efficiente.” Nella visualizzazione\ndelle informazioni, l’“eccellenza grafica” ha l’obiettivo di comunicare\nal lettore il maggior numero di idee nel minor tempo possibile, con meno\ninchiostro possibile, usando il minor spazio possibile. Secondo\nTufte (2001), le rappresentazioni grafiche dovrebbero:mostrare dati;indurre l’osservatore riflettere sulla sostanza piuttosto che\nsulla progettazione grafica, o qualcos’altro;evitare di distorcere quanto dati stanno comunicando (“integrità\ngrafica”);presentare molte informazioni forma succinta;rivelare la coerenza tra le molte dimensioni dei dati;incoraggiare l’osservatore comparare differenti porzioni di dati;rivelare dati diversi livelli di dettaglio, da una visione ampia\nalla struttura di base;servire ad uno scopo preciso (descrizione, esplorazione, o la\nrisposta qualche domanda);essere fortemente integrate con le descrizioni statistiche e verbali\ndei dati fornite nel testo.base questi principi, la funzione di densità empirica fornisce una\nrappresentazione migliore dei dati di Zetsche et al. (2019) di quanto lo faccia un istogramma. Inoltre, se oltre al grupppo di appartenenza non ci sono altre dimensioni importanti da mettere evidenza, allora la nostra scelta dovrebbe\nricadere sul pannello di sinistra della figura 9.6.","code":""},{"path":"statistica-descrittiva.html","id":"indici-di-tendenza-centrale","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7 Indici di tendenza centrale","text":"L’analisi grafica, esaminata precedenza, costituisce la base di\npartenza di qualsivoglia analisi quantitativa dei dati. Tramite\nl’analisi grafica possiamo capire alcune caratteristiche importanti di\nuna distribuzione: per esempio, se è simmetrica o asimmetrica; oppure se\nè unimodale o multimodale. Successivamente, possiamo calcolare degli\nindici numerici che descrivono modo sintetico le caratteristiche di\nbase dei dati esaminati. Tra le misure di tendenza centrale, ovvero tra\ngli indici che forniscono un’idea dei valori attorno ai quali sono\nprevalentemente concentrati dati di un campione, quella più\ncomunemente usata è la media.","code":""},{"path":"statistica-descrittiva.html","id":"media","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.1 Media","text":"Tutti conosciamo la media aritmetica di \\(\\{x_1, x_2, \\dots, x_n\\}\\),\novvero il numero reale \\(\\bar{x}\\) definito da\n\\[\\begin{equation}\n\\bar{x}=\\frac{1}{n}\\sum_{=1}^n x_i.\n\\tag{9.1}\n\\end{equation}\\]\nNell’eq. (9.1) abbiamo usato la notazione delle sommatorie\nper descrivere una somma di valori. Questa notazione è molto usata \nstatistica e viene descritta Appendice.La media gode della seguente importante proprietà: la somma degli scarti\ntra ciascuna modalità \\(x_i\\) e la media aritmetica \\(\\bar{x}\\) è nulla,\ncioè\n\\[\n\\sum_{=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\label{eq:diffmeansumzero}\\] Infatti, \\[\\begin{aligned}\n\\sum_{=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\\end{aligned}\n\\]Ciò ci consente di pensare alla media come al baricentro della distribuzione.Un’altra proprietà della media è la seguente. La somma dei quadrati\ndegli scarti tra ciascuna modalità \\(x_i\\) e una costante arbitraria\n\\(\\\\Re\\), cioè \\[\\varphi() = \\sum_{=1}^n (x_i - )^2,\\notag\\] è\nminima per \\(= \\bar{x}\\).Il concetto statistico di media ha suscitato molte battute. Per esempio,\nil fatto che, media, ciascuno di noi ha un numero di gambe circa pari\n1.9999999. Oppure, il fatto che, media, ciascuno di noi ha un\ntesticolo. Ma la media ha altri problemi, oltre al fatto di ispirare\nbattute simili alle precedenti. particolare, dobbiamo notare che la\nmedia non è sempre l’indice che meglio rappresenta la tendenza centrale\ndi una distribuzione. particolare, ciò non accade quando la\ndistribuzione è asimmetrica, o presenza di valori anomali (outlier)\n– si veda il pannello di destra della figura 9.6. tali circostanze, la tendenza centrale della distribuzione è meglio rappresentata dalla mediana o dalla media spuntata.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-3","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.1.1 Esercizio con R","text":"Calcoliamo la media dei valori BDI-II per due gruppi di soggetti di Zetsche et al. (2019).","code":"\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    avg_bdi = mean(bdi)\n  ) \n#> # A tibble: 2 x 2\n#>   group avg_bdi\n#> * <fct>   <dbl>\n#> 1 ctl      1.69\n#> 2 mdd     30.9"},{"path":"statistica-descrittiva.html","id":"media-spuntata","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.2 Media spuntata","text":"La media spuntata \\(\\bar{x}_t\\) (trimmed mean) non è altro che la\nmedia dei dati calcolata considerando solo il 90% (o altra percentuale)\ndei dati centrali. Per calcolare \\(\\bar{x}_t\\) si ordinando dati secondo\nuna sequenza crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), per\npoi eliminare il primo 5% e l’ultimo 5% dei dati della serie così\nordinata. La media spuntata è data dalla media aritmetica dei dati rimanenti.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-4","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.2.1 Esercizio con R","text":"Calcoliamo la media spuntata dei valori BDI-II per due gruppi di soggetti di Zetsche et al. (2019) escludendo il 10% dei valori più estremi ciascun gruppo.","code":"\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    avg_trim_bdi = mean(bdi, trim = 0.1)\n  ) \n#> # A tibble: 2 x 2\n#>   group avg_trim_bdi\n#> * <fct>        <dbl>\n#> 1 ctl            1  \n#> 2 mdd           30.6"},{"path":"statistica-descrittiva.html","id":"moda-e-mediana","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.3 Moda e mediana","text":"precedenza abbiamo già incontrato altri due popolari indici di\ntendenza centrale: la moda (Mo), ovvero il valore centrale della\nclasse con la frequenza massima (può succedere che una distribuzione\nabbia più mode; tal caso si dice multimodale e questo operatore\nperde il suo significato di indice di tendenza centrale) e la mediana\n\\(\\tilde{x}\\).","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-5","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.7.3.1 Esercizio con R","text":"Calcoliamo quantili di ordine 0.25, 0.5 e 0.75 dei valori BDI-II per due gruppi di soggetti di Zetsche et al. (2019).","code":"\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    q25 = quantile(bdi, probs = 0.25),\n    q50 = quantile(bdi, probs = 0.50),\n    q75 = quantile(bdi, probs = 0.75)\n  ) \n#> # A tibble: 2 x 4\n#>   group   q25   q50   q75\n#> * <fct> <dbl> <dbl> <dbl>\n#> 1 ctl       0     1     2\n#> 2 mdd      26    30    35"},{"path":"statistica-descrittiva.html","id":"indici-di-dispersione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8 Indici di dispersione","text":"Le medie e gli indici di posizione descritti precedenza forniscono\ndelle sintesi dei dati che mettono evidenza la tendenza centrale\ndelle osservazioni. Tali indici, tuttavia, non considerano un aspetto\nimportante della distribuzione dei dati, ovvero la variabilità dei\nvalori numerici della variabile statistica. È dunque necessario\nsintetizzare la distribuzione di una variabile statistica oltre che con\nle misure di posizione anche tramite l’utilizzo di indicatori che\nvalutino la dispersione delle unità statistice.","code":""},{"path":"statistica-descrittiva.html","id":"indici-basati-sullordinamento-dei-dati","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.1 Indici basati sull’ordinamento dei dati","text":"È possibile calcolare degli indici di variabilità basati\nsull’ordinamento dei dati. L’indice più ovvio è l’intervallo di\nvariazione, ovvero la distanza tra il valore massimo e il valore minimo\ndi una distribuzione di modalità, mentre precedenza abbiamo già\nincontrato la differenza interquartile. Questi due indici, però, hanno\nil limite di essere calcolati sulla base di due soli valori della\ndistribuzione (\\(x_{\\text{max}}\\) e \\(x_{\\text{mini}}\\), oppure \\(x_{0.25}\\) e\n\\(x_{0.75}\\)). Pertanto non utilizzano tutte le informazioni che sono\ndisponibili. Inoltre, l’intervallo di variazione ha il limite di essere\npesantemente influenzato dalla presenza di valori anomali.","code":""},{"path":"statistica-descrittiva.html","id":"scostamento-medio-semplice-dalla-media","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.2 Scostamento medio semplice dalla media","text":"Dati limiti delle statistiche precedenti è più comune misurare la\nvariabilità di una variabile statistica come la dispersione dei dati\nattorno ad un indice di tendenza centrale. Scelto l’indice di tendenza\ncentrale rispetto al quale si vuole misurare la dispersione, è possibile\npoi calcolare la media degli scostamenti dei singoli dati dal valore di\nriferimento. Ad esempio, se scegliamo la mediana quale misura di\nposizione centrale, è possibile calcolare la media aritmetica della\ndistribuzione degli scarti valore assoluto tra ciascuna modalità e la\nmediana stessa. Nel caso di una variabile statistica \\(X\\) lo scostamento\nmedio semplice dalla media è la quantità\\[\\begin{equation}\nS_{} = \\frac{1}{n} \\sum_{=1}^n |x_i - x_{0.5}|.\n\\tag{9.2}\n\\end{equation}\\]","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-6","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.2.1 Esercizio con R","text":"Calcoliamo lo scostamento medio semplice dalla media per il BDI-II dei due gruppi di soggetti di Zetsche et al. (2019).Oppure, per due gruppi:La deviazione mediana assoluta è una misura robusta della dispersione statistica di un campione. Per un insieme \\(x_1, x_2, \\dots, x_n\\), il valore di MAD è definito come la mediana del valore assoluto delle deviazioni dei dati dalla mediana, ovvero:\\[\\begin{equation}\nMAD = \\text{med} (|x_i - \\text{med}(x_i)|).\n\\tag{9.3}\n\\end{equation}\\]Per dati di Zetsche et al. (2019) abbiamo:","code":"\nmean(abs(bysubj$bdi - median(bysubj$bdi)))\n#> [1] 14.48387\nmean_abs_dev <- function(x){\n  mean(abs(x - median(x)))\n}\n\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    Mean_abs_dev = mean_abs_dev(bdi)\n  ) \n#> # A tibble: 2 x 2\n#>   group Mean_abs_dev\n#> * <fct>        <dbl>\n#> 1 ctl           1.62\n#> 2 mdd           5.27\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    MAD = mad(bdi)\n  ) \n#> # A tibble: 2 x 2\n#>   group   MAD\n#> * <fct> <dbl>\n#> 1 ctl    1.48\n#> 2 mdd    6.67"},{"path":"statistica-descrittiva.html","id":"varianza","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.3 Varianza","text":"Anche se la statistica definita\ndall’eq. (9.2) è molto intuitiva, la misura di\nvariabilità di gran lunga più usata per valutare la variabilità di una\nvariabile statistica è senza dubbio la varianza. La varianza\n\\[\\begin{equation}\ns^2 = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\n\\tag{9.4}\n\\end{equation}\\]\nè la media dei quadrati degli scarti \\(x_i - \\bar{x}\\) tra ogni valore e la media della distribuzione.\nLa varianza è una misura di dispersione più complessa di quelle esaminate precedenza. È appropriata solo nel caso di distribuzioni simmetriche e, anch’essa, è fortemente influenzata dai valori anomali. Inoltre, è espressa \nun’unità di misura che è il quadrato dell’unità di misura dei dati originari e quindi ad essa non può essere assegnata un’interpretazione intuitiva.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-7","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.3.1 Esercizio con R","text":"Calcoliamo la varianza dei punteggi BDI-II nei due gruppi di soggetti di Zetsche et al. (2019).","code":"\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    variance = var(bdi)\n  ) \n#> # A tibble: 2 x 2\n#>   group variance\n#> * <fct>    <dbl>\n#> 1 ctl       8.03\n#> 2 mdd      43.7"},{"path":"statistica-descrittiva.html","id":"deviazione-standard","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.4 Deviazione standard","text":"Per le ragioni espresse sopra, la misura più usata della dispersione di una distribuzione di dati è la deviazione standard, ovvero la radice quadrata della varianza. differenza della varianza, dunque, la deviazione standard è espressa nella stessa unità di misura dei dati. Come nel caso della varianza, anche la deviazione standard \\(s\\) dovrebbe essere usata soltanto quando la media è adeguata per misurare il centro della distribuzione, ovvero, nel caso di distribuzioni simmetriche. Come nel caso della media \\(\\bar{x}\\), anche la deviazione standard è fortemente influenzata dai dati anomali (outlier), ovvero dalla presenza di uno o di pochi dati che sono molto più distanti dalla media rispetto agli altri valori della distribuzione. Quando tutte le osservazioni sono uguali, \\(s=0\\), altrimenti \\(s > 0\\).Alla deviazione standard può essere assegnata una semplice interpretazione: la deviazione standard è simile (ma non identica) allo scostamento medio semplice dalla media. La deviazione standard ci dice, dunque, quanto sono distanti, media, le singole osservazioni dal centro della distribuzione.","code":""},{"path":"statistica-descrittiva.html","id":"esercizio-con-r-8","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.4.1 Esercizio con R","text":"Calcoliamo la deviazione standard per il BDI-II dei due gruppi di soggetti di Zetsche et al. (2019).","code":"\nbysubj %>% \n  group_by(group) %>% \n  summarise(\n    stdev = sd(bdi)\n  ) \n#> # A tibble: 2 x 2\n#>   group stdev\n#> * <fct> <dbl>\n#> 1 ctl    2.83\n#> 2 mdd    6.61"},{"path":"statistica-descrittiva.html","id":"indici-di-variabilità-relativi","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.8.5 Indici di variabilità relativi","text":"volte può essere interessante effettuare un confronto fra due misure\ndi variabilità di grandezze incommensurabili, ovvero di caratteri\nrilevati mediante differenti unità di misura. questi casi, le misure\ndi variabilità precedentemente descritte si rivelano inadeguate \nquanto dipendono dall’unità di misura adottata. Diventa dunque\nnecessario ricorrere particolari numeri adimensionali detti indici\nrelativi di variabilità. Il più importante di tali indici è il\ncoefficiente di variazione, ovvero il numero puro\n\\[C_v = \\frac{\\sigma}{\\bar{x}}\\] ottenuto dal rapporto tra la deviazione\nstandard e la media dei dati. Un altro indice relativo di variabilità è\nla differenza interquartile rapportata al primo quartile oppure al terzo\nquartile oppure alla mediana, cioè:\n\\[\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\\notag\\]","code":""},{"path":"statistica-descrittiva.html","id":"le-relazioni-tra-variabili","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.9 Le relazioni tra variabili","text":"Zetsche et al. (2019) hanno misurato il livello di depressione dei\nsoggetti del loro esperimento utilizzando due scale psicometriche: il\nBeck Depression Inventory II (BDI-II) e la Center Epidemiologic\nStudies Depression Scale (CES-D). Il BDI-II è uno strumento self-report\nche valutare la presenza e l’intensità di sintomi depressivi pazienti\nadulti e adolescenti di almeno 13 anni di età con diagnosi psichiatrica\nmentre la CES-D è una scala self-report progettata per misurare \nsintomi depressivi che sono stati vissuti nella settimana precedente\nnella popolazione generale, specialmente quella degli\nadolescenti/giovani adulti. Una domanda ovvia che ci può venire \nmente è: quanto sono simili le misure ottenute mediante queste due\nscale?È chiaro che numeri prodotti dalle scale BDI-II e CES-D non possono\nessere identici, e questo per due motivi: (1) la presenza degli errori\ndi misurazione e (2) l’unità di misura delle due variabili. L’errore di\nmisurazione corrompe sempre, almeno parte, qualunque operazione di\nmisurazione. E questo è vero specialmente psicologia dove\nl’attendibilità degli strumenti di misurazione è minore che altre\ndiscipline (quali la fisica, ad esempio). Il secondo motivo per cui \nvalori delle scale BDI-II e CES-D non possono essere uguali è che\nl’unità di misura delle due scale è arbitraria. Infatti, qual è l’unità\ndi misura della depressione? Chi può dirlo! Ma, al di là delle\ndifferenze derivanti dall’errore di misurazione e dalla differente unità\ndi misura, ci aspettiamo che, se le due scale misurano entrambe lo\nstesso costrutto, allora valori prodotti dalle due scale dovranno\nessere tra loro linearmente associati. Per capire cosa si intende con\n“associazione lineare” iniziamo guardare dati. Per fare questo\nutilizziamo una rappresentazione grafica che va sotto il nome di\ndiagramma dispersione.","code":""},{"path":"statistica-descrittiva.html","id":"diagramma-a-dispersione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.9.1 Diagramma a dispersione","text":"Il diagramma di dispersione è la rappresentazione grafica delle coppie\ndi punti individuati dalle variabili BDI-II e CES-D, e si ottiene\nponendo, ad esempio, valori BDI-II sull’asse delle ascisse e quelli\ndel CES-D sull’asse delle ordinate. tale grafico, fornito dalla\nfigura 9.7, cascun punto corrisponde ad un\nindividuo del quale, nel caso presente, conosciamo il livello di\ndepressione misurato dalle due scale psicometriche.\n\n\n\n\nFigura 9.7: Associazione tra le variabili BDI-II e CES-D nello studio di Zetsche et al. (2019). rosso sono rappresentate le osservazioni del gruppo di controllo; blu quelle dei pazienti.\nDalla figura 9.7 possiamo vedere che dati mostrano\nuna certa tendenza disporsi attorno ad una retta – nel gergo\nstatistico, questo fatto viene espresso dicendo che punteggi CES-D\ntendono ad essere linearmente associati ai punteggi BDI-II. È ovvio,\ntuttavia, che tale relazione lineare è lungi dall’essere perfetta – se\nfosse perfetta, tutti punti del diagramma dispersione si\ndisporrebbero esattamente lungo una retta.Il problema che ci poniamo è quello di trovare un indice numerico che\ndescriva di quanto la nube di punti si discosta da una perfetta\nrelazione lineare tra le due variabili. Per risolvere tale problema\ndobbiamo specificare un indice statistico che descriva la direzione e la\nforza della relazione lineare tra le due variabili. Ci sono vari indici\nstatistici che possiamo utilizzare questo scopo.","code":"\nbysubj <- df %>% \n  group_by(esm_id, group) %>% \n  summarise(\n    bdi = mean(bdi),\n    cesd = mean(cesd_sum)\n  ) %>% \n  na.omit() %>% \n  ungroup()\n#> `summarise()` has grouped output by 'esm_id'. You can override using the `.groups` argument.\nm_cesd <- mean(bysubj$cesd)\nm_bdi <- mean(bysubj$bdi)\nFONT_SIZE <- 10\n\np <- bysubj %>%\n  ggplot(\n    aes(x=bdi, y=cesd, color=group)) +\n  geom_point(size=1) +\n  geom_hline(yintercept= m_cesd, linetype=\"dashed\", color = \"gray\") +\n  geom_vline(xintercept = m_bdi, linetype=\"dashed\", color = \"gray\") +\n  geom_text(x=-1, y=16, label=\"I\", color = \"gray\", size=FONT_SIZE) +\n  geom_text(x=0, y=46, label=\"IV\", color = \"gray\", size=FONT_SIZE) +\n  geom_text(x=18, y=46, label=\"III\", color = \"gray\", size=FONT_SIZE) +\n  geom_text(x=18, y=16, label=\"II\", color = \"gray\", size=FONT_SIZE) +\n  labs(\n    x = \"BDI-II\",\n    y = \"CESD\"\n  ) +\n  theme_apa() +\n  theme(legend.position=\"none\") \np"},{"path":"statistica-descrittiva.html","id":"covarianza","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.9.2 Covarianza","text":"Iniziamo considerare il più importante di tali indici, chiamato\ncovarianza. realtà la definizione di questo indice non ci\nsorprenderà più di tanto quanto, una forma solo apparentemente\ndiversa, l’abbiamo già incontrato precedenza. Ci ricordiamo infatti\nche la varianza di una generica variabile \\(X\\) è definita come la media\ndegli scarti quadratici di ciascuna osservazione dalla media:\n\\[\\begin{equation}\nS_{XX} = \\frac{1}{n} \\sum_{=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\tag{9.5}\n\\end{equation}\\]\nInfatti, la varianza viene talvolta descritta come la “covarianza di una\nvariabile con sé stessa.”Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di\nuna sola variabile, chiediamoci come due variabili \\(X\\) e \\(Y\\) “variano\ninsieme” (co-variano). È facile capire come una risposta tale domanda\npossa essere fornita da una semplice trasformazione della formula\nprecedente che diventa:\n\\[\\begin{equation}\nS_{XY} = \\frac{1}{n} \\sum_{=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{9.6}\n\\end{equation}\\]\nL’eq. (9.6) ci fornisce dunque la definizione della covarianza.Per capire il significato dell’eq. (9.6), supponiamo di dividere il grafico della figura 9.7 quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una\nretta orizzontale passante per la media dei valori CES-D. Numeriamo \nquadranti partendo da quello basso sinistra e muovendoci senso\nantiorario.Se prevalgono punti nel e III quadrante, allora la nuvola di punti\navrà un andamento crescente (per cui valori bassi di \\(X\\) tendono ad\nassociarsi valori bassi di \\(Y\\) e valori elevati di \\(X\\) tendono ad\nassociarsi valori elevati di \\(Y\\)) e la covarianza segno positivo. Mentre\nse prevalgono punti nel II e IV quadrante la nuvola di punti avrà un\nandamento decrescente (per cui valori bassi di \\(X\\) tendono ad\nassociarsi valori elevati di \\(Y\\) e valori elevati di \\(X\\) tendono ad\nassociarsi valori bassi di \\(Y\\)) e la covarianza segno negativo. Dunque,\nil segno della covarianza ci informa sulla direzione della relazione\nlineare tra due variabili: l’associazione lineare si dice positiva se la\ncovarianza è positiva, negativa se la covarianza è negativa.Il segno della covarianza ci informa sulla direzione della relazione, ma\ninvece il valore assoluto della covarianza ci dice ben poco. Esso,\ninfatti, dipende dall’unità di misura delle variabili. Nel caso presente\nquesto concetto è difficile da comprendere, dato che le due variabili \nesame non hanno un’unità di misura (ovvero, hanno un’unità di misura\narbitraria e priva di significato). Ma quest’idea diventa chiara se\npensiamo alla relazione lineare tra l’altezza e il peso delle persone,\nad esempio. La covarianza tra queste due quantità è certamente positiva,\nma il valore assoluto della covarianza diventa più grande se l’altezza\nviene misurata millimetri e il peso grammi, e diventa più piccolo\nl’altezza viene misurata metri e il peso chilogrammi. Dunque, il\nvalore della covarianza cambia al mutare dell’unità di misura delle\nvariabili anche se l’associazione tra le variabili resta costante.","code":""},{"path":"statistica-descrittiva.html","id":"correlazione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.9.3 Correlazione","text":"Dato che il valore assoluto della covarianza è di difficile\ninterpretazione – pratica, non viene mai interpretato – è\nnecessario trasformare la covarianza modo tale da renderla immune\nalle trasformazioni dell’unità di misura delle variabili. Questa\noperazione si dice standardizzazione e corrisponde alla divisione\ndella covarianza per le deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due\nvariabili:\\[\\begin{equation}\nr_{XY} = \\frac{S_{XY}}{s_X s_Y}.\n\\tag{9.7}\n\\end{equation}\\]\nLa quantià che si ottiene questo modo viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, la hanno introdotta).Il coefficiente di correlazione ha le seguenti proprietà:ha lo stesso segno della covarianza, dato che si ottiene dividendo\nla covarianza per due numeri positivi;è un numero puro, cioè non dipende dall’unità di misura delle\nvariabili;assume valori compresi tra -1 e +1.Ad esso possiamo assegnare la seguente interpretazione:\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti \npunti si trovano esattamente su una retta con pendenza negativa (dal\nquadrante alto sinistra al quadrante basso destra);\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti \npunti si trovano esattamente su una retta con pendenza positiva (dal\nquadrante basso sinistra al quadrante alto destra);\\(-1 < r_{XY} < +1\\) \\(\\rightarrow\\) presenza di una relazione lineare\ndi intensità diversa;\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e\n\\(Y\\).Per dati della figura 9.7, la covarianza è 207.426. Il segno positivo della covarianza ci dice che tra le due variabili c’è\nun’associazione lineare positiva. Per capire qual è l’intensità della\nrelazione lineare tra le due variabili calcoliamo la correlazione.\nEssendo le deviazioni standard del BDI-II e del CES-D rispettavamente\nuguali 15.37 e 14.93, la correlazione diventa uguale \n\\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore è prossimo \n1.0, il che vuol dire che punti del diagramma dispersione non si\ndiscostano troppo da una retta con una pendenza positiva.","code":""},{"path":"statistica-descrittiva.html","id":"correlazione-e-causazione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.10 Correlazione e causazione","text":"Facendo riferimento nuovamente alla figura 9.7, possiamo dire che, molte applicazioni (ma non nel caso presente!) l’asse \\(x\\) rappresenta una\nquantità nota come variabile indipendente e l’interesse si concentra\nsulla sua influenza sulla variabile dipendente tracciata sull’asse\n\\(y\\). Ciò presuppone però che sia nota la direzione cui l’influenza\ncausale potrebbe risiedere. È importante tenere bene mente che la\ncorrelazione è soltanto un indice descrittivo della relazione lineare\ntra due variabili e nessun caso può essere usata per inferire\nalcunché sulle relazioni causali che legano le variabili. È ben nota\nl’espressione: “correlazione non significa causazione.”Di opinione diversa era invece Karl Pearson (1911), il quale ha\naffermato:Quanto spesso, quando è stato osservato un nuovo fenomeno,\nsentiamo che viene posta la domanda: ‘qual è la sua causa?’ Questa è\nuna domanda cui potrebbe essere assolutamente impossibile rispondere.\nInvece, può essere più facile rispondere alla domanda: ‘che misura\naltri fenomeni sono associati con esso?’ Dalla risposta questa\nseconda domanda possono risultare molte preziose conoscenze.Che alla seconda domanda posta da Pearson sia facile rispondere è indubbio. Che\nla nostra comprensione di un fenomeno possa aumentare sulla base delle\ninformazioni fornite unicamente dalle correlazioni, invece, è molto dubbio e quasi\ncertamente falso.","code":""},{"path":"statistica-descrittiva.html","id":"usi-della-correlazione","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.10.1 Usi della correlazione","text":"Anche se non può essere usata per studiare le relazioni causali, la\ncorrelazione viene usata per molti altri scopi tra quali, per esempio,\nquello di misurare la validità concorrente di un test psiologico. Se\nun test psicologico misura effettivamente ciò che ci si aspetta che\nmisuri (nel caso dell’esempio presente, la depressione), allora dovremo\naspettarci che fornisca una correlazione alta con risultati di altri\ntest che misurano lo stesso costrutto – come nel caso dei dati di\n(Zetsche et al., 2019). Un’altra proprietà desiderabile di un test\npsicometrico è la validità divergente: risultati di test\npsicometrici che misurano costrutti diversi dovrebbero essere poco\nassociati tra loro. altre parole, questo secondo caso dovremmo\naspettarci che la correlazione sia bassa.","code":""},{"path":"statistica-descrittiva.html","id":"correlazione-di-spearman","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.10.2 Correlazione di Spearman","text":"Una misura alternativa della relazione lineare tra due variabili è\nfornita dal coefficiente di correlazione di Spearman e dipende soltanto\ndalla relazione d’ordine dei dati, non dagli specifici valori dei dati.\nTale misura di associazione è appropriata quando, del fenomeno esame,\ngli psicologi sono stati grado di misurare soltanto le relazioni\nd’ordine tra le diverse modalità della risposta dei soggetti, non\nl’intensità della risposta. Le variabili psicologiche che hanno questa\nproprietà si dicono ordinali. Nel caso di variabili ordinali, non è\npossibile sintetizzare dati mediante le statistiche descrittive che\nabbiamo introdotto questo capitolo, quali ad esempio la media e la\nvarianza, ma è invece solo possibile riassumere dati mediante una\ndistribuzione di frequenze per le varie modalità della risposta.","code":""},{"path":"statistica-descrittiva.html","id":"correlazione-nulla","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.10.3 Correlazione nulla","text":"Un ultimo aspetto da mettere evidenza proposito della correlazione\nriguarda il fatto che la correlazione descrive la direzione e\nl’intensità della relazione lineare tra due variabili. Relazioni non\nlineari tra le variabili, anche sono molto forti, non vengono catturate\ndalla correlazione. È importante rendersi conto che una correlazione\npari zero non significa che non c’è relazione tra le due variabili, ma\nsolo che tra esse non c’è una relazione lineare. Un esempio di questo\nfatto è fornito dalla figura 9.8.\nFigura 9.8: Due insiemi di dati (fittizi) per quali coefficienti di correlazione di Pearson sono entrambi 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.\n","code":"\nlibrary(\"datasauRus\")\nslant <- ggplot(datasaurus_dozen_wide, aes(x=slant_down_x,y=slant_down_y),   colour=dataset) \n# loads slant-pattern dataset of datasauRus package into data frame slant\nslant <- slant + \n  geom_point() # as a scatter type geom\nslant <- slant + \n  theme_void() # eliminates unwanted axis labels\nslant <- slant + \n  theme(legend.position = \"none\", \n        panel.border = element_rect(colour = \"black\", fill=NA, size = 1),\n        plot.margin = margin(0,2,0,2), aspect.ratio = 1) \n# removes legend, adds a border, adds margin space below, and specifies \n# required aspect ratio\n\ndino <- ggplot(datasaurus_dozen_wide, aes(x=dino_x,y=dino_y), colour=dataset) +\n  geom_point() \n# loads dino-figure dataset of datasauRus package into data \n#frame dino as a scatter type geom\ndino <- dino +theme_void() # eliminates unwanted axis labels\ndino <- dino + \n  theme(legend.position = \"none\", \n        panel.border =  element_rect(colour = \"black\", fill=NA, size = 1),\n        plot.margin = margin(0,2,0,2), aspect.ratio = 1) \n# removes legend, adds a border, adds margin space below, specifies \n# required aspect ratio\n\nslant + dino"},{"path":"statistica-descrittiva.html","id":"conclusioni-1","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.11 Conclusioni","text":"La prima fase dell’analisi dei dati è sicuramente quella che ci porta \nriassumere dati mediante gli strumenti della statistica descrittiva.\nCi sono diverse domande che vengono affrontate questa fase: qual è la\ndistribuzione delle variabili di interesse? Quali relazioni coppie si\npossono osservare nel campione? Ci sono delle osservazioni ‘anomale,’\novvero estremamente discrepanti rispetto alle altre, sia quando si\nesaminano le statistiche descrittive univariate (ovvero, quelle che\nriguardano le caratteristiche di una variabile presa singolarmente), sia\nquando vengono esaminate le statistiche bivariate (ovvero, le\nstatistiche che descrivono l’associazione tra le variabili)? È\nimportante avere ben chiare le idee su questi punti prima di procedere\ncon qualsiasi procedura statistica di tipo inferenziale. Per rispondere\nalle domande che abbiamo elencato sopra, ed ad altre simili, è molto\nutile procedere con delle rappresentazioni grafiche dei dati. Dovrebbe\nessere chiaro che, quando disponiamo di grandi moli di dati (come è\nsempre il caso psicologia), per fare questo è necessario usare un\nsoftware statistico.","code":""},{"path":"statistica-descrittiva.html","id":"esercizi","chapter":"Capitolo 9 Statistica descrittiva","heading":"9.12 Esercizi","text":"Scarica gli esercizi:Download 14_descr_exercises.RmdGuarda le risposte solo dopo avere provato rispondere tutte le domande:Download 14_descr_answers.Rmd","code":""},{"path":"che-cosè-la-probabilità.html","id":"che-cosè-la-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"Capitolo 10 Che cos’è la probabilità?","text":"È normale fare delle congetture rispetto ciò di cui non siamo sicuri.\nMa perché facciamo questo? Molto spesso perché, anche se sappiamo che le\nnostre conoscenze sono incomplete, dobbiamo comunque prendere delle\ndecisioni. Ad esempio: “non se tra qualche ora pioverà; devo o non\ndevo prendere l’ombrello?” maniera simile, anche se uno psicologo non\nsa maniera certa quali sono meccanismi che regolano fenomeni\npsicologi, deve comunque decidere tra diverse alternative. Per esempio,\ndeve fornire un parere, relativamente chi, tra due genitori, sia più\nadatto per ottenere l’affidamento del figlio caso di divorzio, oppure\nquale sia, un caso specifico, l’approccio più efficace per il\ntrattamento dei disturbi dell’alimentazione. Ovviamente la qualità delle\ncongetture varia, così come varia la qualità delle decisioni che\nprendiamo. La teoria delle probabilità ci fornisce gli strumenti per\nprendere decisioni “razionali” condizioni di incertezza, ovvero per\nformulare le migliori congetture possibili.La teoria delle probabilità ci consente di descrivere maniera\nquantitativa quei fenomeni che, pur essendo altamente variabili,\nrivelano comunque una qualche coerenza lungo termine. Il lancio\nripetuto di una moneta è uno di questi fenomeni. È anche l’esempio\ntipico che viene usato per introdurre una discussione sulle probabilità.\nSapere se una moneta sia onesta o meno, o calcolare la probabilità di\nottenere testa un certo numero di volte può essere interessante nel\nmondo delle scommesse, ma nella vita quotidiana non ci capita spesso di\nlanciare una moneta per prendere una decisione. Allora perché ci\npreoccupiamo di studiare le proprietà statistiche dei lanci di una\nmoneta? questa domanda si può rispondere dicendo che l’esperimento\n(chiamato “casuale”) che corrisponde al lancio di una moneta è il\nsurrogato di una molteplicità di eventi che, della vita reale, sono\nmolto importanti. Per esempio: qual è la probabilità di successo di un\nintervento psicologico? Qual è la probabilità che un test per l’HIV dia\nesito positivo una persona che non ha l’HIV? Qual è la probabilità di\nessere occupato entro un anno dalla laurea? lanci di una moneta\ncostituiscono una rappresentazione generica di molteplici altri eventi\nche hanno un grande significato nella nostra vita. Questa è la ragione\nper cui studiamo le proprietà statistiche dei fenomeni aleatori usando\nil lancio di una moneta quale esempio generico.La discussione della teoria della probabilità è certamente l’argomento\npiù impegnativo affrontato queste dispense. Fare uno sforzo di\ncomprensione per chiarire concetti di base della teoria della\nprobabilità è però necessario per mettersi nelle condizioni di capire le\ncaratteristiche dell’inferenza statistica che verranno discusse \nseguito.","code":""},{"path":"che-cosè-la-probabilità.html","id":"probabilità-nel-linguaggio-naturale","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.1 Probabilità nel linguaggio naturale","text":"un articolo pubblicato su Harward Business Review nel 2018,\nMauboussin e Mauboussin ci ricordano come, nel marzo del 1951, l’Office\nNational Estimates della CIA pubblicò un documento che suggeriva che\nun attacco sovietico alla Jugoslavia nel corso dell’anno fosse una\n“seria possibilità.” Sherman Kent, un professore di storia Yale che fu\nchiamato Washington, D.C. per dirigere l’Office National\nEstimates, espresse perplessità sull’esatto significato\ndell’espressione “seria possibilità.” Lo interpretò nel senso che la\nprobabilità di un attacco era di circa il 65%. Ma quando chiese ai\nmembri del Board National Estimates cosa ne pensassero, gli furono\nriferite cifre che andavano dal 20% ’80%. Una gamma così ampia\nrappresentava chiaramente un problema, poiché le implicazioni politiche\ndi quegli estremi erano nettamente diverse. Kent riconobbe che la\nsoluzione di tale problema era quella di usare numeri per esprimere il\nnostro grado di certezza, notando mestamente:Non abbiamo usato numeri… e sembra chiaro che abbiamo abusando delle parole.Da allora non è cambiato molto. Ancora oggi le persone nel mondo della\npolitica, degli affari e nella vita quotidiana continuano usare parole\nvaghe per descrivere possibili risultati degli eventi. Perché? Phil\nTetlock, professore di psicologia ’Università della Pennsylvania, che\nha studiato fondo il fenomeno psicologico della previsione, suggerisce\nche “una vaga verbosità conferisce sicurezza.” Quando usiamo una parola\nper descrivere la probabilità di un evento incerto, cerchiamo di porci\nnelle condizioni di non essere smentiti dopo che il risultato\ndell’evento verrà rivelato. Se si verifica l’evento che abbiamo\nprevisto, è facile dire: “Ti avevo detto che probabilmente sarebbe\nsuccesso questo.” Se la nostra predizione fallisce, possiamo sempre\ndire: “Ho solo detto che probabilmente sarebbe successo.” Parole così\nambigue non solo consentono ’oratore di evitare di essere smentito,\nma consentono anche al destinatario di interpretare il messaggio modo\ncoerente con le sue nozioni preconcette. Ovviamente, da tale ambiguità\nlinguistica deriva una cattiva comunicazione. È dunque necessario\nprocedere modo diverso nel linguaggio scientifico. Vedremo questo\ncapitolo come sia possibile assegnare al termine “probabilità” un\nsignificato preciso.","code":""},{"path":"che-cosè-la-probabilità.html","id":"probabilità-nel-linguaggio-scientifico","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.2 Probabilità nel linguaggio scientifico","text":"La teoria della probabilità nasce nel 1654. Fu infatti questa data\nche Antoine Gombaud Cavalier De Méré, un nobile francese, nonché\naccanito giocatore d’azzardo scrisse una lettera al suo amico Pascal per\ncercare di comprendere il motivo delle sue continue perdite nel gioco\ndei dadi. De Méré descrisse due diverse scommesse:scommessa Asi lancia un dado per 4 volte di seguito e si vince se esce almeno\nuna volta il 6;\nsi lancia un dado per 4 volte di seguito e si vince se esce almeno\nuna volta il 6;scommessa Bsi lanciano due dadi per 24 volte di seguito e si vince se esce\nalmeno una volta il doppio 6.\nsi lanciano due dadi per 24 volte di seguito e si vince se esce\nalmeno una volta il doppio 6.Il cavaliere De Méré pose Pascal il seguente quesito: le possibilità\ndi vittoria sono maggiori nella scommessa o nella scommessa B? Il\nproblema di De Méré divenne un motivo di scambio epistolare tra Pascal e\nFermat, due più grandi matematici del tempo, e viene considerato come\nla motivazione iniziale dello sviluppo della teoria della probabilità.Ma come può essere risolto il problema di De Méré? Una strategia\npossibile è quella di seguire l’esempio di De Méré, ovvero, giocare\nquesto gioco molte volte. Così facendo, De Méré si rese conto che le\npossibilità di vittoria erano leggermente migliori nel caso della\nscommessa .Utilizzando una simulazione al computer possiamo facilmente giungere \nquesta stessa conclusione senza perdere tutto il tempo che De Méré ha\ndedicato questa materia. Una simulazione al computer ci consente\ninfatti di ripetere il gioco di De Méré moltissime volte e di annotare\nil risultato ottenuto ad ogni ripetizione del gioco. Vedremo seguito\nperché, utilizzando un computer, è possibile ottenere un risultato\ndiverso ogni volta che si ripete una certa operazione, modo tale da\nrappresentare il grado di casualità che si osserva quando si lancia di\nun dado. Per ora ci limitiamo ad esaminare risultati che vengono\nprodotti questo modo e che sono illustrati nella\nfigura 10.1.\nFigura 10.1: Risultati ottenuti da 10000 ripetizioni delle due scommesse di De Méré.\nLa figura 10.1 riportata la proporzione di vittorie funzione\ndel numero di ripetizioni di ciascuna scommessa e rivela che, lungo\ntermine (ovvero, se consideriamo un grande numero di ripetizioni del\ngioco di De Méré), la scommessa risulta più conveniente della\nscommessa B. Nel caso di 10000 ripetizioni del gioco di De Méré, la\nproporzione di vittorie è risultata essere pari 0.5182 per la\nscommessa e pari 0.4909 per la scommessa B. Se ripetiamo la stessa\nsimulazione altre 10000 volte, otteniamo una proporzione di vittorie\nuguale 0.5180 per la scommessa e 0.4878 per la scommessa B.Vedremo questo capitolo come ciascuna di queste proporzioni possa\nessere considerata come una stima empirica di ciò che chiamiamo\nprobabilità. Le proporzioni descritte sopra vengono sono delle “stime”\npoiché approssimano il vero valore della probabilità; infatti, ripetendo\nla simulazione due volte abbiamo ottenuto dei risultati leggermente\ndiversi. Ma allora qual è il “vero” valore della probabilità? Un modo\nsemplice per rispondere questa domanda è quello di dire che,\nutilizzando la procedura descritta sopra, il vero valore della\nprobabilità si otterrebbe se il gioco di De Méré venisse ripetuto\ninfinite volte. Ma ovviamente, per qualunque applicazione concreta, non\nabbiamo bisogno di ripetere la simulazione infinite volte, quanto un\ngrande numero di ripetizioni ci fornisce un’approssimazione sufficiente.conclusione, le considerazioni precedenti ci fanno capire che il\nconcetto di probabilità sia legato quello di incertezza. La\nprobabilità può infatti essere definita come la quantificazione del\nlivello di “casualità” di un evento, laddove viene detto casuale ciò che\nnon è noto o non può essere predetto con certezza.","code":"\n# Game A: Throw a fair die at most four times, and win if you get a six.\nexperiment_a <- function(){\n  rolls <- sample(1:6, size = 4, replace = TRUE)\n  condition <- sum(rolls == 6) > 0\n  return(condition)\n}\n\n# Game B: Throw two fair dice at most twenty-four times, and win if you get a double-six.\nexperiment_b <- function(){\n  first.die <- sample(1:6, size = 24, replace = TRUE)\n  second.die <- sample(1:6, size = 24, replace = TRUE)\n  condition <- sum((first.die == second.die) & (first.die == 6)) > 0\n  return(condition)\n}\n\n# number of replications\nnrep <- 1e4\n# Play game A nrep times. We get a vector of nrep elements. Eeach element of \n# of the simsA vector is the outcome obtained by playing game A once: TRUE if\n# the output is a win, FALSE if the output of the game is a loss. Remember than\n# TRUE = 1 and FALSE = 0.\nsims_a <- replicate(nrep, experiment_a())\n# The proportion of wins in game A \nprop_wins_a <- sum(sims_a)/length(sims_a)\nprop_wins_a\n#> [1] 0.5193\n# To plot the results, we compute the \nnwins_a <- cumsum(sims_a)\nntrials <- 1:nrep\n\nsims_b <- replicate(nrep, experiment_b())\nprop_wins_b <- sum(sims_b)/length(sims_b) \nprop_wins_b\n#> [1] 0.493\n\nnwins_b <- cumsum(sims_b)\n\nd <- data.frame(\n  n = c(ntrials, ntrials), \n  pwin = c(nwins_a/ntrials, nwins_b/ntrials),\n  game = rep(c(\"Scommessa A\", \"Scommessa B\"), each = nrep)\n)\n\nd %>% \n  ggplot(\n    aes(x = n, y = pwin, col = game)\n  ) +\n  geom_point(alpha = 0.4) +\n  geom_line() +\n  scale_x_log10(breaks = c(1, 3, 10, 50, 200, 1000, 3000,  10000)) +\n  theme(legend.title = element_blank()) +\n  labs(\n    x=\"Numero di ripetizioni del gioco di De Méré\", \n    y=\"Proporzione di vincite\") +\n  scale_color_manual(values = c(\"gray80\", \"skyblue\")) +\n  theme(legend.position = \"bottom\")"},{"path":"che-cosè-la-probabilità.html","id":"terminologia-1","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.3 Terminologia","text":"Come qualsiasi altra branca della matematica, la teoria delle\nprobabilità fa uso di una specifica terminologia cui concetti di base\nsono descritti di seguito.Il calcolo delle probabilità si occupa di un generico esperimento casuale. Si dice esperimento casuale qualsiasi attività che produce un risultato osservabile. L’esecuzione di un esperimento casuale è chiamata prova dell’esperimento. Esempi sono: lanciare una moneta, lanciare un dado 6 facce, provare un nuovo percorso per andare al lavoro per vedere se è più veloce di quello che usiamo di solito, o giocare al gioco di De Méré.Il risultato (o esito) di una prova si indica con \\(\\omega\\) ed è detto evento elementare.Prima che l’esperimento casuale venga eseguito non sappiamo quale esito verrà prodotto; dopo che l’esperimento casuale è stato eseguito, l’esito dell’esperimento si “cristallizza” nel risultato osservato.Si dice spazio campionario \\(\\Omega\\) (probability space) l’insieme di tutti possibili esiti di un esperimento casuale. Lo spazio campionario può essere finito, infinito o infinito numerabile. Eseguire un esperimento casuale significa scegliere maniera casuale uno dei possibili eventi elementari dello spazio campionario.Si dice evento composto (o non-elementare) un sottoinsieme dello spazio campionario, ovvero un insieme che può essere sua volta scomposto più eventi elementari. Per esempio, il numero 4 è un evento elementare dello spazio campionario finito \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) che corrisponde ’esperimento casuale del lancio di un dado. L’evento composto \\(\\) “il risultato è pari” è \\(= \\{2, 4, 6\\}\\).","code":""},{"path":"che-cosè-la-probabilità.html","id":"le-diverse-definizioni-della-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.4 Le diverse definizioni della probabilità","text":"Ma, nello specifico, che cos’è la probabilità? questa domanda si può rispondere modi diversi.","code":""},{"path":"che-cosè-la-probabilità.html","id":"una-definizione-ingenua-della-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.4.1 Una definizione “ingenua” della probabilità","text":"Storicamente, la prima definizione della probabilità di un evento è\nstata quella che richiede di contare il numero di modi nei quali un\nevento può manifestarsi e di dividere tale numero per il numero totale\ndi eventi dello spazio campionario \\(\\Omega\\).La definizione 10.1 rende chiaro che il calcolo delle probabilità richiede di contare il numero di modi cui un evento può realizzarsi.\nPer esempio, nell’esperimento casuale corrispondente al lancio di due dadi equilibrati, l’evento \\(\\) = “la somma dei due dati è 5” si può realizzare 4 modi diversi: \\(= \\{ (1, 4), (2, 3), (3, 2), (4, 1) \\}\\).\nContare il numero di modi cui un evento può realizzarsi può essere semplice, nel caso di alcuni eventi (come il presente), oppure estremamente complesso, nel caso di altri eventi. questo secondo caso, per contare il numero di modi cui un evento può realizzarsi, al fine di calcolare la probabilità definita come indicato sopra, è necessario fare uso del calcolo combinatorio. queste dispense ci accontenteremo di presentare alcune nozioni di base del calcolo combinatorio, ma non entreremo nei dettagli di questo argomento.","code":""},{"path":"che-cosè-la-probabilità.html","id":"una-definizione-non-ingenua-della-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.4.2 Una definizione “non ingenua” della probabilità","text":"Il calcolo combinatorio ci consente di contare il numero di casi nello spazio campionario e di applicare la definizione “ingenua” di probabilità descritta nella\ndefinizione 10.1. È però facile rendersi conto che tale definizione di probabilità ha un grosso problema: non può essere applicata al caso di uno spazio campionario infinito. Dobbiamo dunque trovare una definizione che risolva un tale problema. Per fare ciò vengono specificate alcune proprietà che vorremmo potere attribuire alla probabilità – matematica, tali proprietà sono dette assiomi – per poi definire una funzione di probabilità che soddisfi tali proprietà. Arriviamo questo modo alla seguente definizione non ingenua della probabilità.Per la precisione, una \\(\\sigma\\)-algebra è una famiglia di insiemi tali\nche \\(\\emptyset \\\\mathcal{}\\); se \\(\\\\mathcal{}\\) allora anche il suo complementare \\(^C\\) è \\(\\mathcal{}\\); unioni numerabili di elementi di \\(\\mathcal{}\\) appartengono ancora ad \\(\\mathcal{}\\).La funzione di probabilità \\(P()\\) deve soddisfare seguenti assiomi:la probabilità \\(P(\\omega)\\) soddisfa la disuguaglianza \\(0 \\leq P(\\omega) \\leq 1;\\)la probabilità dell’evento certo (ovvero la probabilità dello spazio\ncampionario \\(\\Omega\\)) è 1: \\(P(\\Omega) = \\sum_{\\omega \\\\Omega} P(\\omega) = 1;\\)se \\(A_1, A_2, \\dots, A_k\\) sono eventi disgiunti, allora la\nprobabilità che uno di essi si verifichi è pari alla somma delle\nloro separate probabilità:\n\\[\\begin{equation}\nP(A_1 \\text{ o } A_1 \\dots \\text{ o } A_k) = P(A_1) + P(A_2) +\\dots + P(A_k).\n\\tag{10.1}\n\\end{equation}\\]La definizione 10.2 corrisponde al cosiddetto approccio assiomatico messo punto da Kolmogorov intorno al 1930, il quale è alla base della moderna teoria della probabilità.Nonostante l’enorme complessità dell’approccio assiomatico alla\nprobabilità, per gli scopi dell’analisi dei dati psicologici le cose che\ndobbiamo capire sono molto semplici. vincoli della \\(\\sigma\\)-algebra\nsono necessari per evitare paradossi che si possono creare quando si\nmanipolano gli insiemi (ad esempio, l’utilizzo dell’“l’insieme di tutti\ngli insiemi” tipicamente conduce ad un paradosso). Questi problemi sono\nperò molto lontani dalle applicazioni della teoria della probabilità\n’analisi dei dati psicologici. Gli psicologi tipicamente effettuano\npartizioni molto semplici dello spazio campionario: il trattamento ha\nfunzionato oppure ? Per cui le aporie della teoria degli insiemi cui\nla \\(\\sigma\\)-algebra vuole porre un freno sono problemi che non ci\nriguardano.Gli altri aspetti della teoria assiomatica della probabilità, invece,\nsono molto intuitivi. primi due assiomi possono essere interpretati\nnel modo seguente. Si assegna il valore 0 ’“evento impossibile,”\novvero ’esito dell’esperimento casuale che non può verificarsi (ad\nesempio, il lancio di un dado sei facce produce 7), e si assegna il\nvalore 1 ’evento certo (ad esempio, il lancio di un dado sei facce\nproduce un numero compreso tra 1 e 6). Di conseguenza, la probabilità è\nun numero nell’intervallo \\([0, 1]\\).Il terzo assioma può essere compreso interpretando la probabilità come\nla frequenza relativa lungo termine del verificarsi di un evento. Se\ndue eventi sono incompatibili, allora la frequenza relativa dell’unione\ndi tali eventi è la somma delle due singole frequenze relative. Lo\nstesso si vale per la probabilità.","code":""},{"path":"che-cosè-la-probabilità.html","id":"assegnare-le-probabilità-agli-eventi","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.5 Assegnare le probabilità agli eventi","text":"È importante capire che l’approccio assiomatico non ci dice come sia\npossibile assegnare un valore di probabilità un evento definito \n\\(\\Omega\\). questo proposito esistono due diverse scuole di pensiero.","code":""},{"path":"che-cosè-la-probabilità.html","id":"approccio-frequentista","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.5.1 Approccio frequentista","text":"Una prima possibilità è di definire la nozione di probabilità termini\nempirici. La probabilità di un evento \\(\\) può essere concepita come il\nlimite cui tende la frequenza relativa dell’evento, al tendere\n’infinito del numero delle prove effettuate, ossia\n\\[\\begin{equation}\nP_A = \\lim_{n \\\\infty} \\frac{n_A}{n}.\n\\end{equation}\\]\nQuesto è l’approccio che abbiamo utilizzato precedenza, quando abbiamo discusso il gioco di De Méré.Tale definizione assume che l’esperimento possa essere ripetuto più\nvolte, idealmente infinite volte, sotto le medesime condizioni, e\ncorrisponde alla definizione frequentista di probabilità. Per\nl’approccio frequentista, dire che la probabilità di ottenere testa è\n0.5 significa affermare che l’evento “testa” verrebbe ottenuto nel 50%\ndei casi, se ripetessimo tantissime volte l’esperimento casuale del\nlancio di una moneta.Se non abbiamo disposizione informazioni empiriche proposito del\nverificarsi di un evento possiamo attribuire le probabilità agli eventi\nusando la nostra conoscenza della situazione. Tale approccio è seguito\ndalla definizione classica di probabilità base alla quale la\nprobabilità di un evento è il rapporto tra il numero di casi favorevoli\ne quelli possibili, supposto che tutti gli eventi siano equiprobabili,\nossia \\[P_A = \\frac{n_A}{n},\\] dove \\(n\\) è il numero di casi possibili e\n\\(n_A\\) è il numero di casi favorevoli per l’evento \\(\\). L’assunzione di\nequiprobabilità degli eventi elementari ha senso soprattutto nel caso\ndei giochi d’azzardo.base ’approccio frequentista, la probabilità è il limite cui\ntende una frequenza relativa empirica al crescere del numero di\nripetizioni dell’esperimento casuale. È molto facile utilizzare   per\ncalcolare una tale probabilità. Per esempio, se vogliamo calcolare la\nprobabilità di ottenere 3 nel lancio di un dado equilibrato, possiamo\neseguire la seguente simulazione.Il risultato è ovviamente molto simile \\(1/6\\).","code":"\nn <- 1e5\nx <- sample(1:6, n, replace = TRUE)\nx_01 <- ifelse(x == 3, 1, 0)\nmean(x_01)\n#> [1] 0.1676"},{"path":"che-cosè-la-probabilità.html","id":"approccio-bayesiano","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.5.2 Approccio Bayesiano","text":"Esistono però degli eventi per quali non è possibile calcolare una frequenza relativa, ovvero quelli che si verificano una volta soltanto. Che cos’è allora la probabilità questi casi? base ’approccio Bayesiano la probabilità è una misura del grado di plausibilità di una proposizione. Questa definizione è applicabile qualsiasi evento. Ciò consente di assegnare una probabilità anche proposizioni quali “il candidato \\(\\) vincerà le elezioni” oppure “l’accusato è innocente,” anche se non è possibile ripetere più volte un’elezione o un evento criminoso.Per assegnare le probabilità agli eventi, nell’approccio Bayesiano si\nutilizzano considerazioni “soggettive” che derivano dalle informazioni\ndi cui il soggetto è possesso. Il teorema di Bayes consente di\naggiustare, alla luce dei dati osservati, tali credenze “priori” per\narrivare alla probabilità posteriori. Quindi, tramite l’approccio\nBayesiano, si usa una stima del grado di plausibilità di una\nproposizione prima dell’osservazione dei dati, al fine di associare un\nvalore numerico al grado di plausibilità di quella stessa proposizione\nsuccessivamente ’osservazione dei dati. Questo processo di\n“aggiornamento Bayesiano” corrisponde ’inferenza statistica e verrà\ndiscusso dettaglio nel seguito delle dispense.","code":""},{"path":"che-cosè-la-probabilità.html","id":"proprietà-elementari-della-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.6 Proprietà elementari della probabilità","text":"Indipendentemente da come decidiamo di interpretare la probabilità (\ntermini frequentisti o Bayesiani), alla probabilità possono essere\nassegnate le seguenti proprietà.La probabilità dell’evento impossibile è zero:\n\\[P(\\emptyset) = 1 - P(\\Omega) = 0.\\]La probabilità dell’evento impossibile è zero:\n\\[P(\\emptyset) = 1 - P(\\Omega) = 0.\\]Se consideriamo due eventi \\(\\) e \\(B\\) tali che \\(\\subseteq B\\), cioè\nche \\(\\) è contenuto o coincidente con \\(B\\), da ciò segue che\n\\[P() \\leq P(B).\\]Se consideriamo due eventi \\(\\) e \\(B\\) tali che \\(\\subseteq B\\), cioè\nche \\(\\) è contenuto o coincidente con \\(B\\), da ciò segue che\n\\[P() \\leq P(B).\\]Se \\(^c\\) è il complementare dell’evento \\(\\), allora\n\\[P(^c) = 1 - P().\\]Se \\(^c\\) è il complementare dell’evento \\(\\), allora\n\\[P(^c) = 1 - P().\\]Dati \\(n\\) eventi \\(A_i\\) per \\(= 1, \\cdots, n\\), gli eventi si dicono\nindipendenti se risulta\n\\[P(A_i \\cap A_j \\cap \\cdots \\cap A_k) = P(A_i) P(A_j) \\cdots P(A_k).\\]Dati \\(n\\) eventi \\(A_i\\) per \\(= 1, \\cdots, n\\), gli eventi si dicono\nindipendenti se risulta\n\\[P(A_i \\cap A_j \\cap \\cdots \\cap A_k) = P(A_i) P(A_j) \\cdots P(A_k).\\]Se due eventi \\(\\) e \\(B\\) non sono disgiunti, allora quando sommiamo\nle loro probabilità dobbiamo evitare che la loro parte comune\n\\(\\cap B\\) venga contata due volte. Dati due eventi non\nnecessariamente disgiunti, dunque, la probabilità dell’unione è pari\nalla somma delle singole probabilità dei due eventi meno la\nprobabilità dell’intersezione:\n\\[\\begin{equation}\nP(\\text{ o } B) = P(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{10.2}\n\\end{equation}\\]Se due eventi \\(\\) e \\(B\\) non sono disgiunti, allora quando sommiamo\nle loro probabilità dobbiamo evitare che la loro parte comune\n\\(\\cap B\\) venga contata due volte. Dati due eventi non\nnecessariamente disgiunti, dunque, la probabilità dell’unione è pari\nalla somma delle singole probabilità dei due eventi meno la\nprobabilità dell’intersezione:\n\\[\\begin{equation}\nP(\\text{ o } B) = P(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{10.2}\n\\end{equation}\\]","code":""},{"path":"che-cosè-la-probabilità.html","id":"variabili-aleatorie","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.7 Variabili aleatorie","text":"Il concetto di “variabile aleatoria” è estremamente utile per estendere\nla nostra capacità di quantificare l’incertezza e di riassumere \nrisultati di un esperimento casuale. Le variabili aleatorie sono un\nconcetto fondamentale di tutta la teoria statistica; è quindi cruciale\ncapire quale sia il loro significano. Iniziamo con una definizione.Il dominio della variabile aleatoria \\(X\\) (che è una funzione) è dato dai\npunti dello spazio campionario \\(\\Omega\\). Ad ogni evento elementare\n\\(\\omega_i\\) attribuiamo il numero \\(X(\\omega_i)\\), ovvero il valore che la\nvariabile aleatoria assume sul risultato \\(\\omega_i\\) dell’esperimento\ncasuale. L’attributo “aleatoria” si riferisce al fatto che la variabile\nconsiderata trae origine da un esperimento di cui non siamo grado di\nprevedere l’esito con certezza.Mediante una variabile aleatoria trasformiamo lo spazio campionario\n\\(\\Omega\\), che genere è complesso, uno spazio campionario più\nsemplice formato da un insieme di numeri. Il maggior vantaggio di questa\nsostituzione è che molte variabili aleatorie, definite su spazi\ncampionari anche molto diversi tra loro, danno luogo ad una stessa\n“distribuzione” di probabilità sull’asse reale. Le variabili aleatorie\nsi indicano con le lettere maiuscole ed valori da esse assunti con le\nlettere minuscole.Ci sono due classi di variabili aleatorie: variabili aleatorie discrete\ne variabili aleatorie continue. Consideriamo innanzitutto il caso delle\nvariabili aleatorie discrete.Se \\(X\\) è una variabile aleatoria discreta allora l’insieme dei possibili\nvalori \\(x\\), tali per cui \\(P(X = x) > 0\\), viene detto “supporto” di \\(X\\).Alcuni esempi di variabili aleatorie discrete sono seguenti: il numero\ndi intrusioni di pensieri, immagini, impulsi indesiderabili un\npaziente OCD, il voto ’esame di Psicometria, la durata di vita di un\nindividuo, il numero dei punti che si osservano nel lancio di due dadi e\nil guadagno (la perdita) che un giocatore realizzerà \\(n\\) partite. Si\nnoti che, tutti questi casi, la variabile aleatoria considerata viene\nrappresentata mediante un numero.","code":""},{"path":"che-cosè-la-probabilità.html","id":"a-cosa-servono-le-variabili-aleatorie","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.7.1 A cosa servono le variabili aleatorie?","text":"Facendo riferimento agli esempi elencati sopra, possiamo chiederci\nperché questi numeri vengono considerati come “aleatori.” È ovvio che\nnoi non conosciamo, ad esempio, il voto di Psicometria di Mario Rossi\nprima del momento cui Mario Rossi avrà fatto l’esame. Le variabili\naleatorie si pongono il seguente problema: come possiamo descrivere le\nnostre opinioni rispetto al voto (possibile) di Mario Rossi, prima che\nlui abbia fatto l’esame. Prima dell’esame, il voto di Psicometria di\nMario Rossi si può solo descrivere facendo riferimento ad un insieme di\nvalori possibili. Inoltre, molto spesso, possiamo anche dire che tali\nvalori possibili non sono tutti egualmente verosimili: ci aspettiamo di\nosservare più spesso alcuni di questi valori rispetto agli altri. Le\nproprietà delle variabili aleatorie ci consentono di sistematizzare\nquesto tipo di opinioni. Ovviamente, una volta che Mario Rossi avrà\nfatto l’esame, questa materia non avrà più alcuna componente aleatoria.","code":""},{"path":"che-cosè-la-probabilità.html","id":"funzione-di-massa-di-probabilità","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.7.2 Funzione di massa di probabilità","text":"Per entrare nel merito di questa discussione, chiediamoci ora come sia\npossibile associare delle probabilità ai valori che vengono assunti\ndalle variabili aleatorie. Ad esempio, qual è la probabilità che Mario\nRossi ottenga 29 ’esame? Ci occuperemo qui del caso delle variabili\naleatorie discrete.Alle variabili aleatorie discrete vengono assegnale le probabilità\nmediante le cosiddette “distribuzioni di probabilità.” Una distribuzione\ndi probabilità è un modello matematico che collega ciascun valore di una\nvariabile aleatoria discreta alla probabilità di osservare un tale\nvalore un esperimento casuale. pratica, ad ognuno dei valori che\npossono essere assunti da una variabile aleatoria discreta viene\nassociata una determinata probabilità. La funzione che associa ad ogni\nvalore della variabile aleatoria una probabilità corrispondente si\nchiama “distribuzione di probabilità” oppure “legge di probabilità.”Una descrizione intuitiva del concetto di distribuzione di probabilità\npuò essere formulata nei termini seguenti. Possiamo pensare alla\nprobabilità come ad una quantità positiva che viene “distribuita”\nsull’insieme dei valori della variabile aleatoria. Tale “distribuzione”\n(suddivisione, spartizione) viene scalata maniera tale che ciascun\nelemento di essa corrisponda ad una proporzione del totale, nel senso\nche il valore totale della distribuzione è sempre pari 1. Una\ndistribuzione di probabilità non è dunque altro che un modo per\nsuddividere la nostra certezza (cioè 1) tra valori che la variabile\naleatoria può assumere. modo più formale, possiamo dire quanto segue.maniera più semplice, una distribuzione di (massa) di probabilità è\nformata dall’elenco di tutti valori possibili di una variabile\naleatoria discreta e dalle probabilità loro associate. Si noti che\n\\(P_{\\pi}(X=x)\\) è un numero positivo se il valore \\(x\\) è compreso nel\nsupporto di \\(X\\), altrimenti vale 0.Se \\(\\) è un sottoinsieme della variabile aleatoria \\(X\\), allora denotiamo\ncon \\(P_{\\pi}()\\) la probabilità assegnata ad \\(\\) dalla distribuzione\n\\(P_{\\pi}\\). Mediante una distribuzione di probabilità \\(P_{\\pi}\\) è\npossibile determinare la probabilità di ciascun sottoinsieme\n\\(\\subset X\\) come \\[P_{\\pi}() = \\sum_{x \\} P_{\\pi}(x).\\] Qui non\nfacciamo altro che applicare il terzo assioma di Kolmogorov. Soluzione.  Per risolvere tale problema iniziamo considerare il fatto che l’evento \\(S = 7\\) si verifica corrispondenza di sei punti elementari dello spazio campionario \\(\\Omega\\): {(1, 6), (2, 5), (3, 4), (4, 3), (2, 5), (6, 1)}. Dunque,\n\\[\\begin{equation}\nP(S = 7) = P\\{(1, 6)\\} + P\\{(2, 5)\\} + P\\{(3, 4)\\} + P\\{(4, 3)\\} + P\\{(2, 5)\\} + P\\{(6, 1)\\}.\n\\end{equation}\\]\nSe possiamo assumere che due dadi sono bilanciati, allora ciascun evento elementare dello spazio campionario ha probabilità \\(\\frac{1}{36}\\) e la probabilità cercata diventa \\(\\frac{1}{6}\\). È facile estendere il ragionamento fatto sopra tutti valori che \\(S\\) può assumere. questo modo giungiamo alla funzione di massa di probabilità \\(P_0\\) riportata nella prima riga della tabella seguente.Distribuzione di massa di probabilità per la somma dei punti\nprodotti dal lancio di due dadi bilanciati (\\(P_0\\)) e di due dadi\ntruccati (\\(P_1\\)).Per considerare un caso più generale, poniamoci ora il problema di\ntrovare la funzione di massa di probabilità di \\(S\\) nel caso di due dadi\ntruccati aventi la seguente distribuzione di probabilità:\n\\[\n\\begin{aligned}\nP(\\{(1, 6)\\}) = \\frac{1}{4};\\notag\\\\\nP(\\{(2, 3)\\}) = P(\\{4\\}) = P(\\{5\\}) = \\frac{1}{8}\\notag.\n\\label{eq:loaded_dice}\n\\end{aligned}\n\\]\nNel caso dei due dadi truccati, la probabilità dell’evento elementare (1, 1) è 1/4 1/4. Dunque, P(S = 2) = 4/64. La probabilità dell’evento elementare (1, 2) è 1/4 1/8. Tale valore è uguale alla probabilità dell’evento elementare (2, 1). La probabilità che S sia uguale 3 è 1/4 1/8 + 1/8 1/4 = 4/64, e così via. Svolgendo calcoli per tutti possibili valori di S otteniamo la funzione di massa di probabilità \\(P_1\\) riportata nella seconda riga della tabella precedente.","code":""},{"path":"che-cosè-la-probabilità.html","id":"notazione","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.8 Notazione","text":"Qui sotto è riportata la notazione che verrà usata per fare riferimento\nad eventi e probabilità, nel caso discreto e continuo, maniera tale\nche queste convenzioni siano elencate tutte un posto solo.Gli eventi sono denotati da lettere maiuscole, es. \\(\\), \\(B\\), \\(C\\).Una variabile aleatoria è denotata da una lettera maiuscola, ad\nesempio \\(X\\), e assume valori denotati dalla stessa lettera\nminuscola, ad esempio \\(x\\).La connessione tra eventi e valori viene espressa nei termini\nseguenti: “\\(X = x\\)” significa che l’evento \\(X\\) assume il valore \\(x\\).La probabilità di un evento è denotata con \\(P()\\).Una variabile aleatoria discreta ha una funzione di massa di\nprobabilità denotata con \\(p(x)\\). La relazione tra \\(P\\) e \\(p\\) è che\n\\(P(X=x) = p(x)\\).","code":""},{"path":"che-cosè-la-probabilità.html","id":"conclusioni-2","chapter":"Capitolo 10 Che cos’è la probabilità?","heading":"10.9 Conclusioni","text":"questo capitolo abbiamo visto come si costruisce lo spazio\ncampionario di un esperimento casuale, quali sono le proprietà di base\ndella probabilità e come si assegnano le probabilità agli eventi\ndefiniti sopra uno spazio campionario discreto. Abbiamo anche introdotto\nle nozioni di “variabile aleatoria” e di “funzione di massa di\nprobabilità.” Le procedure di analisi dei dati psicologici che\ndiscuteremo seguito faranno un grande uso di questi concetti e della\nnotazione qui introdotta.","code":""},{"path":"probabilità-condizionata.html","id":"probabilità-condizionata","chapter":"Capitolo 11 Probabilità condizionata","heading":"Capitolo 11 Probabilità condizionata","text":"L’attribuzione di una probabilità ad un evento è sempre condizionata\ndalle conoscenze che abbiamo disposizione. Per un determinato stato di\nconoscenze, attribuiamo ad un dato evento una certa probabilità di\nverificarsi; ma se il nostro stato di conoscenze cambia, allora cambierà\nanche la probabilità che attribuiamo ’evento questione. Per\nesempio, posiamo chiederci quale sia probabilità che Mario Rossi superi\nl’esame di Psicometria nel primo appello del presente anno accademico.\nassenza di altre informazioni, la migliore stima di tale probabilità\nè data dalla proporzione di studenti che hanno superato l’esame di\nPsicometria nel corrispondente appello dei passati anni accademici. Ma\nse sappiamo che Mario Rossi è particolarmente portato per le materie\nquantitative, ha un’ottima preparazione di base e ha studiato molto,\nallora la probabilità sarà sicuramente più alta.","code":""},{"path":"probabilità-condizionata.html","id":"sec:prob_cond","chapter":"Capitolo 11 Probabilità condizionata","heading":"11.1 Probabilità condizionata su altri eventi","text":"La probabilità condizionata è una componente essenziale del ragionamento\nscientifico dato che chiarisce come sia possibile incorporare le\nevidenze disponibili, maniera logica e coerente, nella nostra\nconoscenza del mondo. Infatti, si può pensare che tutte le probabilità\nsiano probabilità condizionate, anche se l’evento condizionante non è\nsempre esplicitamente menzionato. Consideriamo il seguente problema.\nFigura 11.1: Rappresentazione ad albero che riporta le frequenze attese dei risultati di una mammografia un campione di 1,000 donne\n\nNell’esercizio 11.1 la probabilità dell’evento “ottenere un risultato positivo al test” è una probabilità non condizionata, mentre la probabilità dell’evento “avere il cancro al seno, dato che il test ha\ndato un risultato positivo” è una probabilità condizionata. termini\ngenerali, la probabilità condizionata \\(P(\\mid B)\\) rappresenta la\nprobabilità che si verifichi l’evento \\(\\) sapendo che si è verificato\nl’evento \\(B\\); oppure: la probabilità di \\(\\) una prova valida solo se\nsi verifica anche \\(B\\). Ciò ci conduce alla seguente definizione.\nalcuni casi può essere conveniente leggere al contrario la\nformula 11.1 e utilizzarla per calcolare la probabilità\ndell’intersezione di due eventi. Per esempio se conosciamo la\nprobabilità dell’evento \\(B\\) e la probabilità condizionata di \\(\\) su \\(B\\),\notteniamo\n\\[\\begin{equation}\nP(\\cap B) = P(B)P(\\mid B),\n\\tag{11.2}\n\\end{equation}\\]\nmentre se conosciamo la probabilità dell’evento \\(\\) e la probabilità condizionata di \\(B\\) su \\(\\), otteniamo \\(P(\\cap B) = P()P(B \\mid )\\).","code":""},{"path":"probabilità-condizionata.html","id":"la-fallacia-del-pubblico-ministero","chapter":"Capitolo 11 Probabilità condizionata","heading":"11.1.1 La fallacia del pubblico ministero","text":"Un errore comune che si commette è quello di credere che \\(P(\\mid B)\\)\nsia uguale \\(P(B \\mid )\\). Tale fallacia ha particolare risalto \nambito forense tanto che è conosciuta con il nome di “fallacia del\nprocuratore” (prosecutor’s fallacy). essa, una piccola probabilità\ndell’evidenza, data l’innocenza, viene erroneamente interpretata come la\nprobabilità dell’innocenza, data l’evidenza.Consideriamo il caso di un esame del DNA. Un esperto forense potrebbe\naffermare, ad esempio, che “se l’imputato è innocente, c’è solo una\npossibilità su un miliardo che vi sia una corrispondenza tra il suo DNA\ne il DNA trovato sulla scena del crimine.” Ma talvolta questa\nprobabilità è erroneamente interpretata come avesse il seguente\nsignificato: “date le prove del DNA, c’è solo una possibilità su un\nmiliardo che l’imputato sia innocente.”Le considerazioni precedenti risultano più chiare se facciamo nuovamente\nriferimento ’esercizio 11.1. tale esercizio abbiamo visto come la probabilità di cancro dato un risultato positivo al test sia uguale 0.08. Tale probabilità è molto diversa dalla probabilità di un risultato\npositivo al test data la presenza del cancro. Infatti, questa seconda\nprobabilità è uguale 0.90 ed è descritta nel problema come una delle\ncaratteristiche del test questione.","code":""},{"path":"probabilità-condizionata.html","id":"legge-della-probabilità-composta","chapter":"Capitolo 11 Probabilità condizionata","heading":"11.2 Legge della probabilità composta","text":"Il teorema della probabilità composta deriva dal concetto di probabilità\ncondizionata per cui la probabilità che si verifichino due eventi \\(A_i\\)\ne \\(A_j\\) è pari alla probabilità di uno dei due eventi moltiplicato con\nla probabilità dell’altro evento condizionato al verificarsi del primo.L’equazione (11.2) si estende al caso di \\(n\\) eventi \\(A_1, \\dots, A_n\\) nella forma seguente:\n\\[\\begin{equation}\n\\begin{split}\nP(A_1 \\cap A_2 \\cap \\dots\\cap A_n) = {}& P(A_1)P(A_2 \\mid A_1)P(A_3 \\mid A_1 \\cap A_2) \\dots\\\\\n & P(A_n \\mid A_1 \\cap A_2 \\cap \\dots \\cap A_{n-1})\n\\end{split}\n\\tag{10.1}\n\\end{equation}\\]\nla quale esprime forma generale la legge della probabilità composta.Exercizio 11.3  Da un’urna contenente 6 palline bianche e 4 nere si estrae una pallina\nper volta, senza reintrodurla nell’urna. Indichiamo con \\(B_i\\) l’evento:\n“esce una pallina bianca alla \\(\\)-esima estrazione” e con \\(N_i\\)\nl’estrazione di una pallina nera. L’evento: “escono due palline bianche\nnelle prime due estrazioni” è rappresentato dalla intersezione\n\\(\\{B_1 \\cap B_2\\}\\) e la sua probabilità vale, per la (11.2)\n\\[\nP(B_1 \\cap B_2) = P(B_1)P(B_2 \\mid B_1).\n\\]\n\\(P(B_1)\\) vale 6/10, perché nella prima estrazione \\(\\Omega\\) è costituito da 10 elementi: 6 palline bianche e 4 nere. La probabilità condizionata \\(P(B_2 \\mid B_1)\\) vale 5/9, perché nella seconda estrazione, se è verificato l’evento \\(B_1\\), lo spazio campionario consiste di 5 palline bianche e 4 nere. Si ricava\npertanto:\n\\[\n  P(B_1 \\cap B_2) = \\frac{6}{10} \\cdot \\frac{5}{9} = \\frac{1}{3}.\n\\]\nmodo analogo si ha che\n\\[\nP(N_1 \\cap N_2) = P(N_1)P(N_2 \\mid N_1) = \\frac{4}{10} \\cdot \\frac{3}{9} = \\frac{4}{30}.\n\\]Se l’esperimento consiste nell’estrazione successiva di 3 palline, la\nprobabilità che queste siano tutte bianche vale, per\nla (10.1):\n\\[\nP(B_1 \\cap B_2 \\cap B_3)=P(B_1)P(B_2 \\mid B_1)P(B_3 \\mid B_1 \\cap B_2),\n\\]\ndove la probabilità \\(P(B_3 \\mid B_1 \\cap B_2)\\) si calcola supponendo che\nsi sia verificato l’evento condizionante \\(\\{B_1 \\cap B_2\\}\\). Lo spazio\ncampionario per questa probabilità condizionata è costituito da 4\npalline bianche e 4 nere, per cui \\(P(B_3 \\mid B_1 \\cap B_2) = 1/2\\) e\nquindi:\n\\[\nP (B_1 \\cap B_2 \\cap B_3) = \\frac{6}{10}\\cdot\\frac{5}{9} \\cdot\\frac{4}{8}  = \\frac{1}{6}.\n\\]","code":""},{"path":"probabilità-condizionata.html","id":"lindipendendenza-stocastica","chapter":"Capitolo 11 Probabilità condizionata","heading":"11.3 L’indipendendenza stocastica","text":"Un concetto molto importante per le applicazioni statistiche della\nprobabilità è quello dell’indipendenza stocastica. La\ndefinizione (11.1) esprime il concetto intuitivo di indipendenza\ndi un evento da un altro, nel senso che il verificarsi di \\(\\) non\ninfluisce sulla probabilità del verificarsi di \\(B\\), ovvero non la\ncondiziona. Infatti, per la definizione (11.1) di probabilità condizionata, si ha che, se \\(\\) e \\(B\\) sono due eventi indipendenti, risulta:\n\\[\nP(\\mid B) = \\frac{P()P(B)}{P(B)} = P().\\notag\n\\]\nPossiamo dunque dire che due eventi \\(\\) e \\(B\\) sono indipendenti se\n\\[\n\\begin{split}\nP(\\mid B) &= P(), \\\\\nP(B \\mid ) &= P(B).\n\\end{split}\n\\] Soluzione.  Rappresentiamo qui sotto lo spazio campionario dell’esperimento casuale.\nFigura 11.2: Rappresentazione dello spazio campionario dei risultati dell’esperimento casuale corrispondente al lancio di due dadi bilanciati. Sono evidenziati gli eventi elementari che costituiscono l’evento : esce un 1 o un 2 nel primo lancio.\nGli eventi e B non sono statisticamente indipendenti. Infatti, le\nloro probabilità valgono P() = 12/36 e P(B) = 5/36 e la probabilità\ndella loro intersezione è\n\\[\nP(\\cap B) = 1/36 = 3/108 \\neq P()P(B) = 5/108.\n\\]","code":""},{"path":"probabilità-condizionata.html","id":"conclusioni","chapter":"Capitolo 11 Probabilità condizionata","heading":"Conclusioni","text":"La probabilità condizionata è importante perché ci fornisce uno\nstrumento per precisare il concetto di indipendenza statistica. Una\ndelle più importanti domande delle analisi statistiche è infatti quella\nche si chiede se due variabili siano o meno associate. questo\ncapitolo abbiamo discusso il concetto di indipendenza (come contrapposto\nal concetto di associazione); nel capitolo ?? abbiamo descritto poi uno dei modi possibili che ci consentono di quantificare l’associazione tra due variabili. seguito vedremo come sia possibile fare inferenza sull’associazione tra variabili – ovvero, come stabilire il livello di\nfiducia nel verificarsi dell’evento esaminato nel campione un\ncontesto più ampio, cioè quello della popolazione.","code":""},{"path":"il-teorema-di-bayes.html","id":"il-teorema-di-bayes","chapter":"Capitolo 12 Il teorema di Bayes","heading":"Capitolo 12 Il teorema di Bayes","text":"Il teorema di Bayes ha un ruolo centrale nella statistica Bayesiana,\nanche se viene utilizzato anche dall’approccio frequentista. Prima di\nesaminare il teorema di Bayes introdurremo una sua componente, ovvero il\nteorema della probabilità totale.","code":""},{"path":"il-teorema-di-bayes.html","id":"il-teorema-della-probabilità-totale","chapter":"Capitolo 12 Il teorema di Bayes","heading":"12.1 Il teorema della probabilità totale","text":"Il teorema della probabilità totale fa uso della legge della probabilità\ncomposta (10.1) per calcolare le probabilità di casi più\ncomplessi di quelli considerati fino ad ora. La notazione sembra\ncomplessa, ma l’idea sottostante è semplice. Discutiamo qui il teorema\ndella probabilità totale considerando il caso di una partizione dello\nspazio campionario tre sottoinsiemi. È facile estendere tale\nsituazione al caso di una partizione un qualunque numero di\nsottoinsiemi.Il teorema della probabilità totale afferma che, se l’evento \\(E\\) è\ncostituito da tutti gli eventi elementari \\(E \\cap A_1\\), \\(E \\cap A_2\\)\ne \\(E \\cap A_3\\), allora la probabilità \\(P(E)\\) è data dalla somma delle\nprobabilità di queti tre eventi. Ciò è illustrato nella figura seguente.Exercizio 12.1  Si considerino tre urne, ciascuna delle quali contiene 100 palline:Urna 1: 75 palline rosse e 25 palline blu,Urna 2: 60 palline rosse e 40 palline blu,Urna 3: 45 palline rosse e 55 palline blu. Soluzione.  Lo spazio campionario è \\(\\Omega = \\{RR, RV, VR, VV\\}\\). Chiamiamo \\(R_1\\)\nl’evento “la prima pallina estratta è rossa,” \\(V_1\\) l’evento “la prima\npallina estratta è verde,” \\(R_2\\) l’evento “la seconda pallina estratta è\nrossa” e \\(V_2\\) l’evento “la seconda pallina estratta è verde.” Dobbiamo\ntrovare \\(P(R_2)\\) e possiamo risolvere il problema usando il teorema\ndella probabilità\ntotale (12.2):","code":""},{"path":"il-teorema-di-bayes.html","id":"sec:bayes_theorem","chapter":"Capitolo 12 Il teorema di Bayes","heading":"12.2 Il teorema della probabilità delle cause","text":"Il teorema di Bayes rappresenta uno dei fondamenti della teoria della\nprobabilità e della statistica. Lo presentiamo qui considerando prima un\ncaso specifico per poi descriverlo nella sua forma più generale.Sia \\(\\{A_1, A_2\\}\\) una partizione dello spazio campionario \\(\\Omega\\).\nConsideriamo un terzo evento \\(E \\subset \\Omega\\) con probabilità non\nnulla di cui si conoscono le probabilità condizionate rispetto ad \\(A_1\\)\ne \\(A_2\\), ovvero \\(P(E \\mid A_1)\\) e \\(P(E \\mid A_2)\\). È chiaro per le\nipotesi fatte che se si verifica \\(E\\) deve anche essersi verificato\nalmeno uno degli eventi \\(A_1\\) e \\(A_2\\). Supponendo che si sia verificato\nl’evento \\(E\\), ci chiediamo: qual è la probabilità che si sia verificato\n\\(A_1\\) piuttosto che \\(A_2\\)?Per rispondere alla domanda precedente scriviamo:\n\\[\\begin{equation}\n\\begin{aligned}\nP(A_1 \\mid E) &= \\frac{P(E \\cap A_1)}{P(E)}\\notag\\\\ \n&= \\frac{P(E \\mid A_1)P(A_1)}{P(E)}\\notag.\\end{aligned}\n\\end{equation}\\]\nSapendo che \\(E = (E \\cap A_1) \\cup (E \\cap A_2)\\) e che \\(A_1\\) e \\(A_2\\) sono eventi\ndisgiunti, ovvero \\(A_1 \\cap A_2 = \\emptyset\\), ne segue che possiamo\ncalcolare \\(P(E)\\) utilizzando il teorema della probabilità totale:\n\\[\\begin{equation}\n\\begin{aligned}\nP(E) &= P(E \\cap A_1) + P(E \\cap A_2)\\notag\\\\ \n     &= P(E \\mid A_1)P(A_1) + P(E \\mid A_2)P(A_2).\\notag\n\\end{aligned}\n\\end{equation}\\]\nSostituendo il risultato precedente nella formula della probabilità condizionata \\(P(A_1 \\mid E)\\) otteniamo:\n\\[\\begin{equation}\nP(A_1 \\mid E) = \\frac{P(E \\mid A_1)P(A_1)}{P(E \\mid A_1)P(A_1) + P(E \\mid A_2)P(A_2)}.\n\\tag{12.3}\n\\end{equation}\\]\nLa (12.3) si generalizza facilmente al caso di più di due eventi disgiunti, come indicato di seguito.La formula (12.4) prende il nome di Teorema di Bayes e mostra che\nla conoscenza del verificarsi dell’evento \\(E\\) modifica la probabilità\nche abbiamo attribuito ’evento \\(A_j\\).","code":""},{"path":"il-teorema-di-bayes.html","id":"aggiornamento-bayesiano","chapter":"Capitolo 12 Il teorema di Bayes","heading":"12.2.1 Aggiornamento Bayesiano","text":"Consideriamo ora un’altra applicazione del teorema di Bayes che ci fa\ncapire come l’applicazione di questo teorema ci consente di modificare\nuna credenza priori maniera dinamica, via via che nuove evidenze\nvengono raccolta, modo tale da formulare una credenza posteriori la\nquale non è mai definitiva, ma può essere sempre aggiornata base alle\nnuove evidenze disponibili. Questo processo si chiama aggiornamento\nBayesiano.Supponiamo che, per qualche strano errore di produzione, una fabbrica\nproduca due tipi di monete. Il primo tipo di monete ha la caratteristica\nche, quando una moneta viene lanciata, la probabilità di osservare\nl’esito “testa” è 0.6. Per semplicità, sia \\(\\theta\\) la probabilità di\nosservare l’esito “testa.” Per una moneta del primo tipo, dunque,\n\\(\\theta = 0.6\\). Per una moneta del secondo tipo, invece, la probabilità\ndi produrre l’esito “testa” è 0.4. Ovvero, \\(\\theta = 0.4\\). Noi\npossediamo una moneta, ma non sappiamo se è del primo tipo o del secondo\ntipo. Sappiamo solo che il 75% delle monete sono del primo tipo e il 25%\nsono del secondo tipo. Sulla base di questa conoscenza priori –\novvero sulla base di una conoscenza ottenuta senza avere eseguito\nl’esperimento che consiste nel lanciare la moneta una serie di volte per\nosservare gli esiti prodotti – possiamo dire che la probabilità di una\nprima ipotesi, secondo la quale \\(\\theta = 0.6\\), è 3 volte più grande\ndella probabilità di una seconda ipotesi, secondo la quale\n\\(\\theta = 0.4\\). Senza avere eseguito alcun esperimento casuale con la\nmoneta, questo è quello che sappiamo.Ora immaginiamo di lanciare una moneta due volte e di ottenere il\nrisultato seguente: \\(\\{T, C\\}\\). Quello che ci chiediamo è: sulla base di\nquesta evidenza, come cambiano le probabilità che associamo alle due\nipotesi? altre parole, ci chiediamo qual è la probabilità di ciascuna\nipotesi alla luce dei dati che sono stati osservati: \\(P(H \\mid x)\\),\nladdove \\(x\\) sono dati osservati. Tale probabilità si chiama\nprobabilità posteriori. Inoltre, se confrontiamo le due ipotesi, ci\nchiediamo quale valore assuma il rapporto\n\\(\\frac{P(H_1 \\mid x)}{P(H_2 \\mid x)}\\). Tale rapporto ci dice quanto è\npiù probabile \\(H_1\\) rispetto ad \\(H_2\\), alla luce dei dati osservati.\nInfine, ci chiediamo come cambia il rapporto definito sopra, quando\nosserviamo via via nuovi risultati prodotti dal lancio della moneta.Definiamo il problema maniera più chiara. Conosciamo le probabilità \npriori, ovvero \\(P(H_1) = 0.75\\) e \\(P(H_1) = 0.25\\). Quello che vogliamo\nconoscere sono le probabilità posteriori \\(P(H_1 \\mid x)\\) e\n\\(P(H_2 \\mid x)\\). Per trovare le probabilità posteriori applichiamo il\nteorema di Bayes: \\[P(H_1 \\mid x) = \\frac{P(x \\mid H_1) P(H_1)}{P(x)} = \n\\frac{P(x \\mid H_1) P(H_1)}{P(x \\mid H_1) P(H_1) + P(x \\mid H_2) P(H_2)},\\]\nladdove lo sviluppo del denominatore deriva da un’applicazione del\nteorema della probabilità totale. Inoltre, \\[P(H_2 \\mid x) = \n\\frac{P(x \\mid H_2) P(H_2)}{P(x \\mid H_1) P(H_1) + P(x \\mid H_2) P(H_2)}.\\]La probabilità \\(P(x \\mid H_1)\\) si chiama verosimiglianza e descrive la\nplausibilità dei dati osservati base ’ipotesi considerata. Se\nconsideriamo l’ipotesi \\(H_1\\) = “la probabilità di testa è 0.6,” allora\nla verosimiglianza dei dati \\(\\{T, C\\}\\) è \\(0.6 \\times 0.4 = 0.24.\\)\nDunque, \\(P(x \\mid H_1) = 0.24\\). Se invece consideriamo l’ipotesi \\(H_2\\) =\n“la probabilità di testa è 0.4,” allora la verosimiglianza dei dati\n\\(\\{T, C\\}\\) è \\(0.4 \\times 0.6 = 0.24\\), ovvero, \\(P(x \\mid H_2) = 0.24\\). \nbase alle due ipotesi \\(H_1\\) e \\(H_2\\), dunque, dati osservati hanno la\nmedesima plausibilità. Per semplicità, calcoliamo anche\n\\[\\begin{aligned}\nP(x) &= P(x \\mid H_1) P(H_1) + P(x \\mid H_2) P(H_2) = 0.24 \\cdot 0.75 + 0.24 \\cdot 0.25 = 0.24.\\notag\\end{aligned}\\]Le probabilità posteriori diventano: \\[\\begin{aligned}\nP(H_1 \\mid x) &= \\frac{P(x \\mid H_1) P(H_1)}{P(x)} = \\frac{0.24 \\cdot 0.75}{0.24} = 0.75,\\notag\\end{aligned}\\]\n\\[\\begin{aligned}\nP(H_2 \\mid x) &= \\frac{P(x \\mid H_2) P(H_2)}{P(x)} = \\frac{0.24 \\cdot 0.25}{0.24} = 0.25.\\notag\\end{aligned}\\]\nPossiamo dunque concludere dicendo che, sulla base dei dati osservati,\nl’ipotesi \\(H_1\\) ha una probabilità 3 volte maggiore di essere vera\ndell’ipotesi \\(H_2\\).È tuttavia possibile raccogliere più evidenze e, sulla base di esse, le\nprobabilità posteriori cambieranno. Supponiamo di lanciare la moneta\nuna terza volta e di osservare croce. nostri dati saranno dunque\n\\(\\{T, C, C\\}\\). Di conseguenza,\n\\(P(x \\mid H_1) = 0.6 \\cdot 0.4 \\cdot 0.4 = 0.096\\) e\n\\(P(x \\mid H_2) = 0.4 \\cdot 0.6 \\cdot 0.6 = 0.144\\). Ne segue che le\nprobabilità posteriori diventano: \\[\\begin{aligned}\nP(H_1 \\mid x) &= \\frac{P(x \\mid H_1) P(H_1)}{P(x)} = \\frac{0.096 \\cdot 0.75}{0.096 \\cdot 0.75 + 0.144 \\cdot 0.25} = 0.667,\\notag\\end{aligned}\\]\n\\[\\begin{aligned}\nP(H_2 \\mid x) &= \\frac{P(x \\mid H_2) P(H_2)}{P(x)} = \\frac{0.144 \\cdot 0.25}{0.096 \\cdot 0.75 + 0.144 \\cdot 0.25} = 0.333.\\notag\\end{aligned}\\]\nqueste circostanze, le evidenze che favoriscono \\(H_1\\) nei confronti\ndi \\(H_2\\) sono pari solo ad un fattore di 2.Se otteniamo ancora croce un quarto lancio della moneta, nostri\ndati saranno: \\(\\{T, C, C, C\\}\\). Ripetendo il ragionamento fatto sopra,\n\\(P(x \\mid H_1) = 0.6 \\cdot 0.4 \\cdot 0.4 \\cdot 0.4 = 0.0384\\) e\n\\(P(x \\mid H_2) = 0.4 \\cdot 0.6 \\cdot 0.6 \\cdot 0.6 = 0.0864\\). Dunque\n\\[\\begin{aligned}\nP(H_1 \\mid x) &= \\frac{0.0384 \\cdot 0.75}{0.0384 \\cdot 0.75 + 0.0864 \\cdot 0.25} = 0.571,\\notag\\end{aligned}\\]\n\\[\\begin{aligned}\nP(H_2 \\mid x) &= \\frac{0.0864 \\cdot 0.25}{0.0384 \\cdot 0.75 + 0.0864 \\cdot 0.25} = 0.429.\\notag\\end{aligned}\\]\ne le evidenze favore di \\(H_1\\) si riducono 1.33. Se si ottenesse un\naltro esito croce un sesto lancio della moneta, l’ipotesi \\(H2\\)\ndiventerebbe più probabile dell’ipotesi \\(H_1\\).conclusione, questo esercizio ci fa capire come sia possibile, sulla\nbase delle evidenze disponibili, passare da credenze priori credenze\nposteriori. Se prima di lanciare la moneta ritenevamo che l’ipotesi\n\\(H_1\\) fosse tre volte più plausibile dell’ipotesi \\(H_2\\), dopo avere\nosservato uno specifico campione di dati siamo giunti alla conclusione\nopposta. Il processo di aggiornamento Bayesiano ci fornisce dunque un\nmetodo per modificare il livello di fiducia una data ipotesi, alla\nluce di nuova informazione.","code":""},{"path":"il-teorema-di-bayes.html","id":"conclusioni","chapter":"Capitolo 12 Il teorema di Bayes","heading":"Conclusioni","text":"Il teorema di Bayes costituisce il fondamento dell’approccio più moderno\ndella statistica, quello appunto detto Bayesiano. Chi usa il teorema di\nBayes non è, solo per questo motivo, “bayesiano.” Ci vuole ben altro. Ci\nvuole un modo diverso per intendere il significato della probabilità e\nun modo diverso per intendere gli obiettivi dell’inferenza statistica.\nL’approccio bayesiano è stato, negli scorsi decenni, un approccio\npiuttosto dogmatico questi temi e, causa di ciò, è stato considerato\nda alcuni come un metodo un po’ troppo lontano dall’atteggiamento\ncritico e non dogmatico che costituisce il fondamento della comunità\nscientifica. anni recenti, questi aspetti più “ruvidi” dell’approccio\nbayesiano sono stati abbandonati e una gran parte della comunità\nscientifica riconosce ’approccio bayesiano il merito di consentire lo\nsviluppo di modelli anche molto complessi senza, d’altra parte,\nrichiedere conoscenze matematiche troppo avanzate ’utente. Per questa\nragione l’approccio bayesiano sta prendendo sempre più piede, anche \npsicologia. Un introduzione questi temi sarà presentata nell’ultima\nparte di queste dispense.","code":""},{"path":"probabilità-congiunta.html","id":"probabilità-congiunta","chapter":"Capitolo 13 Probabilità congiunta","heading":"Capitolo 13 Probabilità congiunta","text":"Finora abbiamo considerato unicamente le leggi di singole variabile\naleatorie. Tuttavia, psicologia e nella vita quotidiana, siamo spesso\ninteressati studiare problemi di probabilità legati al valore\ncongiunto di due o più variabili aleatorie. Ad esempio, potremmo\nmisurare il QI dei bambini e il loro peso alla nascita, o l’altezza e il\npeso delle giraffe, o il livello di inquinamento atmosferico e il tasso\ndi malattie respiratorie nelle città, o il numero di amici di Facebook e\nl’età. Che relazione tra le variabili ci possiamo aspettare ciascuno\ndi questi esempi? Perché?Per capire la relazione che sussiste tra due variabili aleatorie è\nnecessario calcolare gli indici di covarianza e correlazione. Per\nfare ciò è necessario utilizzare la funzione di probabilità congiunta.\nL’obiettivo di questo capitolo è quello di chiarire cosa si intende per\nfunzione di probabilità congiunta di due variabili casuali \\(X\\) e \\(Y\\).\nEsamineremo qui dettaglio il caso discreto.","code":""},{"path":"probabilità-congiunta.html","id":"funzione-di-probabilità-congiunta","chapter":"Capitolo 13 Probabilità congiunta","heading":"13.1 Funzione di probabilità congiunta","text":"Dopo aver trattato delle distribuzioni di probabilità di una variabile\naleatoria, che associa ad ogni evento elementare dello spazio\ncampionario uno ed un solo numero reale, è naturale estendere questo\nconcetto al caso di due o più dimensioni. Iniziamo descrivere il caso\ndiscreto con un esempio. Consideriamo qui l’esperimento casuale\ncorrispondente al lancio di tre monete equilibrate. Lo spazio\ncampionario di tale esperimento casuale è\n\\[\\Omega = \\{TTT, TTC, TCT, CTT, CCT, CTC, TCC, CCC\\}.\\] Dato che tre\nlanci sono tra loro indipendenti, non c’è ragione di aspettarsi che uno\ndegli otto risultati possibili dell’esperimento sia più probabile degli\naltri, dunque possiamo associare ciascuno degli otto eventi elementari\ndello spazio campionario la stessa probabilità, ovvero 1/8.Su tale spazio campionario consideriamo le variabili aleatorie\n\\(X \\\\{0, 1, 2, 3\\}\\), che conta il numero delle teste nei tre lanci, e\n\\(Y \\\\{0, 1\\}\\), che conta il numero delle teste al primo lancio.\nIndicando con T = ‘testa’ e C = ‘croce,’ si ottiene dunque la situazione\nriportata nella tabella successiva.Spazio campionario dell’esperimento consistente nel lancio di tre\nmonete equilibrate su cui sono state definite le variabili aleatorie\n\\(X\\) e \\(Y\\).Ci poniamo il problema di associare un livello di probabilità ad ogni\ncoppia \\((x, y)\\) definita su \\(\\Omega\\). La coppia \\((X = 0, Y = 0)\\) si\nrealizza corrispondenza di un solo evento elementare, ovvero CCC;\navrà dunque una probabilità pari \\(Pr(X=0, Y=0) = Pr(CCC) = 1/8\\). Nel\ncaso della coppia \\((X = 1, Y = 0)\\) ci sono due eventi elementari che\ndanno luogo al risultato considerato, ovvero, CCT e CTC; la probabilità\n\\(Pr(X=1, Y=0)\\) sarà dunque data dall’unione delle probabilità dei due\neventi elementari corrispondenti, cioé\n\\(Pr(X=1, Y=0) = Pr(CCT \\:\\cup\\: CTC) = 1/8 + 1/8 = 1/4\\). Riportiamo qui\nsotto calcoli svolti per tutti possibili valori di \\(X\\) e \\(Y\\).\n\\[\n\\begin{aligned}\nP(X = 0, Y = 0) &= P(\\omega_8 = CCC) = 1/8; \\notag\\\\\nP(X = 1, Y = 0) &= P(\\omega_5 = CCT) + P(\\omega_6 = CTC) = 2/8; \\notag\\\\\nP(X = 1, Y = 1) &= P(\\omega_7 = TCC) = 1/8; \\notag\\\\\nP(X = 2, Y = 0) &= P(\\omega_4 = CTT) = 1/8; \\notag\\\\\nP(X = 2, Y = 1) &= P(\\omega_3 = TCT) + P(\\omega_2 = TTC) = 2/8; \\notag\\\\\nP(X = 3, Y = 1) &= P(\\omega_1 = TTT) = 1/8; \\notag\n\\end{aligned}\n\\]Le probabilità così trovate possono essere riportate nella\ntabella seguente.Distribuzione di probabilità congiunta per risultati\ndell’esperimento consistente nel lancio di tre monete equilibrate.La tabella qui sopra ci fornisce il risultato che cercavamo, ovvero la distribuzione di probabilità congiunta delle variabili aleatorie \\(X\\) = “numero di realizzazioni con il risultato testa nei tre lanci” e \\(Y\\) = “numero di realizzazioni con il risultato testa nel primo lancio” per l’esperimento casuale considerato.\nUna generica funzione di probabilità congiunta bivariata può essere\nrappresentata come qui indicato.generale, possiamo dire che, dato uno spazio campionario discreto\n\\(\\Omega\\), è possibile associare ad ogni evento elementare \\(\\omega_i\\)\ndello spazio campionario una coppia di numeri reali \\((x, y)\\), essendo\n\\(x = X(\\omega)\\) e \\(y = Y(\\omega)\\), il che ci conduce alla seguente\ndefinizione.La funzione che associa ad ogni coppia \\((x, y)\\) un livello di\nprobabilità prende il nome di funzione di probabilità congiunta:\n\\[P(x, y) = P(X = x, Y = y).\\]\nIl termine “congiunta” deriva dal fatto che questa probabilità è legata\nal verificarsi di una coppia di valori, il primo associato alla\nvariabile aleatoria \\(X\\) ed il secondo alla variabile aleatoria \\(Y\\). Nel\ncaso di due sole variabili, si parla di distribuzione bivariata, mentre\nnel caso di più variabili si parla di distribuzione multivariata.","code":""},{"path":"probabilità-congiunta.html","id":"proprietà","chapter":"Capitolo 13 Probabilità congiunta","heading":"13.1.1 Proprietà","text":"Una distribuzione di massa di probabilità congiunta bivariata deve\nsoddisfare due proprietà:\\(0 \\leq p(x_i, y_j) \\leq 1\\);\\(0 \\leq p(x_i, y_j) \\leq 1\\);la probabilità totale deve essere uguale \\(1.0\\). Tale proprietà può\nessere espressa nel modo seguente\n\\[\\sum_{} \\sum_{j} p(x_i, y_j) = 1.0.\\]la probabilità totale deve essere uguale \\(1.0\\). Tale proprietà può\nessere espressa nel modo seguente\n\\[\\sum_{} \\sum_{j} p(x_i, y_j) = 1.0.\\]","code":""},{"path":"probabilità-congiunta.html","id":"eventi","chapter":"Capitolo 13 Probabilità congiunta","heading":"13.1.2 Eventi","text":"Si noti che dalla probabilità congiunta possiamo calcolare la\nprobabilità di qualsiasi evento definito base alle variabili\naleatorie \\(X\\) e \\(Y\\). Per capire come questo possa essere fatto,\nconsideriamo nuovamente l’esperimento discusso sopra.","code":""},{"path":"probabilità-congiunta.html","id":"funzioni-di-probabilità-marginali","chapter":"Capitolo 13 Probabilità congiunta","heading":"13.1.3 Funzioni di probabilità marginali","text":"Data la funzione di probabilità congiunta \\(p(x, y)\\) è possibile pervenire alla costruzione della funzione di probabilità della singola variabile aleatoria, \\(X\\) o \\(Y\\):\n\\[\np_X(x) = P(X = x) = \\sum_y p(x,y)\n\\]\n\\[\np_Y(y) = P(Y = y) = \\sum_x p(x,y)\n\\]\nche prendono, rispettivamente, il nome di funzione di probabilità marginale di \\(X\\) funzione di probabilità marginale di \\(Y\\). Si noti che \\(P_X\\) e \\(P_Y\\) sono normalizzate:\n\\[\n\\sum_x P_X(x) = 1.0, \\quad \\sum_y P_Y(y) = 1.0.\n\\]Per l’esperimento casuale consistente nel lancio di tre monete equilibrate, si calcolino le probabilità marginali di \\(X\\) e \\(Y\\).Nell’ultima colonna destra e nell’ultima riga basso della tabella seguente sono riportate le distribuzioni di probabilità marginali di \\(X\\) e \\(Y\\). \\(P_X\\) si ottiene sommando su ciascuna riga fissata la colonna \\(j\\), \\(P_X(X = j) = \\sum_y p_{xy}(x = j, y)\\). \\(P_Y\\) si trova sommando su ciascuna colonna fissata la riga \\(,\\) \\(P_Y (Y = ) = \\sum_x p_{xy}(x, y = )\\). Si noti che:\n\\[\n\\sum_x P_X(x) = 1, \\quad \\sum_y P_Y(y) = 1.\n\\]Distribuzione di probabilità congiunta \\(p(x,y)\\) per risultati\ndell’esperimento consistente nel lancio di tre monete equilibrate e\nprobabilità marginali \\(p(x)\\) e \\(p(y)\\).","code":""},{"path":"probabilità-congiunta.html","id":"indipendenza-stocastica","chapter":"Capitolo 13 Probabilità congiunta","heading":"13.1.4 Indipendenza stocastica","text":"Ora abbiamo tutti gli strumenti per potere dare una definizione\nstatistica precisa al concetto di indipendenza. La definizione proposta\nsarà necessariamente coerente con la definizione di indipendenza che\nabbiamo usato fino ad ora. Ma, espressa questi nuovi termini, potrà\nessere utilizzata indagini probabilistiche e statistiche più\ncomplesse. Ricordiamo che gli eventi \\(\\) e \\(B\\) si dicono indipendenti se\n\\(P (\\cap B)\\, = P() P(B)\\). Diciamo quindi che \\(X\\) e \\(Y\\) sono\nindipendenti se qualsiasi evento definito da \\(X\\) è indipendente da\nqualsiasi evento definito da \\(Y\\). La definizione formale che garantisce\nche ciò accada è la seguente.Le variabili aleatorie distribuite congiuntamente \\(X\\) e \\(Y\\) sono\nindipendenti se la loro distribuzione congiunta è il prodotto delle\ndistribuzioni marginali: \\[P(X, Y)\\, = P_X(x)P_Y(y).\\]Nel caso discreto, dunque, l’indipendenza implica che la probabilità\nriportata ciascuna cella della tabella di probabilità congiunta deve\nessere uguale al prodotto delle probabilità marginali di riga e di\ncolonna: \\[p(x_i, y_i)\\, = p_X(x_i) p_Y(y_i).\\notag\\]","code":""},{"path":"probabilità-congiunta.html","id":"conclusioni","chapter":"Capitolo 13 Probabilità congiunta","heading":"Conclusioni","text":"La funzione di probabilità congiunta tiene simultaneamente conto del\ncomportamento di due variabili aleatorie \\(X\\) e \\(Y\\) e di come esse si\ninfluenzano reciprocamente. particolare, si osserva che se le due\nvariabili non si influenzano, cioè se sono statisticamente indipendenti,\nallora la distribuzione di massa di probabilità congiunta si ottiene\ncome prodotto delle funzioni di probabilità marginali di \\(X\\) e \\(Y\\):\n\\(P_{X, Y}(x, y) = P_X(x) P_Y(y)\\).","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"distribuzione-campionaria-della-media-dei-campioni","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"Capitolo 14 Distribuzione campionaria della media dei campioni","text":"L’inferenza statistica può essere descritta come un insieme di operazioni sui dati che producono delle stime e delle affermazioni sul grado di incertezza che il ricercatore attribuisce alle sue previsioni e ai parametri di processi e/o popolazioni. L’obiettivo di questo capitolo è quello di fornire un’introduzione alle basi teoriche della stima e dell’interpretazione che può essere fornita alle inferenze statistiche.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"parametri-e-statistiche","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.1 Parametri e statistiche","text":"statistica, per popolazione si intende un insieme di elementi che\npresenta caratteristiche aleatorie, mentre per campione si intende un\nsottoinsieme della popolazione. Ma cosa corrisponde pratica la\npopolazione? Per uno psicologo la popolazione è un gruppo di individui.\nPer un biologo marino la popolazione è un gruppo di delfini, ad esempio.\nNella maggior parte dei casi, le popolazioni oggetto di interesse per \nricercatori sono insiemi di entità concrete che esistono nel mondo\nreale. Dal punto di vista della statistica, invece, le popolazioni sono\ndelle entità astratte. Infatti, gli statistici operazionalizzano il\nconcetto di “popolazione” nei termini di un oggetto matematico che\nconsente di essere manipolato con facilità. precedenza noi abbiamo\ngià incontrato questi oggetti matematici: sono le distribuzioni di\nprobabilità.L’idea è semplice. Supponiamo di occuparci del quoziente di\nintelligenza, QI. Abbiamo detto che, per uno psicologo, la popolazione\ndi interesse solitamente è un gruppo di individui, ciascuno dei quali è\ndotato di uno specifico punteggio del QI. Uno statistico “semplifica”\ntale situazione definendo maniera operativa la popolazione come la\ndistribuzione di densità rappresentata nella\nfigura 14.1. precedenza abbiamo visto infatti\ncome una distribuzione di densità non sia altro che la descrizione\nmatematica della “forma” di un istogramma che rappresenta un numero\nmolto alto di osservazioni.\nFigura 14.1: Grafico della distribuzione dei punteggi del QI nella popolazione.\ntest di intelligenza sono progettati modo che il QI medio sia pari\n100, la deviazione standard dei punteggi QI sia uguale 15 e la\ndistribuzione dei punteggi del QI sia normale. valori riportati sopra\nsono detti parametri quanto descrivono le proprietà dell’intera\npopolazione. Cioè, diciamo che la media della popolazione è \\(\\mu = 100\\)\ne la deviazione standard della popolazione è \\(\\sigma = 15\\). Dal punto di\nvista statistico, dunque, possiamo rappresentare questa ipotetica\npopolazione di valori del QI mediante l’oggetto matematico che\ncorrisponde una particolare distribuzione Normale:\\[\nQI \\sim \\mathcal{N}(\\mu = 100, \\sigma = 15).\n\\]\nSupponiamo ora di eseguire un esperimento nel quale il test di\nintelligenza viene somministrato 100 persone selezionate caso. Tale\ncampione casuale semplice consiste nel seguente insieme di 100 numeri:Tali valori sono stati trovati utilizzando la funzione rnorm() che genera numeri casuali estratti da una distribuzione normale. Nello specifico, abbiamo estratto 100 valori casuali dalla distribuzione normale con media 100 e deviazione standard 15. Se costruiamo un istogramma con dati di un tale campione otteniamo il grafico mostrato nella figura 14.2.\nFigura 14.2: Istogramma della distribuzione dei punteggi del QI un campione di 100 osservazioni.\nCome possiamo vedere, l’istogramma ha approssimativamente la forma corretta, ma è un’approssimazione molto cruda della distribuzione della popolazione mostrata nella figura 14.1. Se calcoliamo la media del campione, otteniamo un numero abbastanza vicino alla media della popolazione di 100, ma non identico: nel campione considerato la media e la deviazione standard sono uguali :Queste statistiche campionarie descrivono le proprietà di uno specifico campione che è stato osservato e, sebbene siano abbastanza simili ai parametri della popolazione, non\nsono uguali ad essi. generale, le statistiche campionarie sono ciò\nche è possibile calcolare partire dai dati osservati sul campione\nmentre parametri della popolazione sono ciò che vorremmo conoscere.","code":"\nlibrary(\"ggfortify\")\nggdistribution(dnorm, seq(60, 140, 0.1), mean = 100, sd = 15) +\n  labs(\n    x = \"Quoziente d'intelligenza\",\n    y = \"Densità di probabilità\"\n  )\nset.seed(123)\niq1 <- rnorm(100, 100, 15)\n# i valori QI sono numeri interi!\niq1 <- round(iq1) \niq1\n#>   [1]  92  97 123 101 102 126 107  81  90  93 118 105 106 102  92 127 107  71 111  93  84\n#>  [22]  97  85  89  91  75 113 102  83 119 106  96 113 113 112 110 108  99  95  94  90  97\n#>  [43]  81 133 118  83  94  93 112  99 104 100  99 121  97 123  77 109 102 103 106  92  95\n#>  [64]  85  84 105 107 101 114 131  93  65 115  89  90 115  96  82 103  98 100 106  94 110\n#>  [85]  97 105 116 107  95 117 115 108 104  91 120  91 133 123  96  85\ndata.frame(iq1) %>% \n  ggplot(aes(x = iq1)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(\n    x = \"Quoziente d'intelligenza\",\n    y = \"Densità\"\n  )\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nmean(iq1)\n#> [1] 101.42\nsd(iq1)\n#> [1] 13.66643"},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"la-legge-dei-grandi-numeri","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.2 La legge dei grandi numeri","text":"Nella sezione Parametri e statistiche abbiamo considerato risultati di un esperimento casuale nel quale sono stati osservati valori\nfittizi del QI di un campione di ampiezza \\(n = 100\\). risultati sono\nincoraggianti: la media campionaria di 101.42 ci fornisce\nun’approssimazione ragionevole della media della popolazione\n\\(\\mu = 100\\). molti studi un tale livello di precisione è accettabile,\nma altre situazioni è necessario essere più precisi.Cosa dobbiamo fare se vogliamo che le statistiche campionarie siano più\nvicine ai parametri della popolazione? La risposta è ovvia: dobbiamo\nraccogliere più dati. Supponiamo dunque di condurre un nuovo esperimento\nnel quale misuriamo il QI di 10000 persone. Possiamo simulare \nrisultati di questo esperimento usando R:Nella figura 14.3 è riportato l’istogramma dei valori del QI di\nquesto campione più numeroso. È chiaro che, questo secondo caso,\notteniamo un’approssimazione migliore rispetto al precedente campione\npiù piccolo. Ciò si riflette anche nelle statistiche del campione:Questi valori sono molto vicini ai parametri della popolazione.\nFigura 14.3: Istogramma della distribuzione dei punteggi del QI un campione di 10000 osservazioni.\nIl messaggio, un po’ banale, che ricaviamo questa simulazione è che,\ngeneralmente, campioni di dimensioni maggiori forniscono informazioni\nmigliori. Ho chiamato “banali” risultati di questa simulazione perché\ndovrebbe essere evidente tutti che le cose stanno così. Infatti,\nquesto punto è talmente ovvio che, quando Jacob Bernoulli – uno dei\nfondatori della teoria della probabilità – formalizzò questa idea nel\n1713, commentò il risultato nel modo seguente:Perché anche il più stupido degli uomini, basandosi soltanto sul suo istinto, da solo e senza alcuna istruzione (il che è notevole), è convinto che maggiore è il numero di osservazioni, minore è il pericolo di sbagliare.statistica questa intuizione va sotto il nome di Legge dei grandi\nnumeri. La Legge dei grandi numeri ci dice che la media aritmetica di\nun campione di \\(n\\) osservazioni (termini tecnici: di \\(n\\) variabili\naleatorie \\(X_i\\) indipendenti e identicamente distribuite), ovvero\n\\(\\frac{1}{n}\\sum_{=1}^nX_i\\), per \\(n\\) crescente tende o converge al\nvalore atteso teorico \\(\\mu\\). La Legge dei grandi numeri è uno degli\nstrumenti più importanti della statistica.","code":"\nset.seed(123)\niq2 <- rnorm(n = 10000, mean = 100, sd = 15) \niq2 <- round(iq2) \nhead(iq2)\n#> [1]  92  97 123 101 102 126\nmean(iq2)\n#> [1] 99.9671\nsd(iq2)\n#> [1] 14.9855\ndata.frame(iq2) %>% \n  ggplot(aes(x = iq2)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(\n    x = \"Quoziente d'intelligenza\",\n    y = \"Densità\"\n  )\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"distribuzione-campionaria","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.3 Distribuzione campionaria","text":"La Legge dei grandi numeri è uno strumento molto potente, ma non è\nsufficiente per rispondere tutte le nostre domande. Tutto ciò che ci\noffre è una “garanzia lungo termine.” Essa ci garantisce che, lungo\ntermine, le statistiche campionarie saranno corrette – le statistiche\ncampionarie forniranno la risposta esatta se verrà raccolta una quantità\ninfinita di dati. Ma come ha affermato John Maynard Keynes (1923) \neconomia, una garanzia lungo termine è di scarsa utilità nella vita\nreale:Il lungo periodo è una guida fuorviante per ciò che accade ora. Alla\nlunga saremo tutti morti. Gli economisti si sono dati un compito\ntroppo facile, troppo inutile, se nelle stagioni tempestose possono\nsolo dirci che, quando la tempesta sarà passata da un pezzo, l’oceano\nsarà di nuovo piatto.Come economia, così anche psicologia e nella statistica. Non è\nsufficiente sapere che, lungo termine, arriveremo alla risposta\ngiusta. È di scarso conforto sapere che un campione di dati\ninfinitamente grande ci fornisce il valore esatto della media della\npopolazione, quando il campione che possiamo ottenere qualsiasi\nsituazione pratica non può che avere una numerosità modesta.\nNell’attività pratica della ricerca psicologica, quindi, è necessario\nsapere qualcosa di più del comportamento delle statistiche campionarie\n(per esempio, la media) quando esse vengono calcolate partire da un\ncampione di dati molto più piccolo di quello ipotizzato dalla Legge dei\ngrandi numeri. Queste considerazioni ci portano alla necessità di\nformulare un nuovo concetto: quello di distribuzione campionaria\n(sampling distribution).","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"simulazione","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.3.1 Simulazione","text":"Tenendo mente quanto detto nella sezione precedente, abbandoniamo\nl’idea che nostri campioni siano grado di raggiungere numerosità\ndell’ordine di grandezza delle decine o delle centinaia di migliaia di\nosservazioni. Prendiamo invece esame una situazione più vicina \nquella cui gli psicologi si trovano ad operare. Consideriamo, quale\nesempio, un’ampiezza campionaria di \\(n = 5\\). Come precedenza,\npossiamo simulare questo esperimento casuale R, usando la funzione\nrnorm():Il QI medio questo campione risulta pari 108.2. Non sorprende che\nquesto risultato sia molto meno accurato rispetto ’esperimento casuale precedente.Immaginiamo ora di replicare l’esperimento; immaginiamo cioè di ripetere nuovamente la procedura descritta sopra: estraiamo un nuovo campione casuale e misuriamo il QI di 5 persone. Ancora una volta utilizziamo R per effettuare la simulazione:quest altro campione casuale il QI medio è 101.4. Procediamo \nquesto modo e simuliamo l’esperimento casuale dieci volte maniera tale da ottenere risultati seguenti.Iniziamo creando una lista di 10 campioni di ampiezza \\(n = 5\\).Trasformiamo la lista un data.frame.Le medie di ciascuno dei 10 campioni di ampiezza \\(n = 5\\) sono:Poniamoci ora il problema di replicare tante volte la procedura che ci porta calcolare la media dei valori del QI di cinque persone prese caso. Per ciascuna replica dell’esperimento casuale salviamo il valore della media campionaria. Così facendo, generiamo tanti valori, ciascuno dei quali corrisponde alla media di un campione casuale di 5 osservazioni. Usando poteri magici di R, possiamo eseguire una tale simulazione mediante le seguenti istruzioni:Nella figura 14.4 sono riportati risultati della simulazione. Come\nillustrato dalla figura, la media dei 5 punteggi del QI è solitamente\ncompresa tra 80 e 120. Ma il risultato più importante di questa\nsimulazione è quello che ci fa capire che, se ripetiamo l’esperimento\ncasuale più e più volte, otteniamo una distribuzione di medie\ncampionarie. Un tale distribuzione ha un nome speciale statistica: si\nchiama distribuzione campionaria della media.\nFigura 14.4: Istogramma della distribuzione delle medie dei punteggi del QI calcolate su 10000 campioni casuali di ampiezza \\(n=5\\).\nLa “distribuzione campionaria” è un importante concetto della statistica\ned è fondamentale per comprendere il comportamento dei piccoli campioni.\nQuando abbiamo eseguito per la prima volta l’esperimento casuale\nrelativo ’estrazione di cinque punteggi IQ dalla popolazione, abbiamo\ntrovato una media campionaria pari 101.42. Quello che impariamo dalla\ndistribuzione campionaria delle medie di campioni di ampiezza \\(n = 5\\) della\nfigura 14.4 è che un tale esperimento casuale è poco\naccurato. Infatti, la distribuzione campionaria della media dei campioni\ndi ampiezza \\(n=5\\) ci fa capire che, se ripetendo un tale esperimento\ncasuale tante volte, otteniamo delle medie campionarie con valori che\npossono essere compresi nell’intervallo tra 80 e 120. altre parole,\nla distribuzione campionaria della media di campioni di ampiezza 5 ci\ndice che il risultato dell’esperimento casuale (ovvero, la media\nosservata un singolo campione) varia di molto tra diversi campioni\nche possono essere estratti dalla popolazione. Di conseguenza, se il\nnostro obiettivo è quello di stimare la media della popolazione, allora\nnon dobbiamo fidarci troppo del risultato ottenuto per caso da un\nsingolo campione di numerosità \\(n\\) = 5. Nella discussione seguente mostreremo come sia possibile utilizzare la stima della distribuzione campionaria per descrivere le proprietà statistiche delle stime (ovvero, il grado di incertezza che è associato alle stime che otteniamo).La distribuzione campionaria può essere solo stimata","code":"\niq3 <- round(rnorm(n = 5, mean = 100, sd = 15))\niq3\n#> [1] 136  97 114  91 103\niq4 <- round(rnorm(n = 5, mean = 100, sd = 15))\niq4\n#> [1] 117 121  97  76  96\nmean(iq4)\n#> [1] 101.4\nset.seed(123)\nsample_list <- list()\nfor (i in 1:10) {\n  sample_list[[i]] <- round(rnorm(5, 100, 15))\n}\nsample_list[[1]]\n#> [1]  92  97 123 101 102\nsample_list[[2]]\n#> [1] 126 107  81  90  93\ndf <- data.frame(matrix(unlist(sample_list), nrow=length(sample_list), byrow=TRUE))\ndf\n#>     X1  X2  X3  X4  X5\n#> 1   92  97 123 101 102\n#> 2  126 107  81  90  93\n#> 3  118 105 106 102  92\n#> 4  127 107  71 111  93\n#> 5   84  97  85  89  91\n#> 6   75 113 102  83 119\n#> 7  106  96 113 113 112\n#> 8  110 108  99  95  94\n#> 9   90  97  81 133 118\n#> 10  83  94  93 112  99\nrowMeans(df)\n#>  [1] 103.0  99.4 104.6 101.8  89.2  98.4 108.0 101.2 103.8  96.2\nn_samples <- 10000\nsample_size <- 5\nsample_means <- rep(NA, n_samples)\n\nfor (i in 1:n_samples) {\n    y <- round(rnorm(5, 100, 15))\n    sample_means[i] <- mean(y)\n}\ndata.frame(sample_means) %>% \n  ggplot(aes(x = sample_means)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(\n    x = \"Media del quoziente d'intelligenza in campioni di ampiezza n = 5\",\n    y = \"Densità\"\n  )\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"distribuzione-campionaria-della-media","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4 Distribuzione campionaria della media","text":"Consideriamo ora l’inferenza statistica nel caso della statistica campionaria corrispondente alla media del campione. Denotiamo con \\(\\bar{X}_n\\) la media calcolata su un campione di \\(n\\) osservazioni. Abbiamo detto che, ogni volta che osserviamo un nuovo campione di ampiezza \\(n\\), la statistica \\(\\bar{X}_n\\) assumerà un valore diverso. termini tecnici diciamo che \\(\\bar{X}_n\\) è una variabile aleatoria, ovvero è una variabile che assume un nuovo valore ogni qualvolta l’esperimento casuale viene ripetuto (nel caso presente l’esperimento casuale corrisponde ’estrazione di un campione casuale dalla\npopolazione e al calcolo della media delle osservazioni campionarie). L’insieme dei valori che \\(\\bar{X}_n\\) può assumere tutti campioni casuali di ampiezza \\(n\\) che possono essere estratti dalla popolazione è detto distribuzione campionaria della media.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"valore-atteso-della-media-campionaria","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4.1 Valore atteso della media campionaria","text":"Qual è la media (valore atteso) della distribuzione campionaria della\nmedia? È facile mostrare che \\(\\mu_{\\bar{X}_n}\\) coincide con il valore\nmedio \\(\\mu\\) della popolazione da cui campioni di ampiezza \\(n\\) sono\nstati estratti.Dimostrazione. Ponendo \\(\\bar{X}_n = S_n/n\\), dove \\(S_n = X_1 + X_2 + \\dots + X_n\\) è la somma di \\(n\\) variabili aleatorie iid, ne segue che:\n\\[\n\\mathbb{E}(\\bar{X}_n) = \\frac{1}{n} \\mathbb{E}(S_n) = \\frac{1}{n} \\mathbb{E}(X_1 + X_2 + \\dots + X_n ) =  \\frac{1}{n} n \\mu = \\mu.\n\\]","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"varianza-della-media-campionaria","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4.2 Varianza della media campionaria","text":"Qual è la varianza della distribuzione campionaria della media? Anche \nquesto caso si può facilmente mostrare come la varianza della\ndistribuzione delle medie campionarie è legata alla varianza \\(\\sigma^2\\)\ndella popolazione dalla seguente relazione:\\[\nvar(\\bar{X}_n) = \\frac{\\sigma^2}{n},\n\\tag{14.1}\n\\]\ndove \\(n\\) è la numerosità dei campioni casuali.Prima di presentare la dimostrazione dell’eq. (14.1) è necessario ricordare la seguente proprietà della varianza: se una variabile aleatoria \\(X\\) viene moltiplicata per una costante \\(\\), la varianza della variabile aleatoria \\(aX\\) diventa\n\\[\nvar(X) = ^2 var(X).\n\\]\nPossiamo ora comprendere la dimostrazione seguente.Dimostrazione. \\[\nvar(\\bar{X}_n) = \\frac{1}{n^2} var(S_n) = \\frac{1}{n^2} n \\sigma^2 \n= \\frac{\\sigma^2}{n}.\n\\]due risultati che abbiamo ottenuto sopra sono molto importanti. Il primo ci dice che la media campionaria è uno stimatore corretto (ovvero, non distorto) della media della popolazione. Il secondo quantifica l’errore medio che compiamo usando usiamo la media del campione quale stima della media della popolazione.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"errore-standard","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4.3 Errore standard","text":"La radice quadrata della varianza della distribuzione campionaria della\nmedia si chiama errore standard della media campionaria. Questa è una\nquantità molto importante perché ci informa sul livello di incertezza\ndella nostra stima fornendoci un valore che ha la stessa unità di misura\ndelle osservazioni. Se vogliamo stimare la media della popolazione\nutilizzando la media del campione quale stimatore ci possiamo aspettare\ndi compiere un errore medio pari \\(\\frac{\\hat{\\sigma}_n}{\\sqrt{n}},\\)\nladdove \\(\\hat{\\sigma}_n\\) è la deviazione standard del campione\nutilizzata quale stima della deviazione standard della popolazione.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"simulazione-1","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4.3.1 Simulazione","text":"Per chiarire le due conclusioni precedenti, utilizziamo nuovamente la simulazione che abbiamo eseguito precedenza, quando abbiamo generato 10000 medie campionarie per campioni di ampiezza \\(n = 5\\) estratti dalla popolazione \\(\\mathcal{N}(\\mu = 100, \\sigma = 15\\)). La distribuzione di tali medie è rappresentata nella figura 14.4. realtà, quella fornita dalla figura 14.4 non è esattamente la distribuzione campionaria delle medie di campioni casuali di ampiezza \\(n=5\\) estratti dalla popolazione \\(\\mathcal{N}(\\mu = 100, \\sigma = 15\\)): la vera distribuzione campionaria della media si otterrebbe estraendo infiniti campioni di ampiezza \\(n = 5\\) dalla popolazione. Tuttavia, avendo disposizione le medie di 10000 campioni, ci possiamo aspettare un risultato empirico non troppo diverso da quello teorico. Verifichiamo dunque le due conclusioni cui siamo giunti sopra.Sappiamo che la media delle 10000 medie di campioni di ampiezza \\(n=5\\) dovrà essere molto simile (anche se non identica, dato che il numero dei campioni è grande, ma non infinito) alla media della popolazione. Infatti, questa simulazione, abbiamo che \\(\\hat{\\mu}_{\\bar{X}_n} =\\) 99.97 contro un valore teorico \\(\\mu=100\\). ’aumentare del numero di campioni estratti \\(\\mu_{\\bar{X}_n}\\) diventa sempre più simile \\(\\mu\\).Calcoliamo ora la deviazione standard (detta errore standard) delle 10000 medie campionarie che abbiamo trovato. Nella simulazione, tale valore è pari 6.663 mentre il valore teorico è \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{15}{\\sqrt{5}} = 6.708\\). Possiamo dunque dire che, con 10000 medie campionarie le proprietà della distribuzione campionaria della media vengono approssimate molto bene.Si noti che possiamo attribuire \\(\\sigma_{\\bar{X}}\\) la stessa interpretazione che è possibile fornire, generale, alla deviazione standard. Nel caso di un campione, la deviazione standard \\(\\sigma\\) ci dice di quanto, media, valori osservati sono lontani dalla media. Nel caso della distribuzione campionaria delle medie dei campioni, \\(\\sigma_{\\bar{X}}\\) ci dice quale errore medio compiamo stimando \\(\\mu\\) con \\(\\bar{X}\\). altre parole, ci dice che, se considerassimo tutte le medie \\(\\bar{X}\\) che si possono calcolare sulla base degli infiniti campioni di dimensioni \\(n\\) che possiamo estrarre dalla popolazione, la distanza media tra ciascuna di queste medie e la media della distribuzione (che corrisponde alla media della popolazione) è pari \\(\\sigma_{\\bar{X}}\\). La quantità \\(\\sigma_{\\bar{X}}\\) può dunque essere considerata come una misura di errore nella stima di \\(\\mu\\) mediante \\(\\bar{X}\\).","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"distribuzioni-delle-statistiche-campionarie","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.4.4 Distribuzioni delle statistiche campionarie","text":"Qualunque statistica campionaria ha una sua distribuzione teorica. Consideriamo, ad esempio, il massimo del campione quale statistica campionaria di interesse. Ripetiamo la simulazione che abbiamo descritto sopra calcolando, questa volta, il valore massimo del campione.risultati di questa simulazione sono riportati nella figura 14.5.\nFigura 14.5: Istogramma della distribuzione del QI massimo osservato ciascun campione casuali di ampiezza \\(n=5\\). Per creare la figura sono stati considerati 10000 campioni casuali.\nNon dovrebbe sorprenderci che, prendendo 5 persone caso per poi selezionare la persona con il punteggio QI più alto, otteniamo una distribuzione che, rispetto alla distribuzione della figura 14.5, è traslata verso destra. Nella presente simulazione, la distribuzione del QI massimo di un campione casuale di ampiezza \\(n = 5\\) si situa approssimativamente nell’intervallo compreso tra 90 e 150.","code":"\nset.seed(123)\nn_samples <- 10000\nsample_size <- 5\nsample_max <- rep(NA, n_samples)\n\nfor (i in 1:n_samples) {\n    y <- round(rnorm(5, 100, 15))\n    sample_max[i] <- max(y)\n}\ndata.frame(sample_max) %>% \n  ggplot(aes(x = sample_max)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(\n    x = \"Valore massimo del QI in campioni di ampiezza n = 5\",\n    y = \"Densità\"\n  )\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"sec:tlc","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.5 Teorema del limite centrale","text":"Chiediamoci ora quale sia la relazione che intercorre tra la distribuzione campionaria della media e l’ampiezza \\(n\\) dei campioni. ciascun pannello della figura 14.6 sono riportati risultati di una simulazione nella quale sono stati generati 10000 campioni di ampiezza \\(n\\) per poi calcolare il QI medio ciascun campione.\nFigura 14.6: Nel primo pannello alto sinistra ciascun campione contiene una sola osservazione, per cui la media del campione è identica al valore del QI di una persona. Di conseguenza, la distribuzione campionaria della media è identica alla distribuzione dei valori del QI nella popolazione. Quando \\(n=2\\) la media di ciascun campione tende ad essere più simile alla media della popolazione di quanto lo sia ciascuna singola osservazione della popolazione. Quindi anche l’ampiezza dell’istogramma (ovvero, la distribuzione campionaria della media) diminuisce, se confrontata con la dispersione della popolazione. Quando giungiamo ad una numerosità campionaria pari \\(n=30\\) vediamo che la maggior parte delle medie campionarie tende ad addensarsi intorno alla media della popolazione.\nGli istogrammi mostrano la distribuzione delle medie così ottenute, cioè ci forniscono una\nrappresentazione grafica della distribuzione campionaria della media al variare dell’ampiezza campionaria \\(n\\). punteggi del QI sono stati ricavati da una distribuzione normale con media 100 e deviazione standard 15 e tale distribuzione viene visualizzata con una linea nera continua ciascun pannello della figura 14.6.Quello che ci chiediamo è come varia la distribuzione campionaria della\nmedia funzione dell’ampiezza del campione. Intuitivamente, conosciamo\ngià parte della risposta. Se abbiamo disposizione solo poche\nosservazioni, è probabile che la media campionaria sia abbastanza\nimprecisa: se ripetiamo l’esperimento casuale del campionamento e\nricalcoliamo la media del campione, otteniamo una risposta molto diversa\nad ogni ripetizione dell’esperimento casuale. Di conseguenza, la\ndistribuzione campionaria della media comprenderà una gamma di valori\nmolto grande. Invece, si ottengono risultati molto simili tra loro se\nripetiamo l’esperimento del campionamento utilizzando campioni di grandi\ndimensioni. questo secondo caso, la distribuzione campionaria\nincluderà una gamma di valori delle medie molto minore che \nprecedenza. Questo andamento si può notare nei pannelli della\nfigura 14.6: l’errore standard della media campionaria diminuisce ’aumentare dell’ampiezza del campione.Ciò che abbiamo descritto finora, tuttavia, riguarda solo un aspetto di\nquello che accade alla distribuzione campionaria di \\(\\bar{X}\\)\n’aumentare di \\(n\\). Gli esempi discussi finora erano relativi al caso\ndi campioni casuali del QI. Poiché punteggi del QI seguono\napprossimativamente una distribuzione normale, abbiamo assunto che anche\nla popolazione abbia una distribuzione normale. Tuttavia, si presentano\nspesso casi cui la distribuzione della popolazione non è normale. \nqueste circostanze, cosa succede alla distribuzione campionaria della\nmedia? La cosa straordinaria è questa: non importa quale sia la forma\ndella distribuzione della popolazione, ’aumentare della dimensione\ncampionaria \\(n\\), la distribuzione di frequenza delle medie campionarie\nsi approssima sempre più alla tipica forma campana di una\ndistribuzione normale.Per farci un’idea di quello che succede, eseguiamo alcune simulazioni usando R.Consideriamo la distribuzione della popolazione rappresentata dall’istogramma riportato nella figura 14.7. Confrontando l’istogramma triangolare con la curva campana tracciata dalla linea nera risulta chiaro che la distribuzione della popolazione non assomiglia affatto una distribuzione normale.\nFigura 14.7: Dimostrazione del Teorema del limite centrale. Consideriamo una popolazione che non segue la distribuzione normale. La distribuzione di tale popolazione è rappresentata dall’istogramma grigio.\nuna prima simulazione, ho estratto 50000 campioni di ampiezza \\(n=2\\) da questa distribuzione e, per ciascuno di essi ho calcolato la media campionaria. Come si può vedere nella figura 14.8, la distribuzione campionaria non è triangolare. Certamente non è Normale, ma assomiglia di più ad una distribuzione campanulare di quanto assomigli alla distribuzione della popolazione raffigurata nella figura 14.7.\nFigura 14.8: Distribuzione campionaria di \\(ar{X}\\) per campioni casuali di ampiezza \\(n=2\\) estratti dalla popolazione rappresentata nella figura 1.7.\nQuando aumento la numerosità del campione \\(n=4\\) la distribuzione campionaria della media si approssima abbastanza bene alla normale, figura 14.9.\nFigura 14.9: Distribuzione campionaria di \\(ar{X}\\) per campioni casuali di ampiezza \\(n=4\\) estratti dalla popolazione rappresentata nella figura 1.7.\nGià con \\(n=8\\) l’approssimazione diventa molto buona, come indicato nella figura 14.10.\nFigura 14.10: Distribuzione campionaria di \\(ar{X}\\) per campioni casuali di ampiezza \\(n=8\\) estratti dalla popolazione rappresentata nella figura 1.7.\naltre parole, se la dimensione del campione non è piccola, allora la distribuzione campionaria della media sarà approssimativamente normale indipendentemente dalla distribuzione della popolazione! Questo comportamento della distribuzione campionaria di \\(\\bar{X}\\) al variare di \\(n\\) viene descritto maniera formale dal Teorema del limite centrale.Il Teorema del limite centrale ci dice che, se vengono selezionati campioni sufficientemente grandi (tipicamente è sufficiente che \\(n > 30\\) purché il carattere osservato non sia troppo asimmetrico), allora la media campionaria \\(\\bar{X}\\) di \\(n\\) variabili aleatorie indipendenti \\(X_1, X_2, \\dots\\) converge distribuzione ad una variabile aleatoria normale di media \\(\\mu\\) e varianza \\(\\sigma^2/n\\).È altresì molto importante notare che, se le variabili di partenza \\(X_1\\), \\(X_2\\), …\\(X_n\\) sono esse stesse Normali, tutte con lo stesso valore atteso \\(\\mu\\) e la stessa varianza \\(\\sigma^2\\), allora il Teoremadel limite centrale è esatto. Ovvero per ogni \\(n\\),\\[\n\\bar{X}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nQuesta proprietà discende dal seguente teorema.conclusione, il Teorema del limite centrale ci consente di specificare completamente le proprietà della distribuzione campionaria di \\(\\bar{X}_n\\).Se la popolazione è normale, allora \\(\\bar{X}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\) indipendentemente da \\(n\\).Se la popolazione è normale, allora \\(\\bar{X}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\) indipendentemente da \\(n\\).Se invece la popolazione non è normale, allora la distribuzione di \\(\\bar{X}_n\\) tende \\(\\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\) al crescere di \\(n\\).Se invece la popolazione non è normale, allora la distribuzione di \\(\\bar{X}_n\\) tende \\(\\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\) al crescere di \\(n\\).Esaminiamo ora un esercizio cui viene applicato il TLC.Supponiamo di misurare un oggetto con una bilancia non molto precisa. Supponiamo inoltre che l’errore di misura \\(E\\) della bilancia si distribuisca maniera Normale con media \\(0\\) e deviazione standard \\(\\sigma = 2\\) grammi. Se l’oggetto considerato ha un peso uguale \\(w\\), il peso osservato \\(X\\) sarà dato dalla somma del suo peso vero e l’errore di misurazione: \\(X = w + E\\). Dato che \\(w\\) è una costante, \\(X\\) seguirà la distribuzione normale con media \\(\\mathbb{E}(X) = \\mathbb{E}(w + E) = w + \\mathbb{E}(E) = w\\) e varianza \\(var(X) = var(w + E) = var(E) = 4\\). Qual è la probabilità di ottenere una misurazione che non differisce di più di un grammo dal peso vero?Dobbiamo trovare la probabilità\\[\n\\begin{aligned}\nP(-1 \\leq X - w \\leq 1)  &= P\\bigg(-\\frac{1}{2} \\leq \\frac{X - w}{\\sigma} \\leq \\frac{1}{2}\\bigg)\\notag\\\\ &= P\\bigg(-\\frac{1}{2} \\leq Z \\leq \\frac{1}{2}\\bigg)\\notag\n\\end{aligned}\n\\]\novveroConsiderando l’evento complementare, possiamo dunque dire che c’è una probabilità maggiore di \\(0.6\\) che la bilancia produca un valore che differisce di almeno un grammo dal peso vero.Chiediamoci ora cosa succede se, invece di accontentarci di una singola misurazione, calcoliamo la media di \\(n = 10\\) misurazioni. questo secondo caso,\\[\n\\begin{aligned}\nP\\left(-1 \\leq \\frac{S_{10}}{10} - w \\leq 1\\right) \n&= P\\bigg(-\\frac{1}{\\sqrt{4/10}} \\leq \\frac{\\frac{S_{10}}{10} - w}{\\sigma/\\sqrt{10}} \\leq \\frac{1}{\\sqrt{4/10}}\\bigg)\\notag\\\\ \n&= P\\bigg(-\\frac{\\sqrt{10}}{2} \\leq Z \\leq \\frac{\\sqrt{10}}{2}\\bigg)\\notag\n\\end{aligned}\n\\]\novveroConsiderando l’evento complementare, possiamo concludere che c’è una probabilità pari solo 0.114 che la media di 10 misurazioni assuma un valore che differisce di più di un grammo dal peso vero. È dunque ovvio che le medie di misurazioni ripetute sono migliori delle singole misure.","code":"\npar(mfrow=c(2, 3))\n\nmu <- 100\nsigma <- 15\nnrep <- 1e5\n\nqi <- rep(NA, nrep)\n\nget_mean <- function(nobs, mu, sigma) {\n  x <- round(rnorm(n = nobs, mean = mu, sd = sigma))\n  mean(x) \n}\n\nymax <- 0.14\n\nnobs <- 1\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 1\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\nnobs <- 2\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 2\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\nnobs <- 3\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 3\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\nnobs <- 5\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 5\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\nnobs <- 15\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 15\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\nnobs <- 30\nqi <- replicate(nrep, get_mean(nobs, mu, sigma))\nhist(qi, freq=FALSE,\n     yaxt='n', \n     ylim=c(0, ymax),\n     xlim = c(40, 160),\n     ylab = \"\", xlab = \"QI\", main = \"n = 30\")\ncurve(dnorm(x, mean=mu, sd=sigma), add=TRUE, yaxt=\"n\")\n\npar(mfrow=c(1, 1))\n# needed for printing\nwidth <- 6\nheight <- 6\n\n# parameters of the beta\na <- 2\nb <- 1\n\n# mean and standard deviation of the beta\ns <- sqrt(a * b / (a + b)^2 / (a + b + 1))\nm <- a / (a + b)\n\n# define function to draw a plot\nplot_one <- function(n, N = 50000) {\n\n  # generate N random sample means of size n\n  X <- matrix(rbeta(n * N, a, b), n, N)\n  X <- colMeans(X)\n\n  # plot the data\n  hist(\n    X,\n    breaks = seq(0, 1, .025), border = \"white\", freq = FALSE,\n    #col = ifelse(colour, emphColLight, emphGrey),\n    col = \"gray\",\n    xlab = \"Media campionaria\", ylab = \"\", xlim = c(0, 1.2),\n    main = paste(\"n =\", n), axes = FALSE,\n    font.main = 1, ylim = c(0, 5)\n  )\n  #box()\n  axis(1)\n  # axis(2)\n\n  # plot the theoretical distribution\n  lines(x <- seq(0, 1.2, .01), dnorm(x, m, s / sqrt(n)),\n    lwd = 2, col = \"black\", type = \"l\"\n  )\n}\nplot_one(1)\nplot_one(2)\nplot_one(4)\nplot_one(8)\npnorm(0.5, 0, 1) - pnorm(-0.5, 0, 1)\n#> [1] 0.3829249\npnorm(sqrt(10)/2, 0, 1) - pnorm(-sqrt(10)/2, 0, 1)\n#> [1] 0.8861537"},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"intervalli-di-confidenza","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.6 Intervalli di confidenza","text":"","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"parametri-di-un-modello-statistico","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.6.1 Parametri di un modello statistico","text":"Nel gergo statistico, parametri sono valori sconosciuti che determinano un modello statistico. Si consideri il modello statistico \\(Y \\sim \\mathcal{N}(\\mu, \\sigma)\\). Il modello statistico precedente ci dice che \\(Y\\) è una v.. distribuita come una normale di parametri \\(\\mu\\) e \\(\\sigma\\). Supponiamo che la \\(Y\\) sia il QI. questo caso è facile capire cosa sono parametri \\(\\mu\\) e \\(\\sigma\\). Quello del QI, infatti, è un caso particolare perché il test di intelligenza Wechsler Adult Intelligence Scale (WAIS) è stato costruito modo tale da produrre dei dati che si distribuiscono un modo noto: il QI segue la distribuzione normale di parametri \\(\\mu = 100\\) e \\(\\sigma = 15\\). generale, però, parametri di un modello statistico sono sconosciuti.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"lincertezza-della-stima","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"14.6.2 L’incertezza della stima","text":"Dato che parametri sono, genere, sconosciuti, è necessario stimarli. Non è sufficiente, però, ottenere una stima puntuale di un parametro. È anche necessario quantificare l’incertezza della stima. L’incertezza della stima viene descritta dall’approccio frequentista nei termini di un intervallo di confidenza. L’intervallo di confidenza si costruisce mediante l’errore standard.L’errore standard è la deviazione standard stimata della stima di un parametro e quantifica il grado della nostra incertezza sulla quantità di interesse. Chiariamo questa idea facendo riferimento alla la figura 14.4. Tale figura riporta la distribuzione di un grande numero di medie campionarie, laddove la media di ciascun campione può essere considerata come una stima della media \\(\\mu\\) della popolazione. La deviazione standard di queste stime, chiamata errore standard, ci fornisce una misura dell’incertezza della nostra stima. altre parole, quantifica la variabilità dei valori delle stime del parametro \\(\\mu\\) che sono calcolate sulla base di campioni diversi. Come abbiamo visto nella simulazione del TLC, l’errore standard ha la proprietà di diminuire ’aumentare della dimensione del campione.L’errore standard viene utilizzato per calcolare l’intervallo di confidenza. L’intervallo di confidenza rappresenta un intervallo di valori di un parametro o quantità di interesse che sono approssimativamente coerenti con dati, data la distribuzione campionaria presunta. ’intervallo di confidenza possiamo dunque assegnare la seguente interpretazione. Supponiamo che il modello statistico sia corretto e supponiamo di ripetere tante volte il processo di campionamento. Se per ogni campione estratto dalla popolazione calcoliamo una stima del parametro, allora gli intervalli di confidenza del 50% e del 95% includeranno il vero valore del parametro il 50% e il 95% delle volte.Sotto l’ipotesi che la distribuzione campionaria segua la distribuzione normale, per campioni di grandi dimensioni l’intervallo di confidenza al 95% si costruisce nel modo seguente:\n\\[\n\\text{stima del parametro} \\pm 2 \\text{ errori standard.}\n\\]\nDalla distribuzione normale sappiamo che una stima del parametro \\(\\pm\\) 1 errore standard corrisponde ad un intervallo del 68% e una stima del parametro \\(\\pm\\) \\(\\frac{2}{3}\\) di un errore standard corrisponde ad un intervallo del 50%. Un intervallo del 50% è particolarmente facile da interpretare dato che il vero valore del parametro ha la stessa probabilità di essere incluso o escluso dall’intervallo. Un intervallo del 95% basato sulla distribuzione normale è circa tre volte più ampio di un intervallo del 50%.","code":""},{"path":"distribuzione-campionaria-della-media-dei-campioni.html","id":"conclusioni-3","chapter":"Capitolo 14 Distribuzione campionaria della media dei campioni","heading":"Conclusioni","text":"risultati precedenti consentono le seguenti conclusioni. Se \\(X_1, \\dots, X_n\\) è un insieme di variabili aleatorie ..d., tutte con media \\(\\mu\\) e varianza \\(\\sigma^2\\), allora\n\\[\n\\mathbb{E}(\\bar{X}) = \\mu, \\quad var(\\bar{X}) = \\frac{\\sigma^2}{n}.\n\\]\nSe le \\(X_i\\) seguono la distribuzione normale, ne segue che \\(\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma/\\sqrt{n})\\), quanto qualunque combinazione lineare di variabili aleatorie Normali è ancora una variabile aleatoria Normale. Invece, se le \\(X_i\\) non seguono la distribuzione normale, il Teorema del limite centrale ci consente comunque di dire che \\(\\bar{X}\\) tende \\(\\mathcal{N}(\\mu, \\sigma/\\sqrt{n})\\) al crescere di \\(n\\). risultati precedenti sono estremamente importanti perché specificano completamente la distribuzione della media campionaria e vengono utilizzati dall’approccio frequentista per l’inferenza sulla media di una popolazione.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"stima-della-funzione-a-posteriori","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"Capitolo 15 Stima della funzione a posteriori","text":"Quando usiamo il teorema di Bayes per calcolare la distribuzione posteriori del parametro di un modello statistico, al denominatore troviamo un integrale. Tale integrale, nella maggior parte dei casi, non si può risolvere per via analitica. L’inferenza bayesiana si sviluppa dunque mediante una stima numerica della funzione posteriori. Dato che questi metodi sono “computazionalmente intensivi,” possono solo essere svolti mediante software. anni recenti metodi Bayesiani di analisi dei dati sono diventati sempre più popolari proprio perché la potenza di calcolo necessaria per svolgere tali calcoli è ora alla portata di tutti. Questo non era vero solo pochi decenni fa. Per capire come la distribuzione posteriori possa essere approssimata per via numerica esamineremo qui tre diverse tecniche che possono essere utilizzate questo scopo:metodi numerici convenzionali,metodi numerici convenzionali,il metodo dell’approssimazione quadratica,il metodo dell’approssimazione quadratica,metodi Monte Carlo basati su Catena di Markov (MCMC).metodi Monte Carlo basati su Catena di Markov (MCMC).","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:met_numerici_convenzionali","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1 Metodi numerici convenzionali","text":"È possibile stimare l’intera distribuzione posteriori mediante metodi\nnumerici convenzionali. Questo è l’approccio più semplice. Tuttavia,\nanche se tali metodi possono fornire risultati accuratissimi, causa\ndella “maledizione della dimensionalità,” tali procedure numeriche sono\nutilizzabili solo nel caso di modelli statistici semplici, con non più\ndi due parametri. Nella pratica concreta tali metodi vengono sostituiti\nda altre tecniche più efficienti quanto, anche comuni modelli\nutilizzati psicologia, vengono stimati centinaia se non migliaia di\nparametri. Nell’esempio che faremo questa sezione risulterà chiara la\nragione per cui, tali circostanze, non è possibile usare una tale\nprocedura. metodi numerici convenzionali sono invece utili come\nstrumento didattico quanto ci forniscono una procedura molto diretta\ne intuitiva che rende molto trasparente il processo dell’aggiornamento\nBayesiano. Per questa ragione esamineremo qui un esempio relativo tale\nprocedura esaminando un modello statistico che dipende da un solo\nparametro sconosciuto.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:appross_numerica","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.1 La procedura dell’approssimazione numerica","text":"È molto semplice trovare una approssimazione numerica della\ndistribuzione posteriori e ciò può essere fatto come indicato di\nseguito. Anche se la maggior parte dei parametri è continua (ovvero, \nlinea di principio ciascun parametro può assumere un numero infinito di\nvalori), possiamo ottenere un’eccellente approssimazione della\ndistribuzione posteriori considerando solo una griglia finita di\nvalori dei parametri. Per calcolare la probabilità posteriori \ncorrispondenza di ciascun particolare valore del parametro, chiamiamolo\n\\(\\theta'\\), è sufficiente moltiplicare la probabilità priori di\n\\(\\theta'\\) per il valore della funzione di verosimiglianza \ncorrispondenza di \\(\\theta'\\). Una stima della distribuzione posteriori\nsi genera ripetendo questo procedimento per ciascun valore nella\ngriglia.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:es_pratico_zetsche","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.2 Un esempio pratico","text":"Facciamo un esempio concreto consideriando nuovamente la ricerca di\nZetsche et al. (2019). Questi autori si sono chiesti se gli individui\ndepressi manifestino delle aspettative accurate circa il loro umore\nfuturo, oppure se tali aspettative siano distorte negativamente.\nConsideriamo qui 30 partecipanti dello studio di Zetsche et al. (2019)\nche hanno riportato la presenza di un episodio di depressione maggiore\natto. ’inizio della settimana di test questi pazienti è stato\nchiesto di valutare l’umore che si aspettavano di sentire nei giorni\nseguenti della settimana. Mediante una app, partecipanti dovevano poi\nvalutare il proprio umore cinque momenti diversi di ciascuno dei\ncinque giorni successivi. Lo studio considera diverse emozioni, ma qui\nci concentriamo solo sulla tristezza.Sulla base dei dati forniti dagli autori, abbiamo calcolato la media dei\ngiudizi relativi al livello di tristezza raccolti da ciascun\npartecipante tramite la app. Tale media è stata poi sottratta\ndall’aspettativa del livello di tristezza fornita ’inizio della\nsettimana. Per semplificare l’analisi abbiamo considerato la discrepanza\ntra aspettative e realtà come un evento dicotomico: valori positivi di\ntale differenza indicano che le aspettative circa il livello di\ntristezza sono maggiori del livello di tristezza che seguito viene\neffettivamente esperito; ciò significa che le aspettative sono\nnegativamente distorte (evento codificato con “1”). Si può dire il\ncontrario (le aspettative sono positivamente distorte) se tale\ndifferenza assume valori negativi (evento codificato con “0”). Nel\ncampione dei 30 partecipanti clinici qui esaminati, 23 partecipanti\nmanifestano delle aspettative negativamente distorte e 7 partecipanti\nmanifestano delle aspettative positivamente distorte. Chiamiamo \\(\\theta\\)\nla probabilità dell’evento “le aspettative del partecipante sono\ndistorte negativamente.” Ci poniamo il problema di ottenere la stima \nposteriori di \\(\\theta\\), dati 23 \"successi\" 30 prove che sono\nstati osservati.Per questo esempio considereremo 50 valori egualmente spaziati per il parametro \\(\\theta\\): 0.000, 0.0204, …, 0.978, 1.000. R abbiamo:","code":"\nn_points <- 50\np_grid <- seq(from = 0, to = 1, length.out = n_points)\np_grid\n#>  [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082 0.12244898 0.14285714\n#>  [9] 0.16326531 0.18367347 0.20408163 0.22448980 0.24489796 0.26530612 0.28571429 0.30612245\n#> [17] 0.32653061 0.34693878 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776\n#> [25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673 0.61224490 0.63265306\n#> [33] 0.65306122 0.67346939 0.69387755 0.71428571 0.73469388 0.75510204 0.77551020 0.79591837\n#> [41] 0.81632653 0.83673469 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367\n#> [49] 0.97959184 1.00000000"},{"path":"stima-della-funzione-a-posteriori.html","id":"distribuzione-a-priori","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.2.1 Distribuzione a priori","text":"Supponiamo che le nostre credenze priori sulla tendenza di un\nindividuo clinicamente depresso manifestare delle aspettative distorte\nnegativamente circa il suo umore futuro siano molto scarse. Assumiamo\nquindi per \\(\\theta\\) una distribuzione iniziale uniforme nell’intervallo\n[0, 1]. Dato che consideriamo soltanto \\(n = 50\\) valori del parametro\n\\(\\theta\\), creiamo un vettore di 50 elementi che conterrà valori della distribuzione priori scalando ciascun valore di questo vettore per \\(n\\) modo tale che la somma di tutti \nvalori della distribuzione priori (0.02, 0.02, …, 0.02, 0.02) sia\nuguale 1.0 (questo modo viene definita una funzione di massa di\nprobabilità):Stampo valori:La distribuzione priori così costruita è riportata nella figura 15.1.\nFigura 15.1: Rappresentazione della distribuzione priori per il parametro \\(\\theta\\), ovvero la probabilità di aspettative future distorte negativamente (Zetsche et al., 2019).\n","code":"\nprior1 <- dbeta(p_grid, 1, 1) / sum(dbeta(p_grid, 1, 1))\nsum(prior1)\n#> [1] 1\nprior1\n#>  [1] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n#> [18] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n#> [35] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\nlibrary(\"patchwork\")\n\np1 <- data.frame(p_grid, prior1) %>% \n  ggplot(aes(x=p_grid, xend=p_grid, y=0, yend=prior1)) +\n  geom_line()+\n  geom_segment(color = \"#8184FC\") + \n  ylim(0, 0.17) +\n  labs(\n    x = \"Parametro \\U03B8\",\n    y = \"Probabilità a priori\",\n    title = \"50 punti\"\n  )\np1"},{"path":"stima-della-funzione-a-posteriori.html","id":"funzione-di-verosimiglianza","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.2.2 Funzione di verosimiglianza","text":"Calcoliamo ora la funzione di verosimiglianza utilizzando 50 valori\n\\(\\theta\\) che abbiamo considerato. Per ciascun valore \\(\\theta\\) applichiamo la formula della probabilità binomiale tendendo sempre costanti valori dei dati (ovvero 23 “successi” 30 prove).Per esempio, per il valore \\(\\theta = 0.816\\) l’ordinata della funzione di\nverosimiglianza sarà\n\\[\\begin{aligned}\n\\binom{30}{23}& \\cdot 0.816^{23} \\cdot (1 - 0.816)^{7} = 0.135\\notag\n\\end{aligned}\n\\]\ne per \\(\\theta = 0.837\\) l’ordinata della funzione di verosimiglianza sarà\n\\[\\begin{aligned}\n\\binom{30}{23}& \\cdot 0.837^{23} \\cdot (1 - 0.837)^{7} = 0.104.\\notag\n\\end{aligned}\n\\]Svolgendo tutti calcoli con R otteniamo valori seguenti:La funzione di verosimiglianza così ottenuta è riportata nella figura 15.2.\nFigura 15.2: Rappresentazione della funzione di verosimiglianza per il parametro \\(\\theta\\), ovvero la probabilità di aspettative future distorte negativamente (Zetsche et al., 2019).\n","code":"\nlikelihood <- dbinom(23, size = 30, prob = p_grid)\nlikelihood\n#>  [1] 0.000000e+00 2.352564e-33 1.703051e-26 1.644169e-22 1.053708e-19 1.525217e-17\n#>  [7] 8.602222e-16 2.528440e-14 4.606907e-13 5.819027e-12 5.499269e-11 4.105534e-10\n#> [13] 2.520191e-09 1.311195e-08 5.919348e-08 2.362132e-07 8.456875e-07 2.749336e-06\n#> [19] 8.196948e-06 2.259614e-05 5.798673e-05 1.393165e-04 3.148623e-04 6.720574e-04\n#> [25] 1.359225e-03 2.611870e-03 4.778973e-03 8.340230e-03 1.390025e-02 2.214199e-02\n#> [31] 3.372227e-02 4.909974e-02 6.830377e-02 9.068035e-02 1.146850e-01 1.378206e-01\n#> [37] 1.568244e-01 1.681749e-01 1.688979e-01 1.575211e-01 1.348746e-01 1.043545e-01\n#> [43] 7.133007e-02 4.165680e-02 1.972669e-02 6.936821e-03 1.535082e-03 1.473375e-04\n#> [49] 1.868105e-06 0.000000e+00\np2 <- data.frame(p_grid, likelihood) %>% \n  ggplot(aes(x=p_grid, xend=p_grid, y=0, yend=likelihood)) +\n  geom_segment(color = \"#8184FC\") +\n  ylim(0, 0.17) +\n  labs(\n    x = \"Parametro \\U03B8\",\n    y = \"Verosimiglianza\"\n  )\np2"},{"path":"stima-della-funzione-a-posteriori.html","id":"la-stima-della-distribuzione-a-posteriori","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.2.3 La stima della distribuzione a posteriori","text":"La distribuzione posteriori del parametro \\(\\theta\\) è data dal prodotto\ndella verosimiglianza e della distribuzione priori, scalata per una\ncostante di normalizzazione. Quindi, facendo il prodotto dei valori\ndella distribuzione priori e valori della funzione di\nverosimiglianza otteniamo la funzione posteriori non standardizzata.\nDato che la distribuzione priori è uniforme, per ottenere questo\nrisultato è sufficiente moltiplicare ciascun valore della funzione di\nverosimiglianza per 0.02. Per esempio, per il primo valore della funzione di verosimiglianza che abbiamo calcolato sopra, avremo \\(0.135 \\cdot 0.02\\); per il secondo valore della funzione di verosimiglianza che abbiamo calcolato sopra avremo \\(0.104 \\cdot 0.02\\).Usando R, la funzione posteriori non standardizzata diventa:Avendo svolto questo prodotto per tutti 50 valori della funzione di verosimiglianza, dobbiamo poi dividere ciascuno dei 50 numeri così trovati per la costante di\nnormalizzazione. Nel caso discreto, trovare il denominatore del teorema\ndi Bayes è molto facile: esso è dato dalla somma di tutti valori della\ndistribuzione posteriori non normalizzata. Per dati presenti, tale\ncostante di normalizzazione è uguale 0.032.Possiamo dunque standardizzare due valori trovati sopra nel modo seguente:\n\\(0.135 \\cdot 0.02 / 0.032\\) e \\(0.104 \\cdot 0.02 / 0.032\\). Così facendo,\notterremo il risultato per cui la somma di tutti e 50 valori della\ndistribuzione posteriori normalizzata sarà uguale 1.0.Svolgiamo tutti calcoli R:Verifichiamo:questo particolare esempio, la distribuzione posteriori trovata come\ndescritto sopra non è altro che la versione normalizzata della funzione\ndi verosimiglianza: questo avviene perché la distribuzione priori\nuniforme non ha aggiunto altre informazioni oltre quelle che erano già\nfornite dalla funzione di verosimiglianza.La funzione posteriori che abbiamo calcolato con il metodo dell’approssimazione numerica è riportata nella figura 15.3.\nFigura 15.3: Rappresentazione della distribuzione posteriori per il parametro \\(\\theta\\), ovvero la probabilità di aspettative future distorte negativamente (Zetsche et al., 2019).\nLe funzioni rappresentate nelle figure 15.1, 15.2 e 15.3 sono state calcolate utilizzando 50 modalità equi-spaziate per il parametro \\(\\theta\\). segmenti verticali rappresentano l’intensità della funzione corrispondenza di ciascuna modalità parametro \\(\\theta\\). Nella figura 15.1 e nella figura 15.3 la somma delle lunghezze dei segmenti verticali è pari ad 1.0; ciò non si verifica, invece, nel caso della figura 15.3.","code":"\nunstd_posterior <- likelihood * prior1\nunstd_posterior\n#>  [1] 0.000000e+00 4.705127e-35 3.406102e-28 3.288337e-24 2.107415e-21 3.050433e-19\n#>  [7] 1.720444e-17 5.056880e-16 9.213813e-15 1.163805e-13 1.099854e-12 8.211068e-12\n#> [13] 5.040382e-11 2.622390e-10 1.183870e-09 4.724263e-09 1.691375e-08 5.498671e-08\n#> [19] 1.639390e-07 4.519229e-07 1.159735e-06 2.786331e-06 6.297247e-06 1.344115e-05\n#> [25] 2.718450e-05 5.223741e-05 9.557946e-05 1.668046e-04 2.780049e-04 4.428398e-04\n#> [31] 6.744454e-04 9.819948e-04 1.366075e-03 1.813607e-03 2.293700e-03 2.756411e-03\n#> [37] 3.136488e-03 3.363497e-03 3.377958e-03 3.150422e-03 2.697491e-03 2.087091e-03\n#> [43] 1.426601e-03 8.331361e-04 3.945339e-04 1.387364e-04 3.070164e-05 2.946751e-06\n#> [49] 3.736209e-08 0.000000e+00\nsum(unstd_posterior)\n#> [1] 0.0316129\nposterior <- unstd_posterior / sum(unstd_posterior)\nposterior\n#>  [1] 0.000000e+00 1.488357e-33 1.077440e-26 1.040188e-22 6.666313e-20 9.649330e-18\n#>  [7] 5.442222e-16 1.599625e-14 2.914574e-13 3.681425e-12 3.479129e-11 2.597379e-10\n#> [13] 1.594406e-09 8.295316e-09 3.744893e-08 1.494410e-07 5.350268e-07 1.739376e-06\n#> [19] 5.185824e-06 1.429552e-05 3.668548e-05 8.813904e-05 1.991986e-04 4.251792e-04\n#> [25] 8.599178e-04 1.652408e-03 3.023432e-03 5.276472e-03 8.794033e-03 1.400820e-02\n#> [31] 2.133450e-02 3.106310e-02 4.321259e-02 5.736920e-02 7.255582e-02 8.719259e-02\n#> [37] 9.921545e-02 1.063963e-01 1.068538e-01 9.965619e-02 8.532881e-02 6.602021e-02\n#> [43] 4.512719e-02 2.635430e-02 1.248015e-02 4.388601e-03 9.711744e-04 9.321354e-05\n#> [49] 1.181862e-06 0.000000e+00\nsum(posterior)\n#> [1] 1\np3 <- data.frame(p_grid, posterior) %>% \n  ggplot(aes(x=p_grid, xend=p_grid, y=0, yend=posterior)) +\n  geom_segment(color = \"#8184FC\") +\n  ylim(0, 0.17) +\n  labs(\n    x = \"Parametro \\U03B8\",\n    y = \"Probabilità a posteriori\"\n  )\np3"},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:aspett_future_beta2_10","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.3 Un esempio pratico (versione 2)","text":"Continuiamo la discussione dell’esempio precedente supponendo che la\nletteratura precedente ci fornisca delle informazioni proposito di\n\\(\\theta\\), ovvero sulla probabilità che le aspettative future di un\nindividuo clinicamente depresso siano distorte negativamente. tali\ncircostanze, invece di utilizzare la distribuzione uniforme per\n\\(p(\\theta)\\), definiamo la distribuzione priori per \\(\\theta\\) come una\ndistribuzione che ha la forma di una Beta di parametri \\(\\alpha = 2\\) e\n\\(\\beta = 10\\). questo modo, la distribuzione priori di \\(\\theta\\)\nritiene molto plausibili valori bassi di \\(\\theta\\), mentre valori\n\\(\\theta\\) superiori 0.5 vengono considerati impossibili. Questo è\nequivalente dire che ci aspettiamo che le aspettative relative\n’umore futuro siano distorte negativamente solo per pochissimi\nindividui clinicamente depressi – altre parole, ci aspettiamo che la\nmaggioranza degli individui clinicamente depressi sia inguaribilmente\nottimista. Questa è, ovviamente, un’opinione priori molto difficile da\ngiustificare. La esamino qui, non perché abbia senso nel contesto dei\ndati di Zetsche et al. (2019), ma soltanto per fare un esempio che mostra\ncome la distribuzione posteriori fornisca una sorta di “compromesso”\ntra la distribuzione priori e la verosimiglianza.Con calcoli del tutto simili quelli descritti sopra si giunge alla distribuzione posteriori rappresentata nella figura 15.4. Iniziamo definire una griglia unidimensionale equispaziata di possibili valori del parametro \\(\\theta\\). Anche questo caso usiamo 50 valori possibili del parametro \\(\\theta\\):Per la distribuzione priori scelgo una Beta(2, 10).Tale distribuzione priori è rappresentata nella figura 15.4.\nFigura 15.4: Rappresentazione di una funzione priori informativa per il parametro \\(\\theta\\).\nCalcoliamo il valore della funzione di verosimiglianza corrispondenza di ciascun punto della griglia. La funzione di verosimiglianza è identica quella considerata nell’esempio precedente.Calcolo il prodotto tra la verosimiglianza e la distribuzione priori, per ciascun punto della griglia:Normalizzo la distribuzione posteriori modo tale che la somma sia 1.La nuova funzione posteriori è rappresentata nella figura 15.5.\nFigura 15.5: Rappresentazione della funzione posteriori per il parametro \\(\\theta\\) calcolata utilizzando una distribuzione priori informativa.\nFacendo un confronto tra le figure 15.4 e 15.5 si nota come la distribuzione priori per il parametro \\(\\theta\\) e la distribuzione posteriori per il parametro \\(\\theta\\) sono molto diverse. particolare, si noti che la distribuzione posteriori rappresentata nella 15.5 risulta spostata verso destra su\nposizioni più vicine quelle della verosimiglianza, rappresentata nella figura 15.2. Si noti anche, causa dell’effetto della distribuzione priori, le distribuzioni posteriori riportate nelle figure 15.3 e 15.5 sono molto diverse tra loro. Discuteremo seguito l’influenza della distribuzione priori sull’inferenza finale.","code":"\nn_points <- 50\np_grid <- seq(from = 0, to = 1, length.out = n_points)\nalpha <- 2\nbeta <- 10\nprior2 <- dbeta(p_grid, alpha, beta) / sum(dbeta(p_grid, alpha, beta))\nsum(prior2)\n#> [1] 1\nplot_df <- data.frame(p_grid, prior2) \np4 <- plot_df %>% \n  ggplot(aes(x=p_grid, xend=p_grid, y=0, yend=prior2)) +\n  geom_segment(color = \"#8184FC\") +\n  ylim(0, 0.17) +\n  labs(\n    x = \"\",\n    y = \"Probabilità a priori\",\n    title = \"50 punti\"\n  )\np4\nlikelihood <- dbinom(23, size = 30, prob = p_grid)\nunstd_posterior2 <- likelihood * prior2\nposterior2 <- unstd_posterior2 / sum(unstd_posterior2)\nsum(posterior2)\n#> [1] 1\nplot_df <- data.frame(p_grid, posterior2)\np5 <- plot_df %>%\n  ggplot(aes(x = p_grid, xend = p_grid, y = 0, yend = posterior2)) +\n  geom_segment(color = \"#8184FC\") +\n  ylim(0, 0.17) +\n  labs(\n    x = \"Parametro \\U03B8\",\n    y = \"Probabilità a posteriori\"\n  )\np5"},{"path":"stima-della-funzione-a-posteriori.html","id":"sommario-della-funzione-a-posteriori","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.1.4 Sommario della funzione a posteriori","text":"Una volta calcolata la distribuzione posteriori dobbiamo riassumerla qualche modo. Nel caso cui venga usato il metodo di approssimazione numerica, il problema del calcolo delle aree sottese alla funzione posteriori qualunque intervallo può essere risolto vari modi. Tuttavia, questo problema trova una soluzione molto più semplice se viene utilizzato un metodo diverso per la stima della distribuzione posteriori, come vedremo di seguito. Non discuteremo dunque la possibile soluzione di questo problema nel caso presente, quanto il metodo dell’approssimazione numerica per il calcolo della distribuzione posteriori è solo un esempio didattico.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"approssimazione-quadratica","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.2 Approssimazione quadratica","text":"metodi numerici convenzionali possono essere usati solo quando il numero di parametri da stimare è piccolo. La ragione di ciò sta nella cosiddetta “maledizione della dimensionalità.” Vediamo cosa significa. Nel caso di un solo parametro, supponiamo di utilizzare una griglia di\n100 valori. Per due parametri avremo bisogno di \\(100^2\\) valori. Ma già per 10 parametri avremo bisogno di \\(10^{10}\\) valori – è facile capire che una tale quantità di valori è troppo grande anche per un computer potente come quello che utilizziamo normalmente. Dobbiamo dunque affrontare il problema un altro modo.Una possibile soluzione al nostro problema ci viene fornita dal metodo dell’approssimazione quadratica. La motivazione di tale metodo è la seguente. Sappiamo che, generale, la regione della distribuzione posteriori che si trova prossimità del suo massimo può essere ben approssimata dalla forma di una distribuzione Normale. Descrivere la distribuzione posteriori mediante la distribuzione Normale significa utilizzare un’approssimazione che viene, appunto, chiamata “quadratica” (tale approssimazione si dice quadratica perché il logaritmo di una distribuzione gaussiana forma una parabola e la parabola è una funzione quadratica – dunque, mediante questa approssimazione descriviamo il logaritmo della distribuzione posteriori mediante una parabola).L’approssimazione quadratica si pone due obiettivi.Trovare la moda della distribuzione posteriori. Ci sono varie\nprocedure di ottimizzazione, implementate R, \ngrado di trovare il massimo di una distribuzione.Trovare la moda della distribuzione posteriori. Ci sono varie\nprocedure di ottimizzazione, implementate R, \ngrado di trovare il massimo di una distribuzione.Stimare la curvatura della distribuzione prossimità della moda.\nUna stima della curvatura è sufficiente per trovare\nun’approssimazione quadratica dell’intera distribuzione. alcuni\ncasi, questi calcoli possono essere fatti seguendo una procedura\nanalitica, ma solitamente vengono usate delle tecniche numeriche.Stimare la curvatura della distribuzione prossimità della moda.\nUna stima della curvatura è sufficiente per trovare\nun’approssimazione quadratica dell’intera distribuzione. alcuni\ncasi, questi calcoli possono essere fatti seguendo una procedura\nanalitica, ma solitamente vengono usate delle tecniche numeriche.Una descrizione della distribuzione posteriori ottenuta mediante l’approssimazione quadratica si ottiene mediante la funzione quap() contenuta nel pacchetto rethinking. Tale pacchetto, creato da Richard McElreath per accompagnare il suo testo Statistical Rethinking\\(^2\\), può essere scaricato utilizzando le istruzioni seguenti:È possibile che, per diversi sistemi operativi, sia necessaria l’installazione di componenti ulteriori. Si veda https://github.com/rmcelreath/rethinking.Le analisi Bayesiane che discuteremo queste dispense, per la maggior parte, faranno uso della funzione rethinking::quap(). È dunque fondamentale che gli studenti installino sul loro computer il pacchetto rethinking.Dal nostro punto di vista non è importante capire come si svolgono pratica calcoli necessari per la stima della distribuzione posteriori con il metodo dell’approssimazione quadratica. Quello che è importante capire è il significato della distribuzione posteriori e questo significato è stato chiarito nella sezione La procedura dell’approssimazione numerica. L’approssimazione quadratica fornisce risultati simili (o identici) quelli ottenuti con il metodo descritto nella sezione La procedura dell’approssimazione numerica. Il vantaggio dell’approssimazione quadratica è che disponiamo di una serie di funzioni R che svolgono tutti calcoli per noi.realtà, l’approssimazione quadratica è poco usata pratica, perché per problemi complessi è più conveniente usare metodi Monte Carlo basati su Catena di Markov (MCMC) che verranno descritti nella successiva sezione Integrazione con metodo Monte Carlo. Per potere utilizzare metodi MCMC è necessario installare sul proprio computer del software aggiuntivo e tale operazione, talvolta, può risultare complessa. Non è l’obiettivo di questo insegnamento affrontare questo problema Per questa ragione, per svolgere gli esercizi che discuteremo sarà sufficiente fare ricorso al metodo dell’approssimazione quadratica; ovvero sarà sufficiente usare la funzione rethinking::quap().","code":"\ninstall.packages(c(\"coda\", \"mvtnorm\", \"devtools\", \"loo\", \"dagitty\"))\nlibrary(\"devtools\")\ndevtools::install_github(\"rmcelreath/rethinking\")"},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:integr_MC","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.3 Integrazione con metodo Monte Carlo","text":"Prima di introdurre metodi MCMC per la stima della funzione posteriori, spendiamo due parole sul metodo Monte Carlo quale tecnica che consente il calcolo degli integrali mediante simulazione numerica. Il termine Monte-Carlo si riferisce al fatto che per la computazione si ricorre ad un ripetuto campionamento casuale attraverso la generazioni di sequenze di numeri casuali.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:legge_forte_grandi_numeri","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.3.1 Legge forte dei grandi numeri","text":"L’integrazione con metodo Monte Carlo trova la sua giustificazione nella Legge forte dei grandi numeri la quale può essere espressa nei termini seguenti. Data una successione di variabili casuali \\(Y_{1}, Y_{2},\\dots, Y_{n},\\dots\\) indipendenti e identicamente distribuite con media \\(\\mu\\), ne segue che\n\\[\nP\\left( \\lim_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{=1}^n Y_i = \\mu \\right) = 1.\n\\]\nCiò significa che, al crescere di \\(n\\), la media delle realizzazioni di \\(Y_{1}, Y_{2},\\dots, Y_{n},\\dots\\) converge con probabilità 1 al vero valore \\(\\mu\\).Un esempio della legge forte dei grandi numeri riguarda una serie di lanci di una moneta dove \\(Y=1\\) significa “testa” e \\(Y=0\\) significa “croce.” Per la legge forte dei grandi numeri, nel caso di una moneta equilibrata la proporzione di eventi “testa” converge alla vera probabilità dell’evento “testa”\n\\[\n\\frac{1}{n} \\sum_{=1}^n Y_i \\rightarrow \\frac{1}{2}\n\\]\ncon probabilità di uno.Quello che è stato detto sopra non è che un modo sofisticato per dire che, se vogliamo calcolare un’approssimazione del valore atteso di una variabile aleatoria, non dobbiamo fare altro che la media aritmetica di un grande numero di realizzazione di tale variabile aleatoria. Come è facile intuire, l’approssimazione migliora al crescere del numero di dati che abbiamo disposizione.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:mmmc","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4 Metodi MC basati su Catena di Markov","text":"metodi Monte Carlo basati su Catena di Markov consentono di costruire sequenze di punti (le “catene”) nello spazio dei parametri, la cui densità è proporzionale alla distribuzione di probabilità posteriori cui siamo interessati. Questo, evidentemente, è il risultato vorremmo\nottenere. Ma cosa sono le catene di Markov? termini formali possiamo dire che una catena di Markov è una sequenza di variabili aleatorie \\(Y_{1}, Y_{2},\\dots, Y_{n}\\) tale che la dipendenza della distribuzione di \\(Y_{+1}\\) dai valori di \\(Y_{1}, \\dots, Y_{}\\) è interamente dovuta al\nvalore di \\(Y_i\\), cioè il passaggio ad uno stato del sistema dipende unicamente dallo stato immediatamente precedente e non dal come si è giunti tale stato (dalla storia). Per questo motivo si dice che un processo markoviano è senza memoria.generale è possibile generare catene di Markov che convergono ad una soluzione unica e stazionaria tale per cui gli elementi della catena sono campioni dalla distribuzione di interesse. Nel caso dell’inferenza Bayesiana la distribuzione di interesse è la distribuzione posteriori, \\(p(\\theta \\mid x)\\). Le catene di Markov possono quindi essere utilizzate per stimare valori di aspettazione di variabili rispetto alla distribuzione posteriori. altre parole, possiamo utilizzare le catene di Markov per stimare valori posteriori dei parametri sconosciuti di un modello statistico – un esempio è il parametro \\(p\\) nel problema del mappamondo che abbiamo discusso precedenza.La generazioni di elementi di una catena ha una natura probabilistica e esistono diversi algoritmi per costruire catene di Markov. Due aspetti da tenere considerazione sotto questo punto di vista sono il periodo di burn-e le correlazioni tra punti. Al crescere degli step della catena si ottiene una migliore approssimazione della distribuzione target. ’inizio del campionamento però la distribuzione può essere significativamente lontana dalla distribuzione stazionaria. Ci vuole un certo tempo prima di raggiungere la distribuzione stazionaria di equilibrio e tale periodo è detto di burn-. Perciò campioni provenienti da tale parte iniziale della catena vanno tipicamente scartati poiché non rappresentano accuratamente la distribuzione desiderata.Normalmente, un algoritmo MCMC genera catene di Markov di campioni, ognuno dei quali è autocorrelato quelli generati immediatamente prima e dopo di lui. Conseguentemente campioni successivi non sono indipendenti ma formano una catena di Markov con un certo grado di\ncorrelazione. Questa correlazione introduce una distorsione nella soluzione che si ottiene con questo metodo. L’arte dei diversi algoritmi MCMC risiede nel rendere il meccanismo efficiente e capace di produrre un risultato non distorto, il che implica la riduzione al minimo del tempo di burn-e della correlazione tra diversi campioni.Presentiamo ora, una forma intuitiva, l’algoritmo di Metropolis, ovvero il primo algoritmo MCMC che è stato proposto. Tale algoritmo è stato sviluppato seguito per renderlo via via più efficiente. Il nostro obiettivo, però, è solo quello di capire la logica sottostante – lasciamo che siano gli ingegneri risolvere il problema di rendere l’algoritmo più efficiente.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:turista_viagg","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.1 Il problema del turista viaggiatore","text":"L’algoritmo di Metropolis è stato presentato usando varie metafore: quella di un politico che viaggia tra isole diverse (Kruschke, 2014), o quella di un re che, anche lui, si sposta tra le isole di un arcipelago (McElreath, 2020). Qui mutiamo leggermente la metafora e immaginiamo un turista vacanza su un’isola che dispone di 10 spiagge di grandezza diversa. Muovendosi senso orario, la grandezza delle spiagge aumenta: partendo dalla spiaggia più piccola si arriva ad una spiaggia un po’ più grande, via via fino ad arrivare ’ultima spiaggia, la decima, che è la più grande di tutte. Quindi indicheremo con numeri da 1 10 le spiagge dell’isola. Tali numeri rappresentano anche la grandezza (relativa) di ciascuna spiaggia. Dato che l’isola è circolare, la decima spiaggia confina con la prima spiaggia.Nella nostra metafora, immaginiamo un turista vacanza sull’isola che abbiamo appena descritto. Per non annoiarsi, il nostro turista vuole passare un po’ di tempo su ogni spiaggia, ma con il vincolo che il tempo passato su ciascuna spiaggia deve essere proporzionale alla grandezza della spiaggia. Infatti, il turista preferisce le spiagge più grandi; nel contempo, però, vuole anche visitare spiagge diverse, quindi il vincolo descritto sopra sembra un buon compromesso tra il desiderio di cambiare spiaggia di tanto tanto e il desiderio di passare più tempo sulle spiagge più grandi.Essendo vacanza, il turista non vuole preparare un calendario che stabilisca anticipo la spiaggia da visitare ogni giorno, ma vuole decidere maniera rilassata e un po’ casuale, ogni mattina, restando però fedele al vincolo che si è dato. Al bar incontra un altro turista, l’ingegnere Metropolis, che gli suggerisce come fare per ottenere l’obiettivo che si è prefissato. Seguendo le istruzioni di Metropolis, il nostro turista decide di comportarsi nel modo seguente.Ogni mattina decide tra due alternative: ritornare sulla\nspiaggia dove era stato il giorno prima (chiamiamola spiaggia\ncorrente) oppure andare una delle due spiagge contigue.Ogni mattina decide tra due alternative: ritornare sulla\nspiaggia dove era stato il giorno prima (chiamiamola spiaggia\ncorrente) oppure andare una delle due spiagge contigue.Lancia una moneta. Se esce testa, considera la possibilità di andare\nnella spiaggia che confina con la spiaggia corrente muovendosi \nsenso orario; se esce croce, considera la possibilità di andare\nnella spiaggia che confina con la spiaggia corrente muovendosi \nsenso antiorario. La spiaggia individuata questo modo viene\nchiamata spiaggia proposta.Lancia una moneta. Se esce testa, considera la possibilità di andare\nnella spiaggia che confina con la spiaggia corrente muovendosi \nsenso orario; se esce croce, considera la possibilità di andare\nnella spiaggia che confina con la spiaggia corrente muovendosi \nsenso antiorario. La spiaggia individuata questo modo viene\nchiamata spiaggia proposta.Dopo avere trovato la spiaggia proposta, il turista deve decidere se\neffettivamente andare lì oppure e, per decidere,\nprocede questo modo. Prende un numero di conchiglie proporzionale\nalla grandezza della spiaggia proposta – per esempio, se la\nspiaggia proposta è la numero 7, allora prenderà 7 conchiglie.\nPrende un numero di sassolini proporzionale alla grandezza della\nspiaggia corrente – per esempio, se la spiaggia corrente è la\nnumero 6, allora prenderà 6 sassolini.Dopo avere trovato la spiaggia proposta, il turista deve decidere se\neffettivamente andare lì oppure e, per decidere,\nprocede questo modo. Prende un numero di conchiglie proporzionale\nalla grandezza della spiaggia proposta – per esempio, se la\nspiaggia proposta è la numero 7, allora prenderà 7 conchiglie.\nPrende un numero di sassolini proporzionale alla grandezza della\nspiaggia corrente – per esempio, se la spiaggia corrente è la\nnumero 6, allora prenderà 6 sassolini.Se il numero di conchiglie è maggiore del numero di sassolini, il\nturista si sposta sempre nella spiaggia proposta. Ma se ci sono meno\nconchiglie che sassolini, scarta un numero di sassolini uguale al\nnumero di conchiglie e mette gli oggetti rimanenti un sacchetto\n– per esempio, se la spiaggia proposta è la 5 e la spiaggia\ncorrente è la 6, allora metterà nel sacchetto 5 conchiglie e 1\nsassolino. Mescola bene ed estrae dal sacchetto un oggetto: se è una\nconchiglia si sposta nella spiaggia proposta, se è un sassolino\nresta nella spiaggia corrente. Di conseguenza, la probabilità che il\nturista cambi spiaggia (ovvero \\(\\frac{5}{6}\\)) è uguale al numero di\nconchiglie diviso per il numero originale di sassolini.Se il numero di conchiglie è maggiore del numero di sassolini, il\nturista si sposta sempre nella spiaggia proposta. Ma se ci sono meno\nconchiglie che sassolini, scarta un numero di sassolini uguale al\nnumero di conchiglie e mette gli oggetti rimanenti un sacchetto\n– per esempio, se la spiaggia proposta è la 5 e la spiaggia\ncorrente è la 6, allora metterà nel sacchetto 5 conchiglie e 1\nsassolino. Mescola bene ed estrae dal sacchetto un oggetto: se è una\nconchiglia si sposta nella spiaggia proposta, se è un sassolino\nresta nella spiaggia corrente. Di conseguenza, la probabilità che il\nturista cambi spiaggia (ovvero \\(\\frac{5}{6}\\)) è uguale al numero di\nconchiglie diviso per il numero originale di sassolini.Decidere di procedere questo modo potrebbe sembrare un modo per rovinarsi le vacanze. Invece, questo algoritmo funziona! Seguendo la proposta di Metropolis, il turista passerà su ciascuna\nspiaggia un numero di giorni proporzionale alla grandezza della spiaggia.McElreath (2020) ha implementato R l’algoritmo di Metropolis che abbiamo descritto sopra nel modo seguente:Le istruzioni seguenti sono state usate per generare la figura 15.6. Se guardiamo la figura e consideriamo un giorno qualsiasi è difficile capire qual è la spiaggia scelta dal turista.\nFigura 15.6: Risultati dell’algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra la spiaggia scelta dal turista (asse verticale) funzione di ciascun giorno della sua vacanza (asse orizzontale).\nTuttavia, se esaminiamo la figura 15.7 che descrive il comportamento lungo termine dell’algoritmo, ci rendiamo conto che l’algoritmo ha prodotto il risultato che si voleva ottenere: il tempo trascorso dal turista su ciascuna spiaggia è proporzionale alla grandezza della spiaggia.\nFigura 15.7: Risultati dell’algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra che il numero di volte cui ciascuna spiaggia è stata visitata è proporzionale alla grandezza della spiaggia.\nL’algoritmo di Metropolis funziona anche se il turista decide di spostarsi dalla spiaggia corrente qualunque altra spiaggia, non solo su quelle confinanti. Inoltre, l’algoritmo funziona per qualunque numero di spiagge e anche se il turista non sa quante spiagge ci sono sull’isola. Affinché l’algoritmo funzioni è solo necessario conoscere la grandezza della spiaggia “corrente” e quella della spiaggia “proposta.”","code":"\nnum_weeks <- 1e5\npositions <- rep(0, num_weeks)\ncurrent <- 10\nfor (i in 1:num_weeks) {\n  # record current position\n  positions[i] <- current\n  # flip coin to generate proposal\n  proposal <- current + sample(c(-1, 1), size = 1)\n  # now make sure he loops around the archipelago\n  if (proposal < 1) proposal <- 10\n  if (proposal > 10) proposal <- 1\n  # move?\n  prob_move <- proposal / current\n  current <- ifelse(runif(1) < prob_move, proposal, current)\n}\nggplot(\n    data.frame(x = 1:100, y = positions[1:100]),\n    aes(x, y)\n) +\n  geom_point(color = \"#8184FC\") +\n  labs(\n    x = \"Giorno\",\n    y = \"Isola\"\n  ) +\n  scale_y_continuous(breaks=1:10)\nggplot(\n  data.frame(x = 1:10, y = as.numeric(table(positions))),\n  aes(x = x, xend = x, y = 0, yend = y)\n  ) +\n  geom_segment(color = \"#8184FC\", size = 1.5) +\n  labs(\n    x = \"Isola\",\n    y = \"Numero di giorni\"\n  ) +\n  scale_x_continuous(breaks=1:10)"},{"path":"stima-della-funzione-a-posteriori.html","id":"lalgoritmo-di-metropolis","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.2 L’algoritmo di Metropolis","text":"L’algoritmo descritto nella sezione Il problema del turista viaggiatore è un caso speciale dell’algoritmo di Metropolis e l’algoritmo di Metropolis è un caso speciale dei\nmetodi MCMC. L’algoritmo di Metropolis, al di là dell’uso che ne fa il fortunato turista dell’esempio discusso precedenza, viene realtà impiegato per per ottenere una sequenza di campioni casuali da una distribuzione posteriori la cui forma è, solitamente, sconosciuta. Fuor di metafora:numeri che identificano ciascuna spiaggia corrispondono ai valori del parametro che vogliamo stimare – non è necessario che il parametro assuma solo valori discreti, può anche assumere un insieme continuo di valori;numeri che identificano ciascuna spiaggia corrispondono ai valori del parametro che vogliamo stimare – non è necessario che il parametro assuma solo valori discreti, può anche assumere un insieme continuo di valori;la grandezza della spiaggia corrisponde alla densità posteriori associata ciascuno dei possibili valori del parametro;la grandezza della spiaggia corrisponde alla densità posteriori associata ciascuno dei possibili valori del parametro;giorni di permanenza su una spiaggia corrispondono al numero di campioni estratti dalla distribuzione posteriori.giorni di permanenza su una spiaggia corrispondono al numero di campioni estratti dalla distribuzione posteriori.L’aspetto cruciale di questa discussione è il fatto che, ’aumentare delle ripetizioni dell’algoritmo di Metropolis, la distribuzione dei valori così ottenuti diventa via via più simile alla distribuzione posteriori del parametro \\(\\theta\\), anche se questa è sconosciuta. Per un grande numero di passi della catena l’approssimazione è sufficiente. Con questo metodo è dunque possibile generare un grande numero di campioni casuali dalla distribuzione posteriori per poi poterne calcolare misure di sintesi e potere fare inferenza.","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:metropolis_one_mean","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.3 Una applicazione concreta","text":"L’algoritmo di Metropolis consente di effettuare quello che viene chiamato un dependent sampling, ovvero ci consente di generare campioni casuali dalla distribuzione posteriori utilizzando soltanto il numeratore del teorema di Bayes:\\[\nP(\\theta \\mid x) = \\frac{P(x \\mid \\theta)P(\\theta)}{P(x)}\n\\]\novvero\\[\nP(\\theta \\mid x) \\propto P(x \\mid \\theta)P(\\theta)\n\\]\nL’algoritmo di Metropolis è la versione più semplice e più conosciuta degli algoritmi MCMC. Vediamo come funziona.Per prima cosa troviamo un valore casuale del parametro estraendolo da una distribuzione “proposta”: \\(\\theta_0 \\sim \\Pi(\\theta)\\). La distribuzione proposta può essere qualunque distribuzione, anche se, idealmente, è meglio che sia simile alla distribuzione posteriori. Ma pratica la distribuzione posteriori è sconosciuta e quindi utilizziamo un qualche metodo arbitrario di iniziare la catena di Markov (ovvero utilizziamo un valore iniziale arbitrario).Per prima cosa troviamo un valore casuale del parametro estraendolo da una distribuzione “proposta”: \\(\\theta_0 \\sim \\Pi(\\theta)\\). La distribuzione proposta può essere qualunque distribuzione, anche se, idealmente, è meglio che sia simile alla distribuzione posteriori. Ma pratica la distribuzione posteriori è sconosciuta e quindi utilizziamo un qualche metodo arbitrario di iniziare la catena di Markov (ovvero utilizziamo un valore iniziale arbitrario).ciascuna iterazione \\(t\\) viene proposto un nuovo valore del parametro, \\(\\theta'_t\\). Il valore \\(\\theta'_t\\) viene estratto maniera casuale da una qualsiasi distribuzione simmetrica centrata sul valore del parametro dell’interazione precedente, \\(t-1\\). Ad esempio, possiamo usare la distribuzione Normale con una appropriata deviazione standard: \\(\\theta_t \\sim \\mathcal{N}(\\theta_{t-1}, \\sigma)\\). pratica, questo significa che il valore proposto del parametro sarà un valore nella prossimità di quello attualmente considerato.ciascuna iterazione \\(t\\) viene proposto un nuovo valore del parametro, \\(\\theta'_t\\). Il valore \\(\\theta'_t\\) viene estratto maniera casuale da una qualsiasi distribuzione simmetrica centrata sul valore del parametro dell’interazione precedente, \\(t-1\\). Ad esempio, possiamo usare la distribuzione Normale con una appropriata deviazione standard: \\(\\theta_t \\sim \\mathcal{N}(\\theta_{t-1}, \\sigma)\\). pratica, questo significa che il valore proposto del parametro sarà un valore nella prossimità di quello attualmente considerato.Calcoliamo poi il rapporto \\(r\\) tra la distribuzione posteriori non normalizzata determinata dal valore proposto \\(\\theta'_t\\) e la distribuzione posteriori non normalizzata determinata dal valore del parametro \\(\\theta'_{t-1}\\) dell’iterazione precedente della catena: \\(r = \\frac{P(x \\mid \\theta'_t) P(\\theta'_t)}{P(x \\mid \\theta'_{t-1}) P(\\theta'_{t-1})}\\). Soffermiamoci su tale formula per capire bene cosa significa. La distribuzione posteriori non normalizzata corrisponde al numeratore del teorema di Bayes, ovvero \\(P(x \\mid \\theta) P(\\theta)\\), laddove \\(P(x \\mid \\theta)\\) è la verosimiglianza di \\(x\\) dato \\(\\theta\\) e \\(P(\\theta)\\) è la distribuzione priori di \\(\\theta\\). Abbiamo visto nella sezione Un esempio pratico (versione 2) che ciascuna di tali densità può essere rappresentata mediante una curva e che il prodotto di due densità si ottiene facendo il prodotto dei valori delle ordinate corrispondenti di discuna delle due curve. Il numeratore del teorema di Bayes ci fornisce la distribuzione posteriori non normalizzata quanto l’area sottesa alla curva così ottenuta non è unitaria (quindi tale curva non rappresenta una funzione di densità). Dato che qui facciamo un rapporto, però, questo è irrilevante. Al numeratore del rapporto \\(r\\) dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza corrispondenza del valore proposto \\(x = \\theta'_t\\) e la densità della distribuzione priori corrispondenza del valore proposto \\(x = \\theta'_t\\). maniera corrispondente, al denominatore del rapporto \\(r\\) dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza corrispondenza del valore \\(\\theta_{t-1}\\) e la densità della distribuzione priori corrispondenza del valore \\(\\theta_{t-1}\\).Calcoliamo poi il rapporto \\(r\\) tra la distribuzione posteriori non normalizzata determinata dal valore proposto \\(\\theta'_t\\) e la distribuzione posteriori non normalizzata determinata dal valore del parametro \\(\\theta'_{t-1}\\) dell’iterazione precedente della catena: \\(r = \\frac{P(x \\mid \\theta'_t) P(\\theta'_t)}{P(x \\mid \\theta'_{t-1}) P(\\theta'_{t-1})}\\). Soffermiamoci su tale formula per capire bene cosa significa. La distribuzione posteriori non normalizzata corrisponde al numeratore del teorema di Bayes, ovvero \\(P(x \\mid \\theta) P(\\theta)\\), laddove \\(P(x \\mid \\theta)\\) è la verosimiglianza di \\(x\\) dato \\(\\theta\\) e \\(P(\\theta)\\) è la distribuzione priori di \\(\\theta\\). Abbiamo visto nella sezione Un esempio pratico (versione 2) che ciascuna di tali densità può essere rappresentata mediante una curva e che il prodotto di due densità si ottiene facendo il prodotto dei valori delle ordinate corrispondenti di discuna delle due curve. Il numeratore del teorema di Bayes ci fornisce la distribuzione posteriori non normalizzata quanto l’area sottesa alla curva così ottenuta non è unitaria (quindi tale curva non rappresenta una funzione di densità). Dato che qui facciamo un rapporto, però, questo è irrilevante. Al numeratore del rapporto \\(r\\) dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza corrispondenza del valore proposto \\(x = \\theta'_t\\) e la densità della distribuzione priori corrispondenza del valore proposto \\(x = \\theta'_t\\). maniera corrispondente, al denominatore del rapporto \\(r\\) dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza corrispondenza del valore \\(\\theta_{t-1}\\) e la densità della distribuzione priori corrispondenza del valore \\(\\theta_{t-1}\\).Utilizziamo poi il valore del rapporto \\(r\\) per decidere se dobbiamo effettivamente muoverci nella nuova posizione \\(\\theta'_t\\), oppure se dobbiamo campionare un diverso valore \\(\\theta'_t\\). Per decidere, confrontiamo il valore \\(r\\) con un valore casuale estratto da una distribuzione uniforme che assume valori tra zero e uno: \\(U(0, 1)\\). Se \\(r > u \\sim U(0, 1)\\) allora accettiamo \\(\\theta'_t\\) e la catena si muove quella nuova posizione, ovvero \\(\\theta_t = \\theta'_t\\). Altrimenti \\(\\theta_t = \\theta_{t-1}\\) e ripetiamo la procedura descritta sopra campionando un nuovo valore \\(\\theta'_t\\).Utilizziamo poi il valore del rapporto \\(r\\) per decidere se dobbiamo effettivamente muoverci nella nuova posizione \\(\\theta'_t\\), oppure se dobbiamo campionare un diverso valore \\(\\theta'_t\\). Per decidere, confrontiamo il valore \\(r\\) con un valore casuale estratto da una distribuzione uniforme che assume valori tra zero e uno: \\(U(0, 1)\\). Se \\(r > u \\sim U(0, 1)\\) allora accettiamo \\(\\theta'_t\\) e la catena si muove quella nuova posizione, ovvero \\(\\theta_t = \\theta'_t\\). Altrimenti \\(\\theta_t = \\theta_{t-1}\\) e ripetiamo la procedura descritta sopra campionando un nuovo valore \\(\\theta'_t\\).Per fare un esempio concreto, consideriamo nuovamente 30 pazienti esaminati da Zetsche et al. (2019) e discussi nella sezione Un esempio pratico. Di essi, 23 hanno manifestato delle aspettative distorte negativamente sul loro stato d’animo futuro. Utilizzando l’algoritmo di Metropolis, ci poniamo il problema di ottenere la stima posteriori di \\(\\theta\\) (probabilità di manifestare un’aspettativa distorta negativamente) dati 23 “successi” 30 prove e usando la stessa distribuzione priori per \\(\\theta\\) che è stata usata nella sezione Un esempio pratico (versione 2).","code":""},{"path":"stima-della-funzione-a-posteriori.html","id":"verosimiglianza","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.3.1 Verosimiglianza","text":"Per trovare la funzione di verosimiglianza usando 30 valori di Zetsche et al. (2019) definisco la funzione likelihood() come indicato sotto. Tale funzione ritorna l’ordinata della funzione di verosimiglianza binomiale per ciascun valore del vettore param che viene dato input alla funzione.","code":"\nx <- 23\nN <- 30\nparam <- seq(0, 1, length.out = 100)\n\nlikelihood <- function(param, x = 23, N = 30) {\n  dbinom(x, N, param)\n}\n\ndata.frame(x=param, y=likelihood(param)) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  labs(\n    x = expression(theta),\n    y = \"Verosimiglianza\"\n  )"},{"path":"stima-della-funzione-a-posteriori.html","id":"distribuzione-a-priori-1","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.3.2 Distribuzione a priori","text":"Se abbiamo ragioni forti per avere delle aspettative rispetto al valore possibile della nostra stima, una distribuzione priori informativa verrà combinata con le informazioni fornite dal campione per produrre una stima ``razionale’’ posteriori. Nel caso presente utilizziamo la distribuzione informativa presentata nella sezione Un esempio pratico (versione 2) unicamente scopo esemplicativo, ovvero per fare modo da “allontanare” la distribuzione posteriori dalla distribuzione di verosimiglianza.","code":"\nprior <- function(param, alpha = 2, beta = 10) {\n  param_vals <- seq(0, 1, length.out = 100)\n  dbeta(param, alpha, beta) # / sum(dbeta(param_vals, alpha, beta))\n}\n\ndata.frame(x=param, y=prior(param)) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  labs(\n    x = expression(theta),\n    y = \"Densità\"\n  )"},{"path":"stima-della-funzione-a-posteriori.html","id":"distribuzione-a-posteriori","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.3.3 Distribuzione a posteriori","text":"Abbiamo visto precedenza come la funzione posteriori è data dal prodotto della densità priori e della verosimiglianza.Questo è il risultato che vogliamo ottenere utilizzando l’algoritmo di Metropolis. Dalla figura precedente vediamo che la moda della distribuzione posteriori è pari circa 0.6. Questo è il valore più verosimile posteriori per il parametro \\(\\theta\\).","code":"\nposterior <- function(param) {\n  likelihood(param) * prior(param)\n}\n\ndata.frame(x=param, y=posterior(param)) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  labs(\n    x = expression(theta),\n    y = \"Densità\"\n  )"},{"path":"stima-della-funzione-a-posteriori.html","id":"algoritmo-di-metropolis","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"15.4.3.4 Algoritmo di Metropolis","text":"Implementiamo ora l’algoritmo di Metropolis. Utilizziamo una distribuzione proposta gaussiana. Il valore proposto da tale distribuzione ausiliaria corrisponde ad un valore selezionato caso da una distribuzione gaussiana con media uguale al valore del parametro attualmente considerato nella catena e con una deviazione standard ``adeguata’’. questo esempio, la deviazione standard è stata scelta empiricamente modo tale da ottenere un tasso di accettazione sensato. È stato mostrato che un tasso di accettazione ottimale dovrebbe essere tra il 20% e il 30%. Se il tasso di accettazione è troppo grande, infatti, l’algoritmo esplora uno spazio troppo ristretto della distribuzione posteriori. Il tasso di accettazione è influenzato dalla distribuzione proposta: generale, tanto più la distribuzione proposta è simile alla distribuzione target, tanto più alto diventa il tasso di accettazione.questa implementazione molto semplice della distribuzione proposta ho inserito dei controlli che fanno modo che il valore proposto da tale distribuzione ausiliaria sia incluso nell’intervallo [0, 1]. Si possono trovare implementazioni migliori di questa idea di quella fornita qui. Ma lo scopo è solo quello di spiegare la struttura logica dell’algoritmo di Metropolis, non quella di proporre un’implementazione efficente dell’algoritmo. Per nostri scopi, tale implementazione “ingenua” funziona, e tanto basta.L’algoritmo di Metropolis è implementato nella funzione seguente:Generiamo dunqe una catena di valori \\(\\theta\\) con le seguenti istruzioni:Otteniamo così 4,000 valori della distribuzione posteriori per il parametro \\(\\theta\\). Di questi valori, 2,000 vengono considerati burn-e vengono esclusi. Ci restano dunque con 2,000 stime posteriori di \\(\\theta\\).Il tasso di accettazione è pari ail che conferma che la deviazione standard che abbiamo scelto per la distribuzione proposta (\\(\\sigma\\) = 0.9) è adeguata.Una figura che rappresenta la distribuzione posteriori per \\(\\theta\\), insieme alla rappresentazione dei valori della catena di Markov realizzata dall’algoritmo di Metropolis, può essere prodotta mediante le seguenti istruzioni:questo punto è molto facile trovare il massimo posteriori per il parametro \\(\\theta\\):","code":"\nproposal_distribution <- function(param) {\n  while(1) {\n    res = rnorm(1, mean = param, sd = 0.9)\n    if (res > 0 & res < 1)\n      break\n  }\n  res\n}\nrun_metropolis_MCMC <- function(startvalue, iterations) {\n  chain <- vector(length = iterations + 1)\n  chain[1] <- startvalue\n  for (i in 1:iterations) {\n    proposal <- proposal_distribution(chain[i])\n    r <- posterior(proposal) / posterior(chain[i])\n    if (runif(1) < r) {\n      chain[i + 1] <- proposal\n    } else {\n      chain[i + 1] <- chain[i]\n    }\n  }\n  chain\n}\nset.seed(123)\nstartvalue <- runif(1, 0, 1)\nniter <- 1e4\nchain <- run_metropolis_MCMC(startvalue, niter)\nburnIn <- niter / 2\nacceptance <- 1 - mean(duplicated(chain[-(1:burnIn)]))\nacceptance\n#> [1] 0.2511498\np1 <- data.frame(x=chain[-(1:burnIn)]) %>% \n  ggplot(aes(x)) +\n  geom_histogram() +\n  labs(\n    x = expression(theta),\n    y = \"Frequenza\", \n    title = \"Distribuzione a posteriori\"\n  ) +\n  geom_vline(xintercept = mean(chain[-(1:burnIn)]))\n\np2 <- data.frame(x=1:length(chain[-(1:burnIn)]), y=chain[-(1:burnIn)]) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  labs(\n    x = \"Numero di passi\",\n    y = expression(theta), \n    title = \"Valori della catena\"\n  ) +\n  geom_hline(yintercept = mean(chain[-(1:burnIn)]))\n\np1 + p2\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nmean(chain[-(1:burnIn)])\n#> [1] 0.5921799"},{"path":"stima-della-funzione-a-posteriori.html","id":"sec:pregi_inferenza_bayes","chapter":"Capitolo 15 Stima della funzione a posteriori","heading":"Conclusioni","text":"Lo scopo di questa discussione è stato quello di mostrare come sia possibile combinare le nostre conoscenze priori (espresse nei termini di una densità di probabilità) con le evidenze fornite dai dati (espresse nei termini della funzione di verosimiglianza), così da determinare, mediante il teorema di Bayes, una distribuzione posteriori, la quale condensa l’incertezza che si ha sul parametro \\(\\theta\\). Per illustrare tale problema, nel caso più semplice abbiamo considerato una situazione nella quale \\(\\theta\\) corrisponde alla probabilità di successo una sequenza di prove Bernoulliane. Abbiamo visto come, queste circostanze, è ragionevole esprimere le nostre credenze priori mediante la densità Beta, con opportuni parametri. L’inferenza rispetto ad una proporzione rappresenta un caso particolare, ovvero un caso nel quale la distribuzione priori è Beta e la verosimiglianza è Binomiale. tali circostanze, anche la distribuzione posteriori sarà una distribuzione Beta. Per questa ragione, questo caso specifico, parametri della distribuzione posteriori possono essere determinati analiticamente (la soluzione richiede una serie di passaggi algebrici che qui non vengono discussi). generale, però, tale approccio non è perseguibile.La determinazione della distribuzione posteriori richiede il calcolo della funzione di verosimiglianza e dell’integrale che si trova al denominatore del rapporto di Bayes. Nel caso di parametri continui, però, spesso tale integrale può essere impossibile da risolvere analiticamente. passato, tale difficoltà è stata affrontata limitando l’analisi statistica al caso di funzioni di verosimiglianza semplici, le quali possono essere combinate con distribuzioni priori coniugate per la verosimiglianza, così da produrre un integrale trattabile.Invece di approcci matematici analitici, un’altra classe di metodi fa ricorso ’approssimazione numerica dell’integrale. Tale approssimazione numerica dipende dall’uso di metodi MCMC, ovvero dipende dall’uso di una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla grande potenza di calcolo dei moderni computer. Lo sviluppo di software che rendono sempre più semplice l’uso dei metodi MCMC, insieme ’incremento della potenza di calcolo dei computer, ha contribuito rendere sempre più popolare il metodo dell’inferenza Bayesiana che, questo modo, può essere estesa \nproblemi di qualunque grado di complessità.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"una-breve-introduzione-al-modello-di-regressione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"Capitolo 16 Una breve introduzione al modello di regressione","text":"questo capitolo verrà presentata un’introduzione “pratica” ’analisi della regressione, nella quale ci preoccuperemo di capire cosa serve come si interpretano risultati che tale metodo statistico produce. Nel capitolo successivo, gli stessi argomenti verranno trattati un modo più “formale” e con maggiori approfondimenti teorici. Questo capitolo contiene tutto quello che c’è da sapere e non si può non sapere su questo argomento. L’ho pensato per miei laureandi, ovvero per degli studenti che devono usare queste procedure statistiche per risolvere un problema pratico (quello di concludere la tesi). L’altro capitolo è più convenzionalmente “didattico” ed è stato pensato primo luogo per chi deve dare l’esame di Psicometria. Questo primo capitolo su questo tema può essere dunque pensato come un’introduzione “gentile” ciò che verrà discusso nel prossimo capitolo.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"regressione-bivariata","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.1 Regressione bivariata","text":"La regressione bivariata si pone il problema di descrivere la relazione statistica lineare che intercorre tra due variabili, \\(x\\) e \\(y\\). Per relazione “statistica” intendo dire che, nel caso dei dati campionari \\(\\{x, y\\}\\) di cui si occupa il modello di regressione, non c’è mai una “perfetta” relazione lineare (ovvero, punti del diagramma dispersione di \\(\\{x, y\\}\\) non si situano su una retta). alcuni casi, quando guardiamo il diagramma dispersione di \\(\\{x, y\\}\\) ci rendiamo conto che, effetti, punti \\(\\{x, y\\}\\), anche se non si dispongono su una retta, sono sparpagliati attorno ad una retta “virtuale” che passa attraverso la nube di punti. una tale situazione (che è una delle tante possibili, non l’unica), è ragionevole descrivere la relazione tra le variabili \\(x\\) e \\(y\\) mediante la retta che, al meglio, approssima la nube di punti nel diagramma disperiosne. L’analisi di regressione si pone il problema di trovare l’inclinazione di quella retta che passa il più vicino possibile ai punti del diagramma dispersione di \\(\\{x, y\\}\\).","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"scioglimento-del-ghiaccio-marino","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.1.1 Scioglimento del ghiaccio marino","text":"Uno degli impatti più importanti dei cambiamenti climatici che stanno investendo il nostro Pianeta è la riduzione dell’estensione della calotta di ghiaccio marino artico. Esploriamo come l’estensione del ghiaccio marino artico sta cambiando nel tempo utilizzando un modello lineare. dati sono forniti da National Snow Ice Data Center e sono espressi milioni di chilometri quadrati.dati sono seguenti:Quale domanda di ricerca possiamo porci con questi dati? Propongo la seguente domanda.Domanda di ricerca: l’estensione del ghiaccio marino artico sta diminuendo nel tempo?Per esplorare la risposta questa domanda, iniziamo creare una rappresentazione grafica dei dati. Dato che abbiamo due variabi continue (il tempo, espresso anni, e l’estensione del ghiaccio marino artico, milioni di chilometri quadrati), questi dati possono essere rappresentati graficamente mediante un diagramma dispersione.\nVogliamo sapere come varia l’estensione del ghiaccio marino artico funzione del tempo e quindi disegnamo dati ponendo la variabile tempo sull’asse delle ascisse e l’estensione del ghiaccio marino artico sull’asse delle ordinate.\nGuardando la figura vediamo che è ragionevole descrivere mediante una retta la relazione tra l’estensione del ghiaccio marino artico (chiamiamola y) e il tempo (chiamiamolo x). Aggiungiamo al diagramma di dispersione una retta, scegliendola modo tale che si avvicini il più possibile alla nube di punti rappresentata nel grafico. Ovviamente, è possibile scegliere tra infinite rette diverse. La retta che è rappresentata qui è stata scelta base ad un criterio particolare, detto “dei minimi quadrati.” Vedremo meglio seguito cosa questo significa. Per ora ci accontentiamo di riconoscere che la nostra è una buona scelta, per gli scopi presenti.","code":"\ndata.frame(seaice)\n#>    year extent_north extent_south\n#> 1  1979       12.328       11.700\n#> 2  1980       12.337       11.230\n#> 3  1981       12.127       11.435\n#> 4  1982       12.447       11.640\n#> 5  1983       12.332       11.389\n#> 6  1984       11.910       11.454\n#> 7  1985       11.995       11.618\n#> 8  1986       12.203       11.088\n#> 9  1987       12.135       11.554\n#> 10 1988       11.923       12.131\n#> 11 1989       11.967       11.426\n#> 12 1990       11.694       11.410\n#> 13 1991       11.749       11.545\n#> 14 1992       12.110       11.399\n#> 15 1993       11.923       11.420\n#> 16 1994       12.011       11.774\n#> 17 1995       11.415       11.795\n#> 18 1996       11.841       11.769\n#> 19 1997       11.668       11.390\n#> 20 1998       11.757       11.738\n#> 21 1999       11.691       11.761\n#> 22 2000       11.508       11.747\n#> 23 2001       11.600       11.673\n#> 24 2002       11.363       11.222\n#> 25 2003       11.397       11.969\n#> 26 2004       11.240       11.961\n#> 27 2005       10.907       11.695\n#> 28 2006       10.773       11.461\n#> 29 2007       10.474       11.687\n#> 30 2008       10.978       12.239\n#> 31 2009       10.932       12.049\n#> 32 2010       10.711       12.107\n#> 33 2011       10.483       11.501\n#> 34 2012       10.406       12.004\n#> 35 2013       10.897       12.524\n#> 36 2014       10.790       12.776\n#> 37 2015       10.566       12.414\n#> 38 2016       10.151       11.156\n#> 39 2017       10.373       10.693\nseaice %>% \n  ggplot(aes(year, extent_north)) +\n  geom_point()\nseaice %>% \n  ggplot(aes(year, extent_north)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interpretazione-dei-coefficienti-a-e-b","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.2 Interpretazione dei coefficienti \\(a\\) e \\(b\\)","text":"La retta che abbiamo disegnato nella figura precedente rappresenta la risposta alla nostra domanda di ricerca: l’estensione del ghiaccio marino artico sta diminuendo nel tempo.Anziché considerare questa risposta unicamente dal punto di vista grafico, proviamo descrivere la retta disegnata nella figura maniera quantitativa, con dei numeri. Per fare questo, dobbiamo innanzitutto ricordare qual è l’equazione di una retta:\\[\\begin{equation}\ny = + b \\times x\n\\end{equation}\\]Prima di calcolare coefficienti \\(\\) e \\(b\\) della retta di regressione, rimaneggiamo nostri dati. particolare, rinominiamo le variabili e indicizziamo gli anni da 1 39. Nel caso presente, vogliamo sapere se l’estensione del ghiaccio marino artico dall’inizio alla fine del periodo temporale considerato, indipendentemente dal fatto che l’anno iniziale sia il 1979 e l’anno finale il 2017. Quindi sottraiamo 1979 dalle modalità della variabile year modo tale che il primo punto temporale corrisponda zero.Il diagramma dispersione avrà ora la forma seguente:Per trovare coefficienti \\(\\) e \\(b\\) della retta di regressione possiamo usare, ad esempio, la funzione lm():L’output della funzione lm() ci dice che è uguale 12.501 e che b è uguale -0.055. Ma che significato (geometrico) hanno questi valori?Ai coefficienti \\(\\) e \\(b\\) possiamo assegnare la seguente interpretazione:il coefficiente \\(\\) rappresenta il valore della coordinata \\(y\\) (l’estensione del ghiaccio marino artico) della retta di regressione quando la coordinata \\(x\\) vale zero (nel nostro caso, l’anno 1979) – altre parole, corrisponde al punto dove la retta di regressione interseca l’asse \\(y\\) del sistema di assi cartesiani;il coefficiente \\(\\) rappresenta il valore della coordinata \\(y\\) (l’estensione del ghiaccio marino artico) della retta di regressione quando la coordinata \\(x\\) vale zero (nel nostro caso, l’anno 1979) – altre parole, corrisponde al punto dove la retta di regressione interseca l’asse \\(y\\) del sistema di assi cartesiani;il coefficiente \\(b\\) ci dice di quanto aumenta la coordinata \\(y\\) della retta di regressione, quando \\(x\\) aumenta di un’unità.il coefficiente \\(b\\) ci dice di quanto aumenta la coordinata \\(y\\) della retta di regressione, quando \\(x\\) aumenta di un’unità.Il valore \\(\\) = 12.501 significa che, nel 1979, l’estensione del ghiaccio marino artico era pari 12.501 milioni di chilometri quadrati, dato che la modalità x = 0 della variabile indipendente corrisponde ’anno 1979.Il segno di \\(b\\) è negativo; questo significa che l’estensione del ghiaccio marino artico sta diminuendo nel corso del tempo. Il valore -0.055 ci dice che, per ogni anno che passa (nel periodo dal 1979 al 2017), l’estensione del ghiaccio marino artico diminuisce, media, di -0.055 milioni di chilometri quadrati.Se guardiamo la figura, infatti, vediamo che, se ci sposiamo da \\(x = 10\\) \\(x = 20\\) (ovvero, di dieci anni), la coordinata \\(y\\) della retta diminuisce di 10 volte \\(b\\), ovvero di -0.55 milioni di chilometri quadrati. Questo è indicato nella figura qui sotto.","code":"\nseaice <- seaice %>% \n  mutate(\n    x = year - 1979\n  ) %>% \n  rename(\n    y = extent_north\n  )\nglimpse(seaice)\n#> Rows: 39\n#> Columns: 4\n#> $ year         <int> 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 19…\n#> $ y            <dbl> 12.328, 12.337, 12.127, 12.447, 12.332, 11.910, 11.995, 12.203, 12.1…\n#> $ extent_south <dbl> 11.700, 11.230, 11.435, 11.640, 11.389, 11.454, 11.618, 11.088, 11.5…\n#> $ x            <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\nseaice %>% \n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'\nfm <- lm(y ~ x, data = seaice)\ncoef(fm)\n#> (Intercept)           x \n#> 12.50131410 -0.05457389\ndplot <- data.frame(\n  x1 = 10,\n  x2 = 20,\n  y1 = 12.50131410 + -0.05457389 * 10,\n  y2 = 12.50131410 + -0.05457389 * 20\n)\n\nseaice %>%\n  ggplot(aes(x, y)) +\n  geom_point(color=\"gray\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(x = x2, y = y1, xend = x2, yend = y2),\n               arrow=arrow(), size=1.1, data = dplot) +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y1),\n               arrow=arrow(), size=1.1, data = dplot) +\n  annotate(\"text\", x = 24.0, y = 11.75, label = \"10 b = -0.55\") +\n  annotate(\"text\", x = 15, y = 12.15, label = \"delta x = 10.0\")\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"scomposizione-della-y","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.3 Scomposizione della \\(y\\)","text":"Uno degli aspetti importanti della regressione lineare è che, pratica, la retta di regressione scompone ciascun punteggio \\(y\\) due componenti:\\[\\begin{equation}\ny = (+ b \\times x) + e\n\\end{equation}\\]laddove \\(\\hat{y} = + b \\times x\\) è la componente di \\(y\\) che è linearmente predicibile conoscendo \\(x\\), mentre la componente residua, \\(e\\), è la componente di \\(y\\) che non è linearmente predicibile conoscendo \\(x\\).Nei nostri dati, questi significa quanto segue. Esaminiamo le prime 5 osservazioni del nostro campione:Aggiungo al data.frame una colonna che rappresenta valori \\(y\\) predetti dal modello lineare (ovvero, \\(\\hat{y} = + b \\times x\\): le coordinate \\(y\\) della retta di regressione per ciascuna delle osservazioni):Aggiungo ora al data.frame residui del modello di regressione. residui sono definiti come\\[\\begin{equation}\ne = y - (+ b \\times x) = y - \\hat{y}\n\\end{equation}\\]ovveroCiò che volevamo dimostrare è che la somma di \\(\\hat{y}\\) e dei residui è uguale alla \\(y\\). Infatti:altre parole, il modello di regressone scompone la \\(y\\) due componenti: la porzione della \\(y\\) che possiamo prevedere conoscendo \\(x\\) (ovvero, \\(\\hat{y} = + b \\times x\\)) e la porzione della \\(y\\) che non possiamo prevedere sulla base di \\(x\\).","code":"\nseaice %>% \n  dplyr::select(x, y) %>% \n  top_n(5) \n#> Selecting by y\n#>   x      y\n#> 1 0 12.328\n#> 2 1 12.337\n#> 3 3 12.447\n#> 4 4 12.332\n#> 5 7 12.203\nseaice$yhat <- coef(fm)[1] + coef(fm)[2] * seaice$x\nseaice %>% \n  dplyr::select(x, y, yhat) %>% \n  top_n(5) \n#> Selecting by yhat\n#>   x      y     yhat\n#> 1 0 12.328 12.50131\n#> 2 1 12.337 12.44674\n#> 3 2 12.127 12.39217\n#> 4 3 12.447 12.33759\n#> 5 4 12.332 12.28302\nseaice$e <- seaice$y - (coef(fm)[1] + coef(fm)[2] * seaice$x)\nseaice %>% \n  dplyr::select(x, y, yhat, e) %>% \n  top_n(5) \n#> Selecting by e\n#>    x      y     yhat         e\n#> 1 13 12.110 11.79185 0.3181464\n#> 2 15 12.011 11.68271 0.3282942\n#> 3 19 11.757 11.46441 0.2925897\n#> 4 20 11.691 11.40984 0.2811636\n#> 5 22 11.600 11.30069 0.2993114\nseaice$sum <- seaice$yhat + seaice$e\nseaice %>% \n  dplyr::select(x, y, yhat, e, sum) %>% \n  top_n(5) \n#> Selecting by sum\n#>   x      y     yhat           e    sum\n#> 1 0 12.328 12.50131 -0.17331410 12.328\n#> 2 1 12.337 12.44674 -0.10974022 12.337\n#> 3 3 12.447 12.33759  0.10940756 12.447\n#> 4 4 12.332 12.28302  0.04898144 12.332\n#> 5 7 12.203 12.11930  0.08370310 12.203"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"metodo-di-stima-dei-coefficienti-del-modello-di-regressione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.3.1 Metodo di stima dei coefficienti del modello di regressione","text":"Come abbiamo fatto calcolare coefficienti dei minimi quadrati \\(\\) e \\(b\\)? Ci sono tanti metodi; il più comune si chiama “metodo dei minimi quadrati.” Lo esamineremo seguito. Per nostri scopi, è poco importante: per ora ci accontentiamo di chiedere ad R di fare calcoli per noi.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"inferenza-sul-modello-di-regressione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.4 Inferenza sul modello di regressione","text":"Finora siamo riusciti descrivere la relazione statistica tra \\(x\\) e \\(y\\) un campione di osservazioni. Siamo ben consapevoli che, un altro campione di osservazioni, la relazione statistica tra \\(x\\) e \\(y\\) non sarà identica quella osservata nel caso del primo campione – dato che dati saranno diversi. La domanda cruciale dunque diventa la seguente: quanto sono simili coefficienti dei minimi quadrati calcolati sui dati campionari ai coefficienti di un’ipotetica retta di regressione che, sempre mediante il metodo dei minimi quadrati, descriverebbe la relazione tra tutte le osservazioni \\(\\{x, y\\}\\) nella popolazione? Ciò che vogliamo è dunque una quantificazione della nostra incertezza: vogliamo sapere quanto dobbiamo fidarci delle stime campionarie come descrizioni dei valori (sconosciuti) della popolazione. Ci sono due possibilità.Se le stime di \\(\\) e \\(b\\) fornite dal particolare campione che abbiamo osservato sono simili ai valori teorici della popolazione, allora dati del campione ci forniscono informazioni utili per capire quali sono le proprietà della popolazione.Se le stime di \\(\\) e \\(b\\) fornite dal particolare campione che abbiamo osservato sono simili ai valori teorici della popolazione, allora dati del campione ci forniscono informazioni utili per capire quali sono le proprietà della popolazione.Se invece le stime campionarie di \\(\\) e \\(b\\) sono molto diverse dai valori teorici della popolazione, allora dati del campione non ci aiutano capire quali sono le proprietà della popolazione.Se invece le stime campionarie di \\(\\) e \\(b\\) sono molto diverse dai valori teorici della popolazione, allora dati del campione non ci aiutano capire quali sono le proprietà della popolazione.Il problema che abbiamo è quello di decidere quale di queste due situazioni ci troviamo: la prima o la seconda. Questo è il problema dell’inferenza statistica sul modello di regressione.Il problema dell’inferenza viene affrontato utilizzando gli strumenti della teoria della probabilità per costruire un intervallo di valori. Nell’approccio Bayesiano, tale intervallo di valori si chiama intervallo di credibilità. Se calcoliamo, ad esempio, l’intervallo di credibilità ’89% per il parametro \\(\\beta\\) (inclinazione della retta di regressione nella popolazione), interpretiamo tale intervallo di valori nel modo seguente: possiamo dire che “siamo sicuri ’89% che il vero valore di \\(\\beta\\) è contenuto nell’intervallo stimato” – laddove per “vero valore di \\(\\beta\\)” intendiamo il valore del parametro sconosciuto della popolazione. Se calcoliamo l’intervallo di credibilità, ad un dato livello di certezza, possiamo giungere una di tre possibili conclusioni.L’intervallo di credibilità non include lo 0 e il suo limite inferiore è positivo: possiamo concludere, con una certezza dell’89%, che c’è una relazione positiva tra \\(x\\) e \\(y\\) nella popolazione;l’intervallo di credibilità non include lo 0 e il suo limite superiore è negativo: possiamo concludere, con una certezza del’89%, che c’è una relazione negativa tra \\(x\\) e \\(y\\) nella popolazione;l’intervallo di credibilità include lo 0: con un un livello di certezza dell’89%, non possiamo negare la possibilità che nella popolazione la relazione tra \\(x\\) e \\(y\\) sia nulla – ovvero, non abbiamo sufficienti informazioni per sapere se è positiva o negativa.Ritorniamo ora ai nostri dati e calcoliamo, con un metodo che discuteremo seguito, l’intervallo di credibilità ’89%. Per ora non ci preoccupiamo della sintassi dei comandi R, né di cosa significano tali istruzioni. Ci preoccupiamo solo di ottenere le stime dei coefficienti della retta di regressione e gli intervalli di credibilità:L’intervallo di credibilità per \\(b\\) così ottenuto, ovvero [-0.059, -0.049], non include lo zero. Possiamo dunque concludere, con un livello di certezza dell’89%, che nella popolazione c’è una relazione negativa tra \\(x\\) e \\(y\\). altre parole, siamo certi, con un livello di certezza dell’89%, che dal 1979 al 2017 l’estensione del ghiaccio marino artico è diminuita.Se vogliamo, possiamo anche ottenere una stima di \\(b\\) che corrisponde ad un livello di certezza più alto, il che porterà ad un aumento dell’ampiezza dell’interallo di credibilità. Stimiamo dunque l’interallo di credibilità ad un livello di certezza del 99%:Anche questo caso l’interallo di credibilità non include lo zero per cui, anche se pretendiamo un livello di certazza del 99%, confermiamo la conclusione secondo la quale dal 1979 al 2017 l’estensione del ghiaccio marino artico è diminuita.Per quel che riguarda \\(\\), con un livello di certezza dell’99% possiamo dire che il valore dell’intercetta della retta di regressione nella popolazione è incluso nell’intervallo [12.313, 12.661]. Nel caso presente, ciò ha una semplice interpretazione: significa che la nostra stima dell’esensione del ghiaccio marino artico nell’anno 1979 corrisponde un valore compreso tra [12.313, 12.661], con un livello di certezza del 99%.","code":"\nflist <- alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b*x,\n    c(a, b) ~ dnorm(0, 2), \n    sigma ~ dcauchy(0, 2)\n)\n\nfit <- quap(\n  flist, \n  data = list(y=seaice$y, x=seaice$x), \n  start = list(a=0, b=0, sigma=0.1)\n)\n\nout <- round(precis(fit), 3)\nout\n#>         mean    sd   5.5%  94.5%\n#> a     12.487 0.068 12.379 12.595\n#> b     -0.054 0.003 -0.059 -0.049\n#> sigma  0.215 0.024  0.176  0.254\nout1 <- round(precis(fit, prob=0.99), 3)\nout1\n#>         mean    sd   0.5%  99.5%\n#> a     12.487 0.068 12.313 12.661\n#> b     -0.054 0.003 -0.062 -0.046\n#> sigma  0.215 0.024  0.152  0.277"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"osservazione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"Osservazione","text":"Potremmo chiedersi perché sia necessario fare delle inferenze sul fenomeno della variazione dell’estensione del ghiaccio marino artico. Se dati ci dicono che l’estensione del ghiaccio marino artico è diminuita, che bisogno abbiamo di fare un’inferenza? La risposta sembra già essere contenuta nei dati.realtà, le cose sono un po’ più complicate. La misurazione dell’aspetto di interesse, questo caso, l’estensione del ghiaccio marino artico, non è priva di errori di misurazione. È ovvio che, se facessimo misurazioni ripetute nel corso dello stesso anno, otterremmo valori diversi. Inltre, al di là degli errori di misurazione, l’estensione del ghiaccio marino artico è soggetta variazioni stagionali e continue nel tempo e fluttuazioni stagionali.Quindi, dati del nostro campione non catturano perfettamente ciò che vogliamo sapere. Siamo infatti consapevoli del fatto che, se ripetessimo le nostre misurazioni, otterremmo valori diversi. Il problema che abbiamo è dunque il seguente: come facciamo rispondere alla domanda della ricerca avendo osservato solo uno degli infiniti campioni diversi di osservazioni che descrivono l’aspetto che ci interessa? Per quantificare la nostra incertezza associamo alla stima della quantità di interesse (questo caso, la variazione dell’estensione del ghiaccio marino artico nell’unità di tempo) un intervallo di credibilità calcolato utilizzando un determinato livello di certezza.Queste affermazioni sono certamente vere nel caso del ghiaccio marino dell’Artico. Ma un lettore attento si sarà anche reso conto del fatto che tale problema (l’errore di misurazione e la variabilità campionaria) è certamente presente anche quando di occupiamo della misurazione dei costrutti psicologici. Questa è la ragione per la quale, come psicologi, dobbiamo essere grado di quantificare la nostra incertezza quando stimiamo le caratteristiche generali di ciò che ci interessa, ovvero quando vogliamo descrivere ciò che ci interessa al di là degli aspetti idiosincratici del campione di dati che abbiamo osservato.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"errore-standard-della-stima","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.4.1 Errore standard della stima","text":"Il parametto sigma nell’output di precis() ci fornisce un altro utile pezzo di informazione: esso stima infatti quanto sono distanti, media, le osservazioni dalla retta di regressione nella popolazione. Il valore sigma, chiamato errore standard della stima, ci dice che il modello di regressione, stimando la relazione tra \\(y\\) e \\(x\\), compie un errore medio di 0.215 milioni di chilometri quadrati – laddove 0.215 è la stima della distanza media (milioni di chilometri quadrati) tra ciascuno dei punti del diagramma dispersione e la retta di regressione nella popolazione.","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"errori-standard-delle-stime-a-e-b","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.4.2 Errori standard delle stime \\(a\\) e \\(b\\)","text":"Il valore sd associato \\(b\\) nell’output di precis() ci dice invece invece di quanto dobbiamo aspettarci che vari la stima di \\(b\\) da campione campione. altri termini, causa della variabilità campionaria, la stima di \\(b\\) assume un valore diverso ciascuno degli infinti campioni di \\(n\\) osservazioni che descrivono la relazione tra \\(x\\) e \\(y\\), nel periodo temporale considerato. Il valore sd uguale 0.003 ci dice che, media, la stima della pendenza della retta di regressione differirà dal vero valore di questo parametro di una quantità pari 0.003 milioni di chilometri quadrati.\nUna affermazione simile può essere fatta proposito del valore sd associato ad \\(\\) nell’output di precis().","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"una-variabile-indipendente-qualitativa","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.5 Una variabile indipendente qualitativa","text":"Il modello statistico della regressione bivariato che abbiamo descritto sopra è molto limitato: può solo descrivere la relazione lineare tra due variabili continue. Estendiamolo ora al caso cui, oltre ad includere un predittore continuo \\(x\\), il modello di regressione comprende anche una variabile che consente di suddividere le osservazioni del campione due gruppi.Per semplicità, simuliamo un insieme di dati che utilizzeremo nella seguente discussione.Esaminiamo il diagramma dispersione che indica chiaramente che ci sono due gruppi distinti di osservazioni:È ovvio, che nel caso presente, non è sufficiente un modello di regressione che ignora il fatto che dati appartengono due gruppi diversi. Infatti, se adattiamo ai dati un’unica retta di regressione, è facile rendersi conto che tale retta si situerà una posizione molto distante dai dati.Per questi dati è ovviamente più sensato adattare una diversa retta di regressione ciascun gruppo di osservazioni:Come possiamo modificare il modello di regressione che abbiamo esaminato precedenza modo tale da essere grado di stimare coefficienti di regressione delle due rette rappresentate nella figura precedente? Questo problema si risolve nel modo seguente. Crediamo una variabile, chiamata dummy, che ha valore 0 per un gruppo, diciamo gr1, e valore 1 per l’altro gruppo:Esaminiamo ora il modello di regressione che include tale variabile dummy, che chiameremo D:\\[\\begin{equation}\ny = \\alpha + \\beta x + \\gamma D + \\varepsilon.\n\\end{equation}\\]Quando \\(D = 0\\):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 0 + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quando \\(D = 1\\):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 1 + \\varepsilon,\\notag\\\\\n &= (\\alpha + \\gamma) + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quindi, se assumiamo che le due rette di regressione siano parallele (nella discussione precedente abbiamo previsto un unico valore \\(\\beta\\)), coefficienti del modello avranno la seguente interpretazione:\\(\\alpha\\) = intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\beta\\) = pendenza della retta di regressione per il gruppo codificato con D = 0;\\(\\gamma\\) = differenza tra l’intercetta della retta di regressione per il gruppo codificato con D = 1 e l’intercetta della retta di regressione per il gruppo codificato con D = 0.","code":"\nset.seed(1234)\nn <- 100\nx1 <- rnorm(n, 4, 1)\ny1 <- 5 + 5 * x1 + rnorm(n, 0, 2)\n\nx2 <- rnorm(n, 4, 1)\ny2 <- -3 + 2 * x2 + rnorm(n, 0, 2)\n\ny <- c(y1, y2)\nx <- c(x1, x2)\ngroup <- rep(c(\"gr1\", \"gr2\"), each = n)\n\nd <- data.frame(x, y, group) \n\nd %>%\n  group_by(group) %>% \n  do(head(., 5))\n#> # A tibble: 10 x 3\n#> # Groups:   group [2]\n#>       x     y group\n#>   <dbl> <dbl> <fct>\n#> 1  2.79 19.8  gr1  \n#> 2  4.28 25.4  gr1  \n#> 3  5.08 30.6  gr1  \n#> 4  1.65 12.3  gr1  \n#> 5  4.43 25.5  gr1  \n#> 6  4.49  4.81 gr2  \n#> # … with 4 more rows\nd %>%\n  ggplot(aes(x, y)) +\n  geom_point(aes(color = group))\nd %>%\n  ggplot(aes(x, y)) +\n  geom_point(aes(color = group)) +\n  geom_smooth(method = \"lm\", se = FALSE) \n#> `geom_smooth()` using formula 'y ~ x'\nd %>%\n  ggplot(aes(x, y, color = group)) +\n  geom_point(aes(color = group)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\nd$gr <- ifelse(d$group == \"gr1\", 0, 1) \n\nd %>%\n  group_by(gr) %>% \n  do(head(., 5)) %>% \n  as.data.frame()\n#>           x         y group gr\n#> 1  2.792934 19.793718   gr1  0\n#> 2  4.277429 25.437709   gr1  0\n#> 3  5.084441 30.554193   gr1  0\n#> 4  1.654302 12.266556   gr1  0\n#> 5  4.429125 25.493626   gr1  0\n#> 6  4.485227  4.810540   gr2  1\n#> 7  4.696769  4.486980   gr2  1\n#> 8  4.185514  5.012171   gr2  1\n#> 9  4.700734  8.421083   gr2  1\n#> 10 4.311681  5.670615   gr2  1"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interazione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.5.1 Interazione","text":"generale, però, le due rette di regressione non sono parallele. Per potere rappresentare una tale possibilità, introduciamo nel data.frame d una seconda variabile e la chiamiamo DX. Creaimo DX facendo il prodotto delle variabili D e x:Si noti che, quando D = 0, allora DX = 0; quando D = 1, allora DX = x.Riscriviamo il modello di regressione nel modo seguente:\\[\\begin{equation}\ny = \\alpha + \\beta x + \\gamma D + \\zeta DX + \\varepsilon.\n\\end{equation}\\]tali circostanze, quando \\(D = 0\\), abbiamo che:\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\zeta DX  + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 0 + \\zeta \\times 0 + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\varepsilon.\\notag\n\\end{align}\\]Quando \\(D = 1\\) (ricordiamo: questo caso \\(DX = x\\)):\\[\\begin{align}\ny &= \\alpha + \\beta x + \\gamma D + \\zeta DX + \\varepsilon,\\notag\\\\\n &= \\alpha + \\beta x + \\gamma \\times 1 + \\zeta x + \\varepsilon,\\notag\\\\\n &= (\\alpha + \\gamma) + (\\beta + \\zeta) x + \\varepsilon.\\notag\n\\end{align}\\]Ciò significa che coefficienti del modello di regressione avranno ora la seguente interpretazione:\\(\\alpha\\) = intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\beta\\) = pendenza della retta di regressione per il gruppo codificato con D = 0;\\(\\gamma\\) = differenza tra l’intercetta della retta di regressione per il gruppo codificato con D = 1 e l’intercetta della retta di regressione per il gruppo codificato con D = 0;\\(\\zeta\\) = differenza tra la pendenza della retta di regressione per il gruppo codificato con D = 1 e la pendenza della retta di regressione per il gruppo codificato con D = 0;Ritorniamo al nostro esempio numerico e calcoliamo coefficienti del modello di regressione utilizzando l’approccio Bayesiano:un tale modello, la prima domanda che dobbiamo porci è se dati giustificano la conclusione secondo la quale, nella popolazione, le due rette hanno una pendenza diversa. Per rispondere tale domanda esaminiamo l’intervallo di credibilità del coefficiente associato al termine d’interazione, ovvero del coefficiente \\(\\zeta\\). R ci dice che l’intervallo di credibilità ’89% per il coefficiente \\(\\zeta\\) è pari [-3.36, -2.41]. Dato che tale intervallo non include lo zero, con un livello di certezza dell’89% concludiamo che la retta di regressione del gruppo codificato con D = 1 (ovvero gr2) ha una pendenza minore della retta di regressione per il gruppo codificato con D = 0 (ovvero gr1).","code":"\nd$DX <- d$gr * d$x\n\nd %>%\n  group_by(gr) %>% \n  do(head(., 5)) %>% \n  as.data.frame()\n#>           x         y group gr       DX\n#> 1  2.792934 19.793718   gr1  0 0.000000\n#> 2  4.277429 25.437709   gr1  0 0.000000\n#> 3  5.084441 30.554193   gr1  0 0.000000\n#> 4  1.654302 12.266556   gr1  0 0.000000\n#> 5  4.429125 25.493626   gr1  0 0.000000\n#> 6  4.485227  4.810540   gr2  1 4.485227\n#> 7  4.696769  4.486980   gr2  1 4.696769\n#> 8  4.185514  5.012171   gr2  1 4.185514\n#> 9  4.700734  8.421083   gr2  1 4.700734\n#> 10 4.311681  5.670615   gr2  1 4.311681\nflist1 <- alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b*x + gamma*d + zeta*DX,\n    c(a, b, gamma, zeta) ~ dnorm(0, 10), \n    sigma ~ dnorm(0, 5)\n)\nfit1 <- quap(\n  flist1, \n  data = list(y=d$y, x=d$x, d=d$gr, DX=d$DX), \n  start = list(a=0, b=0, gamma=0, zeta=0, sigma=0.5)\n  )\n\nprecis(fit1)\n#>            mean        sd       5.5%     94.5%\n#> a      5.202783 0.8167093   3.897524  6.508043\n#> b      4.967164 0.2057473   4.638340  5.295988\n#> gamma -8.569902 1.2250868 -10.527828 -6.611977\n#> zeta  -2.881817 0.2970176  -3.356509 -2.407126\n#> sigma  2.069882 0.1034130   1.904608  2.235155"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"regressione-multipla","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.6 Regressione multipla","text":"","code":""},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"assenza-di-interazione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.6.1 Assenza di interazione","text":"Esaminiamo ora dati epi.bfi contenuti nel pacchetto psychTools che contengono 231 osservazioni relative 5 scale dall’Eysenck Personality Inventory, 5 scale del Big 5, valori del Beck Depression Inventory e misure di ansia di stato e ansia di tratto.Supponiamo di chiederi se c’è una relazione tra la depressione (bdi), considerata quale variabile dipendente, e Neuroticismo e ansia di stato. Considereremo seguito il problema della possibile interazione tra Neuroticismo e ansia di stato. Per ora, scriviamo il modello di regressione nel modo seguente:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]Abbiamo visto precedenza come si interpretano coefficienti di regressione nel caso della regressione bivariata. coefficienti di regressione, detti parziali, che fanno parte del modello di regressione multipla hanno però un significato diverso da quello descritto precedenza. Poniamoci dunque il problema di capire qual è la differenza tra un coefficiente di regressione (nel modello bivariato) e un coefficiente parziale di regressione (nel modello di regressione multipla).precedenza abbiamo visto che il coefficiente di regressione \\(\\beta\\) ha il seguente significato: indica la variazione del valore atteso della \\(y\\) base al modello lineare nel caso di un cambiamento unitario della \\(x\\). Ma adesso questa spiegazione non basta, quanto nel modello di regressione multipla non c’è una sola variabile indipendente: nel caso considerato qui ce ne sono due. Dunque, qual è il significato di \\(\\beta_1\\) e di \\(\\beta_2\\)?","code":"\nsuppressMessages(library(\"sjPlot\"))\nsuppressMessages(library(\"sjmisc\"))\nlibrary(\"psychTools\")\ndata(epi.bfi)\nglimpse(epi.bfi)\n#> Rows: 231\n#> Columns: 13\n#> $ epiE     <int> 18, 16, 6, 12, 14, 6, 15, 18, 15, 8, 13, 14, 15, 19, 15, 11, 16, 17, 7, …\n#> $ epiS     <int> 10, 8, 1, 6, 6, 4, 9, 9, 11, 5, 9, 12, 10, 11, 10, 5, 10, 11, 4, 8, 7, 7…\n#> $ epiImp   <int> 7, 5, 3, 4, 5, 2, 4, 7, 3, 2, 3, 3, 4, 7, 4, 6, 5, 6, 2, 4, 5, 3, 7, 1, …\n#> $ epilie   <int> 3, 1, 2, 3, 3, 5, 3, 2, 3, 2, 3, 6, 5, 0, 2, 7, 0, 4, 1, 4, 3, 1, 2, 2, …\n#> $ epiNeur  <int> 9, 12, 5, 15, 2, 15, 12, 10, 1, 10, 9, 1, 2, 3, 7, 13, 18, 11, 10, 12, 3…\n#> $ bfagree  <int> 138, 101, 143, 104, 115, 110, 109, 92, 127, 74, 124, 131, 133, 117, 99, …\n#> $ bfcon    <int> 96, 99, 118, 106, 102, 113, 58, 57, 108, 100, 114, 107, 114, 126, 107, 7…\n#> $ bfext    <int> 141, 107, 38, 64, 103, 61, 99, 94, 108, 61, 100, 99, 114, 139, 124, 112,…\n#> $ bfneur   <int> 51, 116, 68, 114, 86, 54, 55, 72, 35, 87, 110, 92, 56, 47, 62, 83, 106, …\n#> $ bfopen   <int> 138, 132, 90, 101, 118, 149, 110, 114, 86, 89, 129, 121, 131, 128, 139, …\n#> $ bdi      <int> 1, 7, 4, 8, 8, 5, 7, 0, 0, 7, 8, 3, 0, 0, 1, 4, 14, 7, 9, 4, 1, 11, 1, 9…\n#> $ traitanx <int> 24, 41, 37, 54, 39, 51, 40, 32, 22, 35, 43, 33, 23, 23, 27, 45, 58, 39, …\n#> $ stateanx <int> 22, 40, 44, 40, 67, 38, 32, 41, 26, 31, 39, 25, 32, 23, 28, 28, 56, 44, …"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"significato-dei-coefficienti-parziali-di-regressione","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.6.1.1 Significato dei coefficienti parziali di regressione","text":"La risposta è che un generico \\(\\beta_j\\) nel modello di regressione multipla rappresenta la variazione del valore atteso della \\(y\\) base al modello lineare nel caso di un cambiamento unitario di \\(x_j\\) al netto dell’effetto (lineare) di tutte le altre variabili \\(x\\) incluse nel modello.La seconda parte dell’affermazione precedente (“al netto di…”) è ciò che dobbiamo capire.Iniziamo calcolare con R coefficienti parziali del modello di regressione multipla descritto sopra:coefficienti parziali di regressione sono:Iniziamo considerare il coefficiente parziale associato Neuroticismo. Il valore \\(\\beta_1\\) = 0.054414 ci dice che ci aspettiamo che il valore di depressione aumenti media di \\(\\beta_1\\) = 0.054414 punti quando il livello di Neuroticismo aumenta di un punto e il livello di ansia di stato viene mantenuto costante.\nMa che vuol dire “mantenere costante” il livello di ansia di stato? Questo è un esempio di controllo statistico e può essere spiegato nel modo seguente.Abbiamo visto precedenza che il modello lineare scompone la \\(y\\) due componenti: la componente predicibile da \\(x\\) e la componente della \\(y\\) che un modello lineare non può prevedere.\nScomponiamo dunque valori bdi due componenti: la quota del bdi che l’ansia di stato può prevedere (con un modello lineare) e la quota del bdi che l’ansia di stato non può prevedere. Tale seconda componente di bdi è data dai residui (\\(\\varepsilon\\)) del seguente modello di regressione:\\[\\begin{equation}\n\\text{bdi} = \\alpha + \\beta \\cdot \\text{stateanx} + \\varepsilon.\n\\end{equation}\\]Usando R, tali residui sono dati da:Eseguiamo una scomposizione simile per valori di Neuroticismo: troviamo la quota di bfneur che l’ansia di stato può prevedere e la quota di bfneur che l’ansia di stato non può prevedere. Tale seconda componente di bfneur è data dai residui del seguente modello di regressione:\\[\\begin{equation}\n\\text{bfneur} = \\alpha + \\beta \\cdot \\text{stateanx} + \\varepsilon.\n\\end{equation}\\]Usando R, tali residui sono dati da:Abbiamo così ottenuto la componente della variabile dipendente linermente indipendente da stateanx e la componente di bfneur linermente indipendente da stateanx. altri termini, abbiamo “statisticamente controllato” l’effetto di stateanx su bdi e su bfneur. Se adesso vogliamo sapere qual è l’effetto di bfneur su bdi, indipendentemente dall’effetto di stateanx su entrambe le variabili, basta che eseguiamo l’analisi di regressione sulle componenti di bdi e bfneur che sono linearmente indipendenti da stateanx. Ovvero, dobbiamo calcolare il coefficiente \\(b\\) del seguente modello di regressione:Il coefficiente di regressione del modello precedenteè identico al coefficiente parziale di regressione associato alla variabile bfneur nel modello di regressione multipla:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]Ciò chiarisce il significato di “controllo statistico” e fa capire qual è la differenza tra il coefficiente di regressione del modello bivariato e il coefficiente parziale di regressione nel modello di regressione multipla.","code":"\nm <- lm(bdi ~ bfneur + stateanx, data = epi.bfi)\nout <- coef(m)\nround(out, 3)\n#> (Intercept)      bfneur    stateanx \n#>      -8.039       0.054       0.252\nm1 <- lm(bdi ~ stateanx, data = epi.bfi)\nhead(m1$res)\n#>          1          2          3          4          5          6 \n#> -0.3151358  0.1743948 -4.0501540  1.1743948 -7.0913093 -1.2133308\nm2 <- lm(bfneur ~ stateanx, data = epi.bfi)\nhead(m2$res)\n#>         1         2         3         4         5         6 \n#> -19.12342  27.87881 -24.12070  25.87881 -29.11785 -32.12144\nm3 <- lm(m1$res ~ m2$res)\ncoef(m3)[2]\n#>     m2$res \n#> 0.05441396\nout[2]\n#>     bfneur \n#> 0.05441396"},{"path":"una-breve-introduzione-al-modello-di-regressione.html","id":"interazione-tra-i-regressone","chapter":"Capitolo 16 Una breve introduzione al modello di regressione","heading":"16.6.2 Interazione tra i regressone","text":"Esaminiamo ora l’ultima possibilità, ovvero quella di un modello di regressione multipla che contiene due variabili indipendenti che “interagiscono” tra loro.\nCosa significa il concetto di “interazione” tra due variabili indipendenti?Per rispondere questa domanda iniziamo considerare un modello senza interazione tra \\(x_1\\) e \\(x_2\\), ovvero il modello di regressione multipla con due regressori continui che abbiamo descritto precedenza. Esaminiamo una rappresentazione geometrica delle predizioni di tale modello.Per esaminare le predizioni del modello\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon.\n\\end{equation}\\]non possiamo procedere come abbiamo fatto precedenza, quando avevamo due gruppi di osservazioni, ovvero, non possiamo disegnare una diversa retta di regressione per ciascun gruppo (non essendoci dei gruppi separati). Possiamo invece usare una funzione R come plot_model() che calcola la retta di regressione di bfneur su bdi selezionando alcuni valori “fissi” della variabile stateanx – nello specifico, la media di stateanx, la media meno una deviazione standard, e la media più una deviazione standard. Si produce così la rappresentazione di tre rette di regressione.Quello che notiamo dalla figura precedente è che le tre rette sono parallele. Un modello di regressione multipla che non include alcun termine di interazione è infatti un modello additivo: per quale che sia il livello della variabile stateanx, l’effetto di bfneur (ovvero, la pendenza della retta di regressione) non cambia. Quindi, l’effetto di stateanx semplicemente si somma ’effetto di bfneur.Una situazione ben diversa si ottiene nel caso di un modello con interazione, come il seguente:\\[\\begin{equation}\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\cdot x_2 + \\varepsilon.\n\\end{equation}\\]Adattiamo questo modello ai datie esaminiamo la souluzione ottenuta:Se rappresentiamo graficamente le predizioni del modellovediamo che, ora, le rette di regressione di bfneur su bdi corrispondenza dei tre valori prescelti di stateanx non sono più parallele. Questo significa che l’effetto di bfneur su bdi (la pendenza della retta di regressione) dipende dal valore di stateanx. Questo è il significato di “interazione”: bfneur ha un effetto diverso su bdi seconda del livello di stateanx.Nel caso presente, vediamo dalla figura che il Neuroticismo ha un effetto più grande sul livello di depressione quando consideriamo coloro che hanno livelli di ansia di stato elevati.coefficienti si interpretano come precedenza. L’intercetta corrisponde al valore atteso di bdi quando bfneur e stateanx hanno il valore di zero. Questo è di poco interesse. Per cui trasformiamo dati modo da utilizzare variabili indipendenti “centrate,” ovvero variabili da cui abbiamo sottratto la media.Riadattiamo il modello con le variabili centrate:Si vede che il coefficiente associato al termine di interazione è rimasto immutato. Ma ora è più facile interpretare il coefficiente dell’intercetta. questo caso, l’intercetta corrisponde al valore atteso di bdi quando bfneur e stateanx assumono il loro valore medio (ovvero zero, per le variabili trasformate). Per quanto riguarda gli effetti separati di bfneur e stateanx, questi non sono interpretabili, quanto la presenza di un’interazione significa, appunto, che bfneur e stateanx non hanno effetti separati su bdi.","code":"\nplot_model(m, type = \"pred\", terms = c(\"bfneur\", \"stateanx\"))\nm4 <- lm(bdi ~ bfneur * stateanx, data = epi.bfi)\nsummary(m4)\n#> \n#> Call:\n#> lm(formula = bdi ~ bfneur * stateanx, data = epi.bfi)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3273 -2.7107 -0.4746  1.9646 14.3104 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)      0.768518   3.763671   0.204   0.8384  \n#> bfneur          -0.040827   0.040952  -0.997   0.3199  \n#> stateanx         0.012673   0.100598   0.126   0.8999  \n#> bfneur:stateanx  0.002501   0.001008   2.483   0.0138 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.416 on 227 degrees of freedom\n#> Multiple R-squared:  0.4229, Adjusted R-squared:  0.4152 \n#> F-statistic: 55.44 on 3 and 227 DF,  p-value: < 2.2e-16\nplot_model(m4, type = \"pred\", terms = c(\"bfneur\", \"stateanx\"))\nepi.bfi$neur_c <- epi.bfi$bfneur - mean(epi.bfi$bfneur)\nepi.bfi$stateanx_c <- epi.bfi$stateanx - mean(epi.bfi$stateanx)\nm5 <- lm(bdi ~ neur_c * stateanx_c, data = epi.bfi)\nsummary(m5)\n#> \n#> Call:\n#> lm(formula = bdi ~ neur_c * stateanx_c, data = epi.bfi)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3273 -2.7107 -0.4746  1.9646 14.3104 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       6.450781   0.319275  20.204  < 2e-16 ***\n#> neur_c            0.058853   0.014445   4.074 6.38e-05 ***\n#> stateanx_c        0.232727   0.030117   7.727 3.51e-13 ***\n#> neur_c:stateanx_c 0.002501   0.001008   2.483   0.0138 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.416 on 227 degrees of freedom\n#> Multiple R-squared:  0.4229, Adjusted R-squared:  0.4152 \n#> F-statistic: 55.44 on 3 and 227 DF,  p-value: < 2.2e-16"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"il-modello-statistico-della-regressione-lineare","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"Capitolo 17 Il modello statistico della regressione lineare","text":"Lo scopo della ricerca è trovare le associazioni tra le variabili e fare\nconfronti fra le condizioni sperimentali. Nel caso della psicologia, il\nricercatore vuole scoprire le leggi generali che descrivono le relazioni\ntra costrutti psicologici e le relazioni che intercorrono tra \nfenomeni psicologici e quelli non psicologici (sociali, economici,\nstorici, …). Abbiamo già visto come la correlazione di Pearson sia uno\nstrumento adatto questo scopo. Infatti, essa ci informa sulla\ndirezione e sull’intensità della relazione lineare tra due variabili.\nTuttavia, la correlazione non è sufficiente, quanto il ricercatore ha\ndisposizione solo dati di un campione, mentre vorrebbe descrivere la\nrelazione tra le variabili nella popolazione. causa della variabilità\ncampionaria, le proprietà dei campioni sono necessariamente diverse da\nquelle della popolazione: ciò che si può osservare nella popolazione\npotrebbe non emergere nel campione e, al contrario, il campione\nmanifesta caratteristiche che non sono necessariamente presenti nella\npopolazione. È dunque necessario chiarire, dal punto di vista\nstatistico, il legame che intercorre tra le proprietà del campione e le\nproprietà della popolazione da cui esso è stato estratto. Come nel caso\ndella media, anche questo caso dovrà essere costruita la\ndistribuzione di una statistica; ma però, nel caso presente, la\nstatistica di interesse sarà costruita utilizzando, non dati di una\nsola variabile, ma bensì dati che descrivono l’andamento congiunto di\ndue variabili.Il modello di regressione utilizza la funzione matematica più semplice\nper descrivere la relazione fra due variabili, ovvero la funzione\nlineare. Inizieremo descrivere le proprietà geometriche della funzione lineare per poi utilizzare questa semplice funzione per costruire il modello statistico della regressione lineare.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"la-funzione-lineare","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.1 La funzione lineare","text":"Si chiama funzione lineare una funzione del tipo\\[\nf(x) = + b x,\n\\]\ndove \\(\\) e \\(b\\) sono delle costanti.\nIl grafico di tale funzione è una retta di cui il parametro \\(b\\) è detto coefficiente angolare e il parametro \\(\\) è detto intercetta con l’asse delle \\(y\\) (infatti, la retta interseca l’asse \\(y\\) nel punto \\((0,)\\), se \\(b \\neq 0\\)).Per assegnare un’interpretazione geometrica alle costanti \\(\\) e \\(b\\) si consideri la funzione\\[\ny = b x.\n\\]\nTale funzione rappresenta un caso particolare, ovvero quello della proporzionalità diretta tra \\(x\\) e \\(y\\). Il caso generale della linearità\\[\ny = + b x\n\\]\nnon fa altro che sommare una costante \\(\\) ciascuno dei valori \\(y = b x\\). Nella funzione lineare \\(y = + b x\\), se \\(b\\) è positivo allora \\(y\\) aumenta al crescere di \\(x\\); se \\(b\\) è negativo allora \\(y\\) diminuisce al crescere di \\(x\\); se \\(b=0\\) la retta è orizzontale, ovvero \\(y\\) non muta al variare di \\(x\\).Consideriamo ora il coefficiente \\(b\\). Si consideri un punto \\(x_0\\) e un incremento arbitrario \\(\\varepsilon\\) come indicato nella figura 17.1. Le differenze \\(\\Delta x = (x_0 + \\varepsilon) - x_0\\) e \\(\\Delta y = f(x_0 + \\varepsilon) - f(x_0)\\) sono detti incrementi di \\(x\\) e \\(y\\). Il coefficiente angolare \\(b\\) è uguale al rapporto\\[\n    b = \\frac{\\Delta y}{\\Delta x} = \\frac{f(x_0 + \\varepsilon) - f(x_0)}{(x_0 + \\varepsilon) - x_0},\n\\]\nindipendentemente dalla grandezza degli incrementi \\(\\Delta x\\) e \\(\\Delta y\\). Il modo più semplice per assegnare un’interpretazione geometrica al coefficiente angolare (o pendenza) della retta è dunque quello di porre \\(\\Delta x = 1\\). tali circostanze infatti \\(b = \\Delta y\\).\nFigura 17.1: La funzione lineare \\(y = + bx\\).\n","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"lerrore-di-misurazione","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.2 L’errore di misurazione","text":"Per descrivere l’associazione tra due variabili, tuttavia, la funzione lineare non è sufficiente. Nel mondo empirico, infatti, la relazione tra variabili non è mai perfettamente lineare. È dunque necessario includere nel modello di regressione anche una componente d’errore, ovvero una componente della \\(y\\) che non può essere spiegata dal modello lineare. Nel caso di due sole variabili, questo ci conduce alla seguente formulazione del modello di regressione:\\[\\begin{equation}\ny = \\alpha + \\beta x + \\varepsilon,\n\\tag{17.1}\n\\end{equation}\\]laddove parametri \\(\\alpha\\) e \\(\\beta\\) descrivono l’associazione tra le variabili aleatorie \\(y\\) e \\(x\\) (nella popolazione), e il termine d’errore \\(\\varepsilon\\) specifica quant’è grande la porzione della variabile \\(y\\) che non può essere predetta nei termini di una relazione lineare con la \\(x\\).Si noti che l’eq. (17.1) ci consente di formulare una predizione, nei termini di un modello lineare, del valore atteso della \\(y\\) conoscendo \\(x\\), ovvero\\[\\begin{equation}\n\\hat{y} = \\mathbb{E}(y \\mid x) = \\alpha + \\beta x.\n\\tag{17.2}\n\\end{equation}\\]altri termini, se parametri del modello (\\(\\alpha\\) e \\(\\beta\\)) sono noti, allora è possibile predire \\(y\\) sulla base della nostra conoscenza della \\(x\\).Per esempio, se conosciamo la relazione lineare tra quoziente di intelligenza ed aspettativa di vita, allora possiamo prevedere quanto lungo vivrà una persona sulla base del suo QI. Sì, c’è una relazione lineare tra intelligenza e aspettativa di vita! – per una discussione, si veda, ad esempio, l’articolo di David Z. Hambrick pubblicato su Scientific American il 22 dicembre 2015. Ma quando sarà accurata la nostra previsione? Ciò dipende dal termine d’errore dell’eq. (17.1). L’analisi di regressione ci fornisce un metodo per rispondere domande di questo tipo.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"scopi-della-regressione-lineare","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.3 Scopi della regressione lineare","text":"Il modello di regressione lineare si pone tre obiettivi:descrivere l’associazione tra le variabili \\(x\\) e \\(y\\) nel campione esaminato;misurare la bontà dell’adattamento dell’associazione tra \\(x\\) e \\(y\\);fare inferenze sull’associazione tra \\(x\\) e \\(y\\) nella popolazione da cui il campione è stato estratto.Il primo obiettivo intende rispondere alla stessa domanda cui risponde il coefficiente di correlazione: quali sono l’intensità e il segno della relazione lineare che descrive l’associazione tra due variabili? Vedremo che c’è una precisa relazione tra il coefficiente \\(b\\) del modello di regressione (che rappresenta la pendenza della retta di regressione) e\nil coefficiente di correlazione \\(r\\) di Pearson: il coefficiente di\ncorrelazione non è altro che la pendenza della retta di regressione\nquando dati sono standardizzati. Vi è però un’importante differenza\ntra la correlazione ed il modello di regressione. La correlazione è un\nindicatore simmetrico di associazione tra due caratteri. Il modello di\nregressione, invece, si chiede come varia una variabile, detta\ndipendente e solitamente denotata con \\(y\\), al variare di un’altra\nvariabile, detta indipendente (o predittore), solitamente denotata con\n\\(x\\). L’analisi della regressione lineare si pone dunque il problema di\nstudiare la relazione asimmetrica tra due variabili.Il secondo obiettivo del modello di regressione lineare si chiede se il\nmodello di regressione sia sensato per descrivere l’associazione\nosservata tra le due variabili. Vogliamo trovare un indice che descriva\nquanto distanti sono dati dalla retta di regressione. Se punti di un\ndiagramma dispersione sono molto vicini alla retta di regressione,\nallora il modello di regressione è adeguato per descrivere\nl’associazione tra le due variabili. questo caso la bontà di\nadattamento del modello ai dati è grande. Oppure può succedere che \npunti di un diagramma dispersione siano molto lontani alla retta di\nregressione e/o che la retta di regressione sia piatta. questi due\nultimi casi non vi è evidenza di una associazione lineare tra le due\nvariabili e l’indice che misura la bontà dell’adattamento\ndell’associazione tra \\(x\\) e \\(y\\) assume un valore basso e prossimo allo\nzero. Tale indice va sotto il nome di coefficiente di determinazione.Il terzo obiettivo è quello più ambizioso: ci chiediamo quale potrebbe\nessere l’associazione tra le variabili \\(x\\) e \\(y\\) nella popolazione, alla\nluce delle informazioni che sono state osservate nel campione. Quello che vorremmo conoscere è \\(p(\\theta \\mid \\text{dati})\\), laddove \\(\\theta\\) è il parametro sconosciuto che rappresenta la pendenza della retta di regressione nella popolazione. Vedremo come l’approccio Bayesiano può essere usato per rispondere questa domanda.Il modello di regressione è, probabilmente, il più importante dei modelli statistici. Noi qui ne\nesamineremo solo le sue caratteristiche di base. Ma tale modello può\nessere esteso modo tale da includere più di un predittore (nel qual\ncaso si parla di modello di regressione multipla), oppure una variabile\ndipendente qualitativa (il che produce il modello di regressione\nlogistica), oppure molteplici variabili dipendenti continue (il che\nproduce il modello di regressione multivariato). Sviluppi più moderni di\nquesto modello considerano inoltre il caso della violazione\ndell’assunzione di indipendenza tra le osservazioni, il che conduce alla\ncostruzione dei modelli ad effetti misti (mixed-effects models).\nInfine, uno sviluppo importante del modello di regressione lineare è\nl’analisi fattoriale, nel qual caso viene ipotizzata l’esistenza di\nvariabili indipendenti inosservabili (latenti), le quali corrispondono\nai costrutti psicologici. Il modello fattoriale, così formulato,\ncostituisce il fondamento della Psicometria, ovvero di quelle tecniche\nstatistiche che stanno alla base della costruzione e della validazione\ndei reattivi psicologici.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"quantificare-lassociazione-fra-due-caratteri-quantitativi","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.4 Quantificare l’associazione fra due caratteri quantitativi","text":"Consideriamo tre variabili aleatorie \\(X\\), \\(Y\\) ed \\(\\varepsilon\\) legate dalla relazione lineare\\[\ny = \\alpha + \\beta x + \\varepsilon,\n\\]dove \\(\\alpha\\) e \\(\\beta\\) sono numeri reali e \\(\\mathbb{E}(\\varepsilon) = 0\\). Chiameremo modello lineare (semplice) la relazione dell’eq. (17.1) e chiameremo retta di regressione la retta\\[\\begin{equation}\ny = \\alpha + \\beta x.\n\\tag{17.3}\n\\end{equation}\\]Il parametro \\(\\alpha\\) è l’ordinata ’origine (o intercetta) mentre il parametro \\(\\beta\\) è il coefficiente angolare della retta. Possiamo interpretare l’eq. (17.1) pensando che le variabili aleatorie \\(x\\) ed \\(y\\) siano legate tra loro da una relazione lineare perturbata da un errore casuale \\(\\varepsilon\\).Dato un insieme di realizzazioni campionarie delle variabili aleatorie \\(x\\) e \\(y\\), ci poniamo lo scopo di determinare la retta di regressione campionaria\\[\\begin{equation}\n\\hat{y}_i  =  + b x_i\n\\tag{17.4}\n\\end{equation}\\]che approssima il meglio possibile la distribuzione dei punti \\(x_i\\), \\(y_i\\), \\(= 1, \\dots, n\\). Lo studio di questo problema è detto regressione lineare.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"stime-dei-minimi-quadrati","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.5 Stime dei minimi quadrati","text":"Il primo obiettivo dell’analisi di regressione è quello di trovare la retta che meglio descrive l’andamento dei dati osservati un campione. Iniziamo con il definire residui \\(e_i\\) tramite la relazione\\[\\begin{equation}\ne_i  = y_i - (+ b x_i).\n\\tag{17.5}\n\\end{equation}\\]altri termini, il residuo \\(\\)-esimo è la differenza fra l’ordinata del punto (\\(x_i\\), \\(y_i\\)) e quella del punto di ascissa \\(x_i\\) sulla retta di regressione campionaria.Per determinare coefficienti \\(\\) e \\(b\\) della retta (17.4) non è sufficiente minimizzare la somma dei residui \\(\\sum_{=1}^{n}e_i\\), quanto residui possono essere sia positivi che negativi e la loro somma può essere molto prossima allo zero anche per differenze molto grandi tra valori osservati e la retta\ndi regressione. Infatti, ciascuna retta passante per il punto (\\(\\bar{x}, \\bar{y}\\)) ha \\(\\sum_{=1}^{n}e_i=0\\).Dimostrazione. Una retta passante per il punto (\\(\\bar{x}, \\bar{y}\\)) soddisfa l’equazione \\(\\bar{y} = + b \\bar{x}\\). Sottraendo tale equazione dall’equazione \\(y_i = + b x_i + e_i\\) otteniamo\n\\[\ny_i - \\bar{y} =  b (x_i - \\bar{x}) + e_i.\n\\]\nSommando su tutte le osservazioni, si ha che\\[\n\\sum_{=1}^n e_i = \\sum_{=1}^n (y_i - \\bar{y} ) -  b \\sum_{=1}^n (x_i - \\bar{x}) = 0 - b(0) = 0. \n\\]Questo problema viene risolto scegliendo coefficienti \\(\\) e \\(b\\) che\nminimizzano, non tanto la somma dei residui, ma bensì l’errore\nquadratico, cioè la somma dei quadrati degli errori:\\[\nS(, b) = \\sum_{=1}^{n} e_i^2 = \\sum (y_i - - b x_i)^2.\n\\]Il metodo più diretto per determinare quelli che vengono chiamati coefficienti dei minimi quadrati è quello di trovare le derivate parziali della funzione \\(S(, b)\\) rispetto ai coefficienti \\(\\) e \\(b\\):\\[\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial S(,b)}{\\partial } &= \\sum (-1)(2)(y_i - - b x_i), \\notag \\\\\n\\frac{\\partial S(,b)}{\\partial b} &= \\sum (-x_i)(2)(y_i - - b x_i).\n\\end{aligned}\n\\tag{17.6}\n\\end{equation}\\]Ponendo le derivate uguali zero e dividendo entrambi membri per \\(-2\\) si ottengono le equazioni normali\\[\\begin{equation}\n\\begin{aligned}\n + b \\sum x_i &= \\sum y_i, \\notag \\\\\n \\sum x_i + b \\sum x_i^2 &= \\sum x_i y_i. \n\\end{aligned}\n\\tag{17.7}\n\\end{equation}\\]coefficienti dei minimi quadrati \\(\\) e \\(b\\) si trovano risolvendo le equazioni (17.7) e sono uguali :\\[\\begin{equation}\n\\begin{aligned}\n&= \\bar{y} - b \\bar{x},\\\\\nb &= \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}.\n\\end{aligned}\n\\tag{17.8}\n\\end{equation}\\]","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"monotwinsiq","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.5.1 Un esempio concreto","text":"Consideriamo dati relativi 34 coppie di gemelli monozigoti separati alla nascita (Anderson & Finn (2012)). Dei gemelli conosciamo l’ordine della nascita e il quoziente di intelligenza misurato con il Dominoes Intelligence test. Il test è costituito da 48 domande ciascuna delle quali viene assegnato un punto nel caso di risposta corretta. La media del test nella popolazione è di 28 punti, che corrisponde al punteggiodi 100 sulla scala WAIS. dati sono:Un diagramma di dispersione per questi dati, insieme alla retta di regressione dei minimi quadrati, è riportato nella figura 17.2.\nFigura 17.2: Retta di regressione che descrive la relazione lineare tra il quoziente di intelligenza del secondo nato e il quoziente di intelligenza del primo nato.\ncoefficienti di regressione si trovano con le formule dei minimi quadrati. Usando R, per \\(b\\) otteniamoe per \\(\\) otteniamoTali risultati corrispondono ai valori trovati dalla funzione lm() con la seguente sintassi:L’oggetto creato da {lm() può essere visionato utilizzando coef(fm) o con summary(fm).valori predetti dal modello di regressione sono dati dao, maniera equivalente, possono essere trovati con predict(fm)residui di regressione, ovvero la differenza tra il valore osservato e il valore predetto dal modello, si trovano mediante l’istruzioneo, maniera equivalente, con residuals(fm)residui possono essere rappresentanti graficamente come riportato nella figura 17.3.\nFigura 17.3: Residui del modello di regressione che esprime il quoziente di intelligenza del secondo nato funzione del quoziente di intelligenza del primo nato.\n","code":"\niq1 <- c(22, 32, 29, 13, 32, 24, 33, 19, 13, 36, 26, 26, 32, 27, 6, 16, 41, 29, 13, 20, 28, 30, 22, 23, 27, 40, 30, 30, 21, 27, 15, 38, 4, 12)\n\niq2 <- c(12, 28, 35, 4, 18, 33, 26, 9, 22, 34, 17, 20, 33, 28, 10, 28, 40, 30, 10, 24, 22, 34, 23, 21, 25, 38, 25, 26, 27, 24, 9, 27, 2, 9)\n\ndf <- data.frame(iq1, iq2)\np <- df %>% \n  ggplot(aes(x = iq1, y = iq2)) +\n  geom_smooth(method = \"lm\", se=FALSE, color=\"lightgrey\", formula = y ~ x) +\n  geom_point() +\n  labs(\n    x = \"Qi primo nato\",\n    y = \"QI secondo nato\",\n    title = \"Gemelli monozigoti separati alla nascita\",\n    caption = \"(Fonte: Anderson e Finn, 2012)\"\n  )\np\nb <- cov(df$iq1, df$iq2) / var(df$iq1) \nb\n#> [1] 0.8498545\na <- mean(df$iq2) - b * mean(df$iq1)\na\n#> [1] 1.838871\nfm <- lm(iq2 ~ iq1, data = df)\nsummary(fm)\n#> \n#> Call:\n#> lm(formula = iq2 ~ iq1, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -11.0342  -3.8218  -0.5852   3.4658  12.5635 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.8389     3.0269   0.608    0.548    \n#> iq1           0.8499     0.1155   7.357 2.29e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.102 on 32 degrees of freedom\n#> Multiple R-squared:  0.6285, Adjusted R-squared:  0.6169 \n#> F-statistic: 54.13 on 1 and 32 DF,  p-value: 2.288e-08\nyhat <- a + b * df$iq1\npredict(fm) \n#>         1         2         3         4         5         6         7         8         9 \n#> 20.535671 29.034216 26.484652 12.886980 29.034216 22.235380 29.884070 17.986107 12.886980 \n#>        10        11        12        13        14        15        16        17        18 \n#> 32.433634 23.935089 23.935089 29.034216 24.784943  6.937998 15.436543 36.682907 26.484652 \n#>        19        20        21        22        23        24        25        26        27 \n#> 12.886980 18.835962 25.634798 27.334507 20.535671 21.385525 24.784943 35.833052 27.334507 \n#>        28        29        30        31        32        33        34 \n#> 27.334507 19.685816 24.784943 14.586689 34.133343  5.238289 12.037125\ne <- df$iq2 - yhat\nresiduals(fm)\n#>           1           2           3           4           5           6           7 \n#>  -8.5356706  -1.0342160   8.5153476  -8.8869798 -11.0342160  10.7646203  -3.8840705 \n#>           8           9          10          11          12          13          14 \n#>  -8.9861070   9.1130202   1.5663659  -6.9350888  -3.9350888   3.9657840   3.2150567 \n#>          15          16          17          18          19          20          21 \n#>   3.0620019  12.5634566   3.3170932   3.5153476  -2.8869798   5.1640385  -3.6347978 \n#>          22          23          24          25          26          27          28 \n#>   6.6654931   2.4643294  -0.3855252   0.2150567   2.1669478  -2.3345069  -1.3345069 \n#>          29          30          31          32          33          34 \n#>   7.3141839  -0.7849433  -5.5866889  -7.1333432  -3.2382890  -3.0371253\ndf$predicted <- predict(fm)   \ndf$residuals <- residuals(fm) \n\np1 <- df %>% \n  ggplot(aes(x = iq1, y = iq2)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +  \n  geom_segment(aes(xend = iq1, yend = predicted), alpha = .2) + \n  geom_point() +\n  geom_point(aes(y = predicted), shape = 1) + \n  labs(\n    x = \"Qi primo nato\",\n    y = \"QI secondo nato\",\n    title = \"Gemelli monozigoti separati alla nascita\",\n    caption = \"(Fonte: Anderson e Finn, 2012)\"\n  )\np1\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"sec:beta_r","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.5.2 Coefficiente angolare e correlazione di Pearson","text":"Ricordando che \\(r_{xy}=s_{xy} / (s_x s_y)\\) è il coefficiente di\ncorrelazione lineare e che \\(b=s_{xy} /s_x^2\\) è la stima dei minimi\nquadrati del coefficiente angolare della retta di regressione,\nsostituendo \\(r_{xy}s_xs_y\\) al numeratore dell’equazione di \\(b\\) e\nsemplificando, si ottiene\n\\[\\begin{equation}\nb = r_{yx}\\frac{s_y}{s_x}.\n\\end{equation}\\]\nSe dati vengono standardizzati, dunque, l’equazione della retta di regressione\ncampionaria diventa\n\\[\\begin{equation}\nz_{y_i} = r_{xy} z_{x_i} + e_i,\n\\end{equation}\\]\nquanto \\(= \\bar{z}_y - b\\bar{z}_x =0\\) e \\(s_x = s_y = 1\\).Si può dunque assegnare al coefficiente di correlazione di Pearson la seguente\ninterpretazione: \\(r_{xy}\\) è uguale alla pendenza \\(b\\) della retta di\nregressione quando le variabili \\(x\\) e \\(y\\) vengono standardizzate\n(Rodgers & Nicewander, 1988).Facciamo un esempio calcolando coefficienti di regressione sui punteggi standardizzati del\nquoziente di intelligenza dei gemelli monozigoti.Utilizzando valori standardizzati del QI l’intercetta diventa pari zero e la pendenza della retta di regressione diventa uguale alla correlazione tra le due variabili:","code":"\nziq1 <- scale(df$iq1)\nziq2 <- scale(df$iq2)\nfm1 <- lm(ziq2 ~ ziq1)\ncoef(fm1)\n#>  (Intercept)         ziq1 \n#> 1.818206e-16 7.927594e-01\ncor(df$iq1, df$iq2)\n#> [1] 0.7927594"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"regressione-verso-la-media","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.5.3 Regressione verso la media","text":"Il termine regressione fu introdotto da Francis Galton (1822-1911), un\nantropologo che fu, tra le altre cose, promotore dell’eugenetica. Nel\n1886, nell’ambito dei suoi studi sull’ereditarietà dei caratteri, Galton\nraccolse le stature di \\(928\\) figli adulti e dei loro \\(205\\) genitori\n(padri e madri) – dati sono disponibili nel data.frame Galton\ncontenuto nel pacchetto R  HistData. Galton esaminò la relazione tra\nl’altezza media dei figli e l’altezza media dei genitori, che chiamò\n“mid-parent height.” questi dati, genitori e figli hanno la stessa\naltezza media di \\(68.2\\) pollici. Galton osservò però come l’altezza\nmedia dei figli nati da genitori di una data altezza era più simile al\nvalore dell’altezza media della popolazione intera di quanto lo fosse la\nmid-height dei genitori. Ad esempio, per genitori con una mid-height\ncompresa tra \\(70\\) e \\(71\\) pollici, l’altezza media dei figli risultò\nessere di \\(69.5\\) pollici. Nelle parole di Galton, questo corrispondeva\nad una regression toward mediocrity, un concetto che noi oggi\nchiamiamo “regressione verso la media.” Nonostante l’interpretazione\n(errata) di Galton, è importante capire come questo sia un fenomeno\nstatistico, non genetico. Esaminiamo la ragione per cui ciò si verifica.precedenza abbiamo visto come, nel caso di dati standardizzati, la retta di\nregressione campionaria diventa: \\[\\hat{z}_{y_i} = r_{xy} z_{x_i}.\\] Dal\nmomento che \\(r_{xy}\\) è il coefficiente di regressione, esso assume\nvalori compresi tra \\(-1\\) e \\(1\\). Assumiamo che \\(r_{xy}\\) sia positivo e\nminore di \\(1\\) (ovvero, assumiamo che la correlazione tra \\(x\\) e \\(y\\) sia\npositiva ma non perfetta). La formula \\(\\hat{z}_{y_i} = r_{xy} z_{x_i}\\)\nimplica che, se \\(z_{x_i}\\) è positivo, allora il valore predetto\n\\(\\hat{z}_{y_i}\\) dovrà essere minore di \\(z_{x_i}\\). maniera equivalente,\nsi può dire che la ‘distanza’ tra il valore predetto \\(\\hat{y}\\) della\nvariabile di risposta e la media \\(\\bar{y}\\) tenderà ad essere minore della\ndistanza tra \\(x\\) e \\(\\bar{x}\\):\\[\n\\frac{\\hat{y} - \\bar{y}}{s_y} < \\frac{x - \\bar{x}}{s_x}.\n\\]Il termine ‘distanza’ è stato messo tra virgolette quanto è necessario tenere \nconsiderazione l’unità di misura delle variabili. Per fare questo, la\ndistanza tra le osservazioni e il centro della distribuzione viene\nmisurata solo dopo avere standardizzato le variabili – ovvero, viene\nmisurata unità di deviazioni standard.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"punti-influenti-e-valori-anomali","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.5.4 Punti influenti e valori anomali","text":"La soluzione dei minimi quadrati è fortemente influenzata dalla presenza di punti influenti che sono anche delle osservazioni anomale. Un’osservazione anomala è un’osservazione con un residuo elevato (ovvero, avente un valore anomalo di \\(y\\) rispetto alla previsione). Un punto di leva è un punto con un valore anomalo \\(x\\). Un punto influente è un’osservazione che influenza maniera rilevante le stime dei minimi quadrati. Non sempre un punto anomalo è anche un punto influente. Per contro esistono punti non anomali che influiscono notevolmente sulle stime dei minimi quadrati – si veda la Figura 17.4.\nFigura 17.4: Osservazioni anomale e osservazioni influenti.\n","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"bontà-delladattamento","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.6 Bontà dell’adattamento","text":"Il secondo obiettivo dell’analisi della regressione è quello di misurare la bontà di adattamento del modello di regressione ai dati.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"errore-standard-della-stima-1","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.6.1 Errore standard della stima","text":"Un indice assoluto della bontà di adattamento è fornito dalla deviazione\nstandard dei residui, \\(s_e\\), chiamata anche errore standard della\nstima. Uno stimatore non distorto della varianza dei residui nella\npopolazione è dato da\\[\\begin{equation}\ns^2_e = \\frac{\\sum e_i^2}{n-2}\n\\tag{17.9}\n\\end{equation}\\]\ne quindi l’errore standard della stima sarà\n\\[\\begin{equation}\ns_e = \\sqrt{\\frac{\\sum e_i^2}{n-2}}.\n\\tag{17.10}\n\\end{equation}\\]\nDato che \\(s_e\\) è possiede la stessa unità di misura della variabile \\(y\\), l’errore standard della stima può essere considerato come una sorta di “residuo medio.”Consideriamo nuovamente l’esempio dei gemelli monozigoti separati alla\nnascita. L’errore standard della regressioneè simile, anche se non identico, al valore medio dei residuiIn conclusione, se usiamo la retta di regressione per predire il quoziente di intelligenza del gemello nato per secondo partire dal quoziente di intelligenza del gemello nato per primo compiamo, media, un errore di circa 6 punti.","code":"\nsqrt(sum(e^2) / (length(e) - 2)) \n#> [1] 6.101646\nmean(abs(fm$residuals)) \n#> [1] 4.91695"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"indice-di-determinazione","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.6.2 Indice di determinazione","text":"Un importante risultato dei minimi quadrati riguarda la cosiddetta scomposizione della devianza di regressione mediante la quale si definisce l’indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati\ndel campione. Come indicato nella figura 17.5, per una generica osservazione\n\\(x_i, y_i\\), la variazione di \\(y_i\\) rispetto alla media \\(\\bar{y}\\) può essere descritta come la somma di due componenti: il residuo \\(e_i=y_i- \\hat{y}_i\\) e lo scarto di \\(\\hat{y}_i\\) rispetto alla media \\(\\bar{y}\\): \\(y_i - \\bar{y} = (y_i- \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) = e_i + (\\hat{y}_i - \\bar{y})\\).\nFigura 17.5: Scomposizione della devianza.\nSe consideriamo tutte le osservazioni, la devianza delle \\(y\\) può essere scomposta nel seguente modo:\\[\n\\begin{aligned}\n \\sum (y_i - \\bar{y})^2 &= \\sum \\left[ e_i + (\\hat{y}_i - \\bar{y})\n \\right]^2 \n = \\sum e_i^2 + \\sum (\\hat{y}_i - \\bar{y})^2 + 2 \\sum e_i (\\hat{y}_i -\n \\bar{y}) \\notag\\end{aligned}\n \\]Per vincoli imposti sui residui dalle equazioni normali, il doppio prodotto si annulla, infatti\\[\n\\begin{aligned}\n\\sum e_i (\\hat{y}_i - \\bar{y}) &= \\sum e_i \\hat{y}_i - \\bar{y}\\sum e_i = \\sum e_i (+ b x_i) \\notag \\\\\n&= \\sum e_i + b \\sum e_i x_i = 0 \\notag\\end{aligned}\n\\]Di conseguenza, possiamo concludere che la devianza totale (\\(\\dev_T\\)) si scompone nella somma della devianza di dispersione (\\(dev_E\\)) e della devianza di regressione (\\(\\dev_T\\)):\\[\n\\begin{aligned}\n\\underbrace{\\sum_{=1}^n (y_i - \\bar{y})^2}_{\\tiny{\\text{Devianza\ntotale}}} &= \\underbrace{\\sum_{=1}^n e_i^2}_{\\tiny{\\text{Devianza\ndi dispersione}}} + \\underbrace{\\sum_{=1}^n  (\\hat{y}_i -\n\\bar{y})^2}_{\\tiny{\\text{Devianza di regressione}}} \\notag\n\\end{aligned}\n\\]La devianza di regressione, \\(dev_R \\triangleq dev_T - dev_E\\), indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto \\(dev_T/dev_T\\), detto indice di determinazione, esprime tale riduzione degli errori termini proporzionali e definisce il coefficiente di correlazione al quadrato:\\[\\begin{equation}\nr^2 \\triangleq \\frac{dev_R}{dev_T} = 1 - \\frac{dev_E}{dev_T}.\n\\tag{17.11}\n\\end{equation}\\]Quando l’insieme di tutte le deviazioni della \\(y\\) dalla media è spiegato dall’insieme di tutte le deviazioni della variabile teorica \\(\\hat{y}\\) dalla media, si ha che l’adattamento (o accostamento) del modello al campione di dati è perfetto, la devianza residua è nulla ed \\(r^2 = 1\\); nel caso opposto, la variabilità totale coincide con quella residua, per cui \\(r^2 = 0\\). Tra questi due estremi, \\(r\\) indica l’intensità della relazione lineare tra le due variabili e \\(r^2\\), con \\(0 \\leq r^2 \\leq 1\\), esprime la porzione della devianza totale della \\(y\\) che è spiegata dalla regressione lineare sulla \\(x\\).Per dati dei gemelli monozitoti separati alla nascita, la devianza totale si scompone nelle componenti di “devianza spiegata” e “devianza non spiegata” nel modo seguente:le quali assumono valori, rispettivamente, pari \\(3206.618\\), \\(2015.255\\) e \\(1191.363\\). Ne segue che il coefficiente di determinazione è dev_r / dev_t = 0.628, ovvero 1 - dev_e / dev_t = 0.628. Questo risultato coincide con quello trovato con lm():Possiamo quindi concludere che, nel caso del campione esaminato, fattori genetici spiegano circa il 63% della varianza del quoziente di intelligenza dei gemelli monozigoti (quando prevediamo il QI dei secondi nati dal QI dei primi nati).","code":"\ndev_t <- sum((df$iq2 - mean(df$iq2))^2) \ndev_r <- sum((yhat - mean(df$iq2))^2)\ndev_e <- sum((df$iq2 - yhat)^2)\nsummary(fm)$r.squared\n#> [1] 0.6284675"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"inferenza-sullassociazione-tra-x-e-y-nella-popolazione","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.7 Inferenza sull’associazione tra \\(x\\) e \\(y\\) nella popolazione","text":"Il terzo obiettivo dell’analisi di regressione è quello di fare inferenze sull’associazione tra le due variabili nella popolazione da cui il campione deriva. Ci chiediamo se l’associazione osservata nel campione rifletta le proprietà della popolazione oppure sia imputabile agli errori di campionamento.Se si segue la scuola frequentista, nella regressione bivariata il\nproblema dell’inferenza statistica è basato sulla stessa logica seguita\nnel caso di una singola variabile aleatoria. Nell’inferenza su una\nmedia, per esempio, viene valutata l’ipotesi nulla \\(H_0: \\mu=0\\) e il\nparametro di interesse, la media \\(\\mu\\) della popolazione, viene stimato\nmediante un’opportuna statistica, ovvero la media campionaria \\(\\bar{y}\\).\nLe inferenze statistiche sono basate sulla conoscenza delle proprietà\ndella distribuzione della statistica campionaria \\(\\bar{y}\\).È possibile però anche definire degli stimatori che dipendono da due (o\npiù) caratteri. Per esempio, il coefficiente \\(b\\) della retta di\nregressione campionaria, che viene usato quale stimatore del\ncoefficiente angolare \\(\\beta\\) della funzione di regressione nella\npopolazione \\(y = \\alpha + \\beta x + \\varepsilon\\), è definito rispetto \ndue caratteri, \\(x\\) e \\(y\\). Per ciascun campione casuale di \\(n\\)\nosservazioni \\(x, y\\), lo stimatore \\(b\\) di \\(\\beta\\) assume un diverso\nvalore (\\(b\\) è una variabile aleatoria). L’insieme delle stime \\(b\\) di\n\\(\\beta\\) nell’universo dei campioni di ampiezza \\(n\\) costituisce la\ndistribuzione campionaria di \\(b\\). Analogamente si può dire dello\nstimatore \\(\\) di \\(\\alpha\\). Il problema che ci poniamo ora è appunto\nquello di descrivere le proprietà delle distribuzioni campionarie dei\ndue stimatori dei minimi quadrati \\(\\) e \\(b\\). Per fare questo, dobbiamo\nperò prima introdurre il modello statistico della regressione lineare.","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"modello-statistico-di-regressione-lineare","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.7.1 Modello statistico di regressione lineare","text":"corrispondenza di ciascun valore della variabile \\(x\\), che si\nipotizza essere costante da campione campione, corrisponde nella\npopolazione una distribuzione di valori \\(y\\). Ci chiediamo che relazione\nintercorra tra le medie condizionali \\(\\bar{y}_i \\mid x_i\\) e la variabile\n\\(x\\). Se disponiamo di un campione di ciascuna distribuzione condizionata\n\\(y_i \\mid x_i\\), allora possiamo calcolare la media condizionale nel\ncampione per stimare la corrispondente media nella popolazione. Una tale\nsituazione si può verificare un contesto sperimentale, cui,\nmantenendo fissi valori del carattere \\(x\\), la ripetizione delle prove\nproduce un campione del carattere \\(y\\) subordinatamente ad ogni \\(x\\).\nNel caso di dati di tipo osservazionale, invece, vengono osservate\ncoppie di valori (\\(x_i, y_i\\)), con \\(=1, \\dots, n\\), e per ogni valore\n\\(x\\) si ha disposizione un unico valore \\(y\\).Allo scopo di attenuare le conseguenze derivanti dalle limitazioni di\ncui soffrono dati disposizione, si definisce il modello statistico\ndi regressione lineare introducendo nell’analisi delle ipotesi sulla\npopolazione. Il modello statistico di regressione è basato sulle quattro seguenti\nipotesi proposito della struttura della popolazione.La funzione di regressione è lineare (linearità):\n\\[\n\\mathbb{E}(y_i \\mid x_1, \\dots, x_n) = \\alpha + \\beta x_i, \\quad\n=1, \\dots, n,\n\\]\novvero, le medie delle distribuzioni condizionali \\(y \\mid x_i\\) sono linearmente associate alla variabile esplicativa x.La funzione di regressione è lineare (linearità):\n\\[\n\\mathbb{E}(y_i \\mid x_1, \\dots, x_n) = \\alpha + \\beta x_i, \\quad\n=1, \\dots, n,\n\\]\novvero, le medie delle distribuzioni condizionali \\(y \\mid x_i\\) sono linearmente associate alla variabile esplicativa x.Le varianze delle distribuzioni condizionali \\(y \\mid x_i\\) sono costanti al variare della \\(x\\) (omoschedasticità):\n\\[\nvar(y_i \\mid x_1, \\dots,  x_n) = \\sigma^2, \\quad =1,\n\\dots, n.\n\\]Le varianze delle distribuzioni condizionali \\(y \\mid x_i\\) sono costanti al variare della \\(x\\) (omoschedasticità):\n\\[\nvar(y_i \\mid x_1, \\dots,  x_n) = \\sigma^2, \\quad =1,\n\\dots, n.\n\\]Le osservazioni \\(y_i\\) sono tra loro incorrelate subordinatamente alle \\(x_i\\) (indipendenza):\n\\[\ncov(y_i, y_j \\mid x_1, \\dots, x_n) = 0, \\quad per \\hskip.1 \\neq j,\n\\]\novvero, l’osservazione \\(y_i\\) è selezionata dalla distribuzione condizionale \\(y_i \\mid x_i\\) tramite un campionamento casuale indipendente.Le osservazioni \\(y_i\\) sono tra loro incorrelate subordinatamente alle \\(x_i\\) (indipendenza):\n\\[\ncov(y_i, y_j \\mid x_1, \\dots, x_n) = 0, \\quad per \\hskip.1 \\neq j,\n\\]\novvero, l’osservazione \\(y_i\\) è selezionata dalla distribuzione condizionale \\(y_i \\mid x_i\\) tramite un campionamento casuale indipendente.La distribuzione di \\(y_i\\) subordinata \\(X=x_i\\) segue la distribuzione gaussiana (normalità):\n\\[\n(y_i \\mid x_i) \\sim \\mathcal{N}(\\alpha+\\beta x_i, \\sigma^2).\n\\]La distribuzione di \\(y_i\\) subordinata \\(X=x_i\\) segue la distribuzione gaussiana (normalità):\n\\[\n(y_i \\mid x_i) \\sim \\mathcal{N}(\\alpha+\\beta x_i, \\sigma^2).\n\\]","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"proprietà-degli-stimatori-dei-minimi-quadrati","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.7.2 Proprietà degli stimatori dei minimi quadrati","text":"Può essere dimostrato (vedi Appendici) che, se le assunzioni del modello lineare sono soddisfatte, allora coefficienti dei minimi quadrati avranno le seguenti proprietà:\\[\\begin{equation}\n\\begin{aligned}\nb &\\sim \\mathcal{N}\\bigg(\\beta,  \\frac{\\sigma^2_{\\varepsilon}}{\\sum(x_i-\\bar{x})^2}\\bigg),\\\\\n&\\sim \\mathcal{N}\\bigg(\\alpha, \\frac{\\sigma^2_{\\varepsilon}\\textstyle\\sum x_i^2}{n \\textstyle\\sum (x_i-\\bar{x})^2} \\bigg).\n\\end{aligned}\n\\tag{17.12}\n\\end{equation}\\]","code":""},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"le-inferenze-sul-modello-di-regressione","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"17.7.3 Le inferenze sul modello di regressione","text":"L’inferenza statistica sul modello di regressione può essere svolta modi diversi. Esamineremo qui l’approccio frequentista per affrontare seguito l’approccio Bayesiano.L’inferenza statistica frequentista si articola nella formulazione degli intervalli di confidenza per parametri di interesse e nei test di significatività statistica.\nUn’ipotesi che viene frequentemente sottoposta verifica è quella di significatività, cioè l’ipotesi che alla variabile esplicativa sia associato un coefficiente nullo. tal caso, l’ipotesi nulla è\n\\[\nH_0:\\beta=0\n\\]\ne l’ipotesi alternativa è\n\\[\nH_1:\\beta \\neq 0.\n\\]\nSotto l’ipotesi nulla \\(H_0: \\beta = 0\\) la statistica\n\\[\nt_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}}\n\\]\nsi distribuisce come una variabile aleatoria \\(t\\) di Student con \\(n-2\\) gradi\ndi libertà.Di fronte al problema di decidere se il valore stimato \\(\\hat{\\beta}\\) sia\nsufficientemente ‘distante’ da zero, modo da respingere l’ipotesi\nnulla che il vero valore \\(\\beta\\) sia nullo, non è sufficiente basarsi\nsoltanto sul valore numerico assunto da \\(\\hat{\\beta}\\), ma occorre tener\nconto della variabilità campionaria. La statistica ottenuta dividendo\n\\(\\hat{\\beta}\\) per la stima del suo errore standard, \\(s_{\\hat{\\beta}}\\),\nci permette di utilizzare la distribuzione \\(t\\) di Student come metrica\nper stabilire se la stima trovata si debba considerare ‘diversa’ da\nquanto ipotizzato sotto \\(H_0\\).L’ipotesi nulla viene rifiutata quando il valore assoluto del rapporto è\nesterno alla regione di accettazione, cui limiti sono definiti dai\nvalori critici della distribuzione \\(t\\) di Student con \\(n - 2\\) gradi di\nlibertà per il livello di significatività \\(\\alpha\\) prescelto. Se\nl’ipotesi nulla viene rifiutata si dice che il coefficiente\n\\(\\hat{\\beta}\\) è “statisticamente significativo” ammettendo così la\npossibilità di descrivere con un modello lineare la relazione esistente\ntra le variabili \\(x\\) e \\(y\\). Quando non si può rifiutare l’ipotesi nulla\nnel modello di regressione, si conclude che il coefficiente angolare\ndella retta non risulta significativamente diverso da zero, individuando\ncosì nella popolazione una retta parallela ’asse delle ascisse.Il valore-\\(p\\) esprime la probabilità di ottenere un valore del test\nuguale o superiore quello ottenuto nel campione esaminato, utilizzando\nla distribuzione campionaria del test sotto l’ipotesi nulla. Se\n\\(t_{\\hat{\\beta}}\\) è il valore osservato del rapporto \\(t\\) per il\ncoefficiente angolare della retta di regressione, allora il \\(p\\)-valore è\ndato da \\[p = 2 \\times Pr(t \\geq |t_{\\hat{\\beta}}|),\\] dove \\(t\\) è il\nvalore di una variabile aleatoria \\(t\\) di Student con \\((n-2)\\) gradi di\nlibertà.Ogni volta che il \\(p\\)-valore del test è inferiore al livello di\nsignificatività che si è scelto per \\(H_0\\), il test porta al rifiuto\ndell’ipotesi nulla. Solitamente si sceglie un livello \\(\\alpha\\) pari \n0.05 o 0.01.Consideriamo nuovamente la regressione del QI del secondo nato sul QI del primo nato nei gemelli monozigoti esaminati da Anderson & Finn (2012). Dall’output prodotto dalla funzione lm() possiamo ricavare le informazioni per il calcolo della statistica \\(t\\):che risulta essere\\[\nt = \\frac{B}{s_{\\hat{\\beta}}}=\\frac{0.8499}{0.1155} = 7.357.\n\\]\nSupponendo un’ipotesi alternativa bidirezionale, \\(H_1: \\beta \\neq 0\\), la regione critica sarà suddivisa nelle due code della distribuzione \\(t\\) di Student con \\(25\\) gradi di libertà. Essendo il valore critico \\(t_{n-2, 1-\\alpha/2}\\) pari asi può rifiutare \\(H_0\\).maniera corrispondente, possiamo considerare il \\(p\\)-valore. Il \\(p\\)-valore è l’area sottesa alla funzione di densità \\(t\\) di Student con \\(n-2=32\\) gradi di libertà nei due intervalli \\([-\\infty, -t_{\\hat{\\beta}}]\\) e \\([t_{\\hat{\\beta}}, \\infty]\\) èDato che il \\(p\\)-valore è minore di \\(\\alpha = 0.05\\), l’approccio frequentista conclude rigettando \\(H_0\\). Il risultato si può riportare nel modo seguente:L’analisi della regressione bivariata ha rivelato una relazione lineare positiva tra il QI dei gemelli monozigoti primi nati e il QI dei gemelli secondi nati, \\(\\hat{\\beta} = 0.85\\), \\(t_{32} = 7.36\\), \\(p = .0001\\).test di significatività possono essere eseguiti con R  utilizzando la funzione summary() applicata ’oggetto creato dal lm(): Il test statistico sul parametro \\(\\beta\\) del modello di regressione verifica l’ipotesi nulla di indipendenza, ovvero l’ipotesi che, nella popolazione, la pendenza della retta di regressione sia uguale zero.Più informativo del test statistico \\(H_0: \\beta=0\\) è l’intervallo di confidenza per il parametro \\(\\beta\\):\n\\[\n\\hat{\\beta} \\pm t_{\\alpha/2} s_{\\hat{\\beta}}.\n\\]\nNel caso presente, abbiamoDato che il limite inferiore dell’intervallo di confidenza è superiore allo zero, possiamo concludere che vi è un’associazione (lineare) positiva tra il QI del primo nato e il QI del secondo nato, nelle coppie di gemelli monozigoti che sono state esaminate da Anderson & Finn (2012).","code":"\nsummary(fm)\n#> \n#> Call:\n#> lm(formula = iq2 ~ iq1, data = df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -11.0342  -3.8218  -0.5852   3.4658  12.5635 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.8389     3.0269   0.608    0.548    \n#> iq1           0.8499     0.1155   7.357 2.29e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.102 on 32 degrees of freedom\n#> Multiple R-squared:  0.6285, Adjusted R-squared:  0.6169 \n#> F-statistic: 54.13 on 1 and 32 DF,  p-value: 2.288e-08\nqt(.975, 32)\n#> [1] 2.036933\n1 - pt(7.357, 32)\n#> [1] 1.145083e-08\nfm$coef[2] + c(-1, 1) * qt(.025, 32) * 0.1155\n#> [1] 1.0851203 0.6145887"},{"path":"il-modello-statistico-della-regressione-lineare.html","id":"considerazioni-conclusive","chapter":"Capitolo 17 Il modello statistico della regressione lineare","heading":"Considerazioni conclusive","text":"Il modello di regressione lineare semplice viene usato per descrivere la\nrelazione tra due variabili e per determinare il segno e l’intensità di\ntale relazione. Inoltre, il modello di regressione ci consente di\nprevedere il valore della variabile dipendente base ad alcuni nuovi\nvalori della variabile indipendente. Il modello di regressione lineare\nsemplice è realtà molto limitato, quanto descrive soltanto la\nrelazione tra la variabile dipendente \\(y\\) e una sola variabile\nesplicativa \\(x\\). Esso diventa molto più utile quando incorpora più\nvariabili indipendenti. questo secondo caso, però, calcoli per la\nstima dei coefficienti del modello diventano più complicati. Abbiamo\ndeciso qui di presentare solo il modello di regressione lineare semplice\nperché, quel caso, sia la logica dell’inferenza sia le procedure di\ncalcolo sono facilmente maneggiabili. Nel caso più generale, quello del\nmodello di regressione multipla, la logica dell’inferenza rimarrà\nidentica quella discussa qui, ma le procedure di calcolo richiedono\nl’uso dell’algebra matriciale che esula dagli scopi del presente\ninsegnamento. Il modello di regressione multipla può includere sia\nregressori quantitativi, sia regressori qualitativi, utilizzando un\nopportuna schema di codifica. È interessante notare come un modello di\nregressione multipla che include una sola variabile esplicativa\nquantitativa corrisponde ’analisi della varianza ad una via; un\nmodello di regressione multipla che include più di una variabile\nesplicativa quantitativa corrisponde ’analisi della varianza più vie.\nQuesti argomenti verranno sviluppati negli insegnamenti di carattere\nquantitativo più avanzati. Possiamo qui concludere dicendo che il\nmodello di regressione, nelle sue varie forme e varianti, costituisce la\ntecnica di analisi dei dati maggiormente usata psicologia.","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"la-modellizzazione-bayesiana","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"Capitolo 18 La modellizzazione Bayesiana","text":"Prima di descrivere come il modello di regressione lineare possa essere applicato ai dati mediante l’approccio Bayesiano, esamineremo modelli statistici Bayesiani che vengono utilizzati alcuni casi più semplici, ovvero (1) il modello statistico per una proporzione e (2) il modello statistico utilizzato per il confronto tra due proporzioni. Estenderemo poi la discussione al caso cui viene considerato (3) un campione di osservazioni misurate su scala continua, assumendo che ciascuna osservazione provenga da una distribuzione Normale. tali circostanze, l’oggetto dell’inferenza sarà il parametro \\(\\mu\\) che rappresenta la media della popolazione da cui le osservazioni sono state tratte. questo punto saremo nelle condizioni di discutere (4) l’inferenza Bayesiana sulla differenza tra le medie di due popolazioni. Tale problema verrà affrontato specificando un modello di regressione lineare che include una variabile dipendente continua e una variabile indipendente dicotomica. Una volta chiarite le proprietà del modello di regressione questo caso semplice, sarà immediato estendere la discussione (5) al caso di una variabile indipendente continua.","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"modello-binomiale","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.1 Modello Binomiale","text":"Se facciamo nuovamente riferimento ’esempio del mappamondo di McElreath (2020), nel quale abbiamo osservato \\(= 6\\) volte “acqua” \\(N = 9\\) prove Bernoulliane indipendenti, allora il modello statistico che descrive l’esperimento casuale può essere\nformulato nei termini seguenti:\\[\\begin{equation}\n\\begin{aligned}\n&\\sim \\text{Binomiale}(N, p) \\\\\np &\\sim \\text{Uniforme}(0, 1) \n\\end{aligned}\n\\tag{18.1}\n\\end{equation}\\]dove la prima riga definisce la funzione di verosimiglianza e la seconda riga definisce la distribuzione priori. Il segno \\(\\sim\\) (tilde) si può leggere “si distribuisce come.” La prima riga, dunque, ci dice che la variabile aleatoria \\(Y\\) segue la distribuzione Binomiale di parametri \\(N\\) e \\(p\\). La seconda riga specifica che, quale distribuzione priori, assumiamo una distribuzione uniforme (0 e 1) per il parametro \\(p\\).","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"sec:idrossi","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.2 Il presidente Trump e l’idrossiclorochina","text":"Cito dal Washington Post del 7 aprile 2020:One bizarre disturbing aspects President Trump’s nightly press briefings coronavirus pandemic turns drug salesman. Like cable TV pitchman hawking ‘male enhancement’ pills, Trump regularly extols virtues taking hydroxychloroquine, drug used treat malaria lupus, potential ‘game changer’ just might cure Covid-19.Tralasciamo qui il fatto che il presidente Trump non è un esperto questo campo. Esaminiamo invece le evidenze iniziali supporto dell’ipotesi che l’idrossiclorochina possa essere utile per la cura del Covid-19, ovvero le evidenze che erano disponibili nel momento cui il presidente Trump ha fatto le affermazioni riportate sopra (seguito, quest’idea è stata completamente screditata). Tali evidenze sono state fornite da Gautret et al. (2020).Il disegno sperimentale di Gautret et al. (2020) comprende, tra le altre cose, il confronto tra una condizione sperimentale e una condizione di controllo. Un articolo pubblicato da Hulme et al. (2020) si è posto il problema di rianalizzare dati di Gautret et al. (2020) – si veda\nhttps://osf.io/5dgmx/. Tra gli autori di questo secondo articolo figura anche Eric-Jan Wagenmakers, uno psicologo molto conosciuto per suoi contributi metodologici. Hulme et al. (2020) sottolineano il fatto che Gautret et al. (2020) si sono concentrati, nella loro analisi dei dati, soltanto su una parte del loro campione. Se però vengono considerati anche pazienti che sono stati esclusi dall’analisi dei dati, le conclusioni cui sono giunti Gautret et al. (2020) risultano fortemente indebolite.L’analisi dei dati proposta da Hulme et al. (2020) richiede l’uso di alcuni\nstrumenti statistici che, queste dispense, non verranno discussi. Ma\npossiamo giungere alle stesse conclusioni di Hulme et al. (2020) anche usando le\nprocedure statistiche che abbiamo descritto finora. Nella ricerca di\nGautret et al. (2020) il confronto importante è tra la proporzione di paziente\npositivi al virus SARS-CoV-2 nel gruppo cui è stata somministrata\nl’idrossiclorochina (6 su 14) e nel gruppo di controllo (cui non è\nstata somministrata l’idrossiclorochina: 14 positivi su 16). Ciò che\nfaremo sarà di calcolare la distribuzione posteriori per queste due\nproporzioni. Rappresenteremo graficamente le due distribuzioni \nposteriori per il parametro \\(p\\) che rappresenta la probabilità di\nrisultare positivo al SARS-CoV-2. Calcoleremo anche, separatamente per \ndue gruppi, l’intervallo di credibilità al 95%. Quindi concluderemo\nfacendo il confronto tra gli intervalli di credibilità dei due gruppi.Leggiamo dati R  creando due vettori seguenti. Il vettore ym\ncontiene dati del gruppo cui è stata somministrata\nl’idrossiclorochina e il vettore yc dati del gruppo di controllo. Il\nvalore \\(y = 1\\) indica che il paziente è positivo al virus SARS-CoV-2\n(l’ordine di 0 e 1 è irrilevante).Utilizzando la sintassi di rethinking, definiamo ora il modello statistico per una proporzione specificando una distribuzione priori non informativa:Calcoliamo la distribuzione posteriori per il parametro \\(p\\) nel caso del gruppo sperimentaleCosì facendo troviamo il seguente intervallo di credibilità al 95%:Creiamo ora un grafico che rappresenta la distribuzione posteriori del parametro \\(p\\):Ripetiamo quindi la stessa procedura seguita sopra, usando però dati del gruppo di controllo, e rappresentiamo la nuova distribuzione posteriori del parametro \\(p\\) come abbiamo fatto precedenza:Le due figure che abbiamo realizzato presentano le distribuzioni posteriori del parametro \\(p\\) (cioè la probabilità di risultare positivo al virus SARS-CoV-2) per due gruppi di pazienti considerati nella ricerca di Gautret et al. (2020). Le figure mostrano che le due distribuzioni posteriori sono chiaramente separate, il che suggerisce che il parametro \\(p\\) assume valori diversi nei due gruppi. Coerentemente con la conclusioni di Gautret et al. (2020), le stime posteriori per il parametro \\(p\\) che abbiamo trovato suggeriscono dunque che pazienti del gruppo sperimentale (cui è stata somministrata l’idrossiclorochina) hanno una minore probabilità di risultare positivi al SARS-CoV-2 rispetto ai pazienti del gruppo di controllo (cui non è stata somministrata l’idrossiclorochina).Possiamo giungere questa conclusione senza guardare le due figure ma confrontando gli intervalli di credibilità al 95% dei due gruppi. Gli intervalli di credibilità non si sovrappongono e questo suggerisce che il parametro \\(p\\) è diverso nei due gruppi. Possiamo dunque concludere, con un grado di certezza soggettiva del 95%, che nel gruppo sperimentale vi è una probabilità più bassa di risultare positivi al SARS-CoV-2 rispetto al gruppo di controllo.Fino questo punto non abbiamo fatto altro che replicare le conclusioni cui sono giunti Gautret et al. (2020), sia pur utilizzando una procedura statistica diversa. Tuttavia, questa anlisi dei dati c’è un aspetto che non abbiamo considerato. Hulme et al. (2020) hanno osservato che Gautret et al. (2020), nella loro analisi statistica, hanno escluso alcuni pazienti quali, nel gruppo sperimentale, sono realtà peggiorati, anziché essere migliorati. Se consideriamo dunque dati di tutti pazienti del campione che è stato raccolto (non solo quelli selezionati da Gautret et al. (2020), la situazione è la seguente. Gruppo sperimentale: 10 positivi su 18; gruppo di controllo: 14 positivi su 16. Ripetiamo dunque l’analisi descritta sopra utilizzando, per il gruppo sperimentale, tutti dati che abbiamo disposizione. Così facendo otteniamo il seguente intervallo di credibilità al 95%:Quando utilizziamo tutti dati (e non soltanto pazienti selezionati da Gautret et al. (2020)) notiamo che l’intervallo di credibilità al 95% per il gruppo sperimentale, ovvero [0.33, 0.79], si sovrappone ’intervallo di credibilità al 95% per il gruppo di controllo, ovvero [0.71 1.04]. base agli standard correnti, un risultato di questo tipo non viene considerato come evidenza sufficiente per potere concludere che il parametro \\(p\\) assume un valore diverso nei due gruppi.Concludiamo dicendo che questo, ovviamente, è solo un esercizio didattico: la ricerca di Gautret et al. (2020) include tante altre informazioni che qui non sono state considerate. Tuttavia, notiamo che la semplice analisi statistica che abbiamo descritto è stata grado di replicare le conclusioni cui sono giunti (per altra via) Hulme et al. (2020).","code":"\nym <- c(rep(1, 6), rep(0, 8))\nym\n#>  [1] 1 1 1 1 1 1 0 0 0 0 0 0 0 0\nyc <- c(rep(1, 14), rep(0, 2))\nyc\n#>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\nsuppressPackageStartupMessages(library(\"rethinking\"))\noptions(mc.cores = parallel::detectCores())\n\nflist <- alist(\n  y ~ dbinom(1, p),\n  p ~ dbeta(1, 1) \n)\nm <- quap( \n  flist, \n  data = list(y = ym)\n)\nprecis(m, prob = 0.95)\n#>        mean        sd      2.5%     97.5%\n#> p 0.4285638 0.1322588 0.1693412 0.6877863\npost <- extract.samples(m)\nplot(\n  density(post$p), \n  xlim = c(0, 1),\n  ylim = c(0, 5),\n  main = \"\",\n  xlab = \"Parametro p\",\n  ylab = \"Densità\"\n)\nm <- quap( \n  flist, \n  data = list(y = yc)\n)\nprecis(m, prob = 0.95)\n#>        mean         sd      2.5%    97.5%\n#> p 0.8749966 0.08267702 0.7129526 1.037041\n\npost <- extract.samples(m)\nplot(\n  density(post$p), \n  xlim = c(0, 1),\n  ylim = c(0, 5),\n  main = \"\",\n  xlab = \"Parametro p\",\n  ylab = \"Densità\"\n)\nlines(density(post$p), xlim = c(0, 1))\nym <- c(rep(1, 10), rep(0, 8))\nym\n#>  [1] 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\nm <- quap( \n  flist, \n  data = list(y = ym)\n)\nprecis(m, prob = 0.95)\n#>        mean        sd      2.5%     97.5%\n#> p 0.5555554 0.1171209 0.3260026 0.7851081"},{"path":"la-modellizzazione-bayesiana.html","id":"modello-normale","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.3 Modello Normale","text":"Facciamo ora un altro esempio considerando, questo caso, la distribuzione Normale\\[\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathcal{N}(\\mu = \\bar{X}, \\sigma = 100) \\\\\n\\sigma &= s_Y \n\\end{aligned}\n\\]\nQuesto secondo modello statistico ci dice che la variabile aleatoria \\(Y\\) segue la distribuzione Normale di parametri \\(\\mu\\) e \\(\\sigma\\). Il parametro \\(\\mu\\) è sconosciuto e abbiamo deciso di descrivere la nostra incertezza priori relativa ad esso mediante una distribuzione priori che segue la legge Normale con media uguale alla media campionaria e con deviazione standard 100. Il parametro \\(\\sigma\\) è fissato ad un valore pari alla deviazione standard del campione, \\(s_y\\). generale, però, anche il parametro \\(\\sigma\\) viene considerato ignoto e ad esso potrebbe essere assegnata una distribuzione priori come, ad esempio, \\(\\sigma \\sim \\text{Unif}(0, 100)\\).","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"il-modello-normale-con-quap","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.3.1 Il modello normale con quap()","text":"Per fare un esempio, consideriamo 30 valori del BDI-II dei soggetti clinici di Zetsche et al. (2019):Calcoliamo le statistiche descrittive:Definiamo ora il modello statistico mediante la funzione alist()e stimiamo la distribuzione posteriori di \\(\\mu\\):Esaminando il risultato ottenuto mediante precis():Possiamo così stabilire che l’intervallo di credibilità al 95% per il valore medio del BDI-II è compreso tra [29.0, 32.8]. Questo esempio ci mostra come possiamo calcolare l’intervallo di credibilità nel caso della media di un campione. Nella sezione successiva ci porremo il problema di come sia possibile fare il confronto tra le medie di due campioni indipendenti.","code":"\ndf <- data.frame(\n  x <- c(26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 24, 19, 39, 31, 25, 28, 35, 30, 26, 31, 41, 36, 26, 35, 33, 28, 27, 34, 27, 22)\n)\ntrue_sd <- sd(df$x)\ntrue_sd\n#> [1] 6.606858\n\nsample_mean <- mean(df$x)\nsample_mean\n#> [1] 30.93333\nflist <- alist(\n  x ~ dnorm(mu, sigma), \n  mu ~ a,\n  a ~ dnorm(sample_mean, 100),\n  sigma ~ true_sd\n)\nset.seed(123)\nm1 <- quap( \n    flist,\n    data = df \n)\nprecis(m1)\n#>       mean       sd     5.5%  94.5%\n#> a 30.93333 1.206154 29.00567 32.861"},{"path":"la-modellizzazione-bayesiana.html","id":"sec_mod_lin","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.4 Il modello di regressione lineare","text":"due modelli statistici che abbiamo presentato sopra descrivono il comportamento di una singola variabile aleatoria: una proporzione di “successi” o la media del livello BDI-II un campione. Se la distribuzione priori non è informativa, la distribuzione posteriori risulta centrata sul valore della statistica campionaria utilizzata per la stima del parametro (\\(\\bar{y}\\) o \\(p\\)). Quello che “guadagnamo” calcolando la distribuzione posteriori del parametro è la possibilità di quantificare la nostra incertezza rispetto alla stima del parametro (\\(\\mu\\) o \\(\\pi\\)): se l’intervallo di credibilità è grande, questo significa che dati del campione sono poco informativi rispetto al valore del parametro; se invece l’intervallo di credibilità è piccolo, allora concludiamo che siamo piuttosto certi del valore della nostra stima.Tuttavia, modelli di interesse per la psicologia (e per le altre scienze) descrivono le relazioni tra due o più variabili, e non soltanto il valore di una singola variabile. Per esempio, nel suo studio Regression towards mediocrity hereditary stature, Galton (1886) si è chiesto come sia possibile descrivere la relazione tra l’altezza dei figli e l’altezza dei padri. Il modo più semplice per rispondere ad una domanda di questo tipo è quello di formulare la risposta nei termini di un modello di regressione lineare – infatti, la tecnica statistica della regressione lineare fu inventata da Galton proprio per questo scopo.Il modello di regressione lineare è dunque il più semplice dei modelli statistici che descrivono la relazione tra due (o più) variabili. Usando la notazione che abbiamo introdotto questo capitolo, il modello di regressione può essere descritto nel modo seguente:\\[\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta(X_i - \\bar{X}) \\\\\n\\alpha &\\sim \\mathcal{N}(0, \\sigma_{\\alpha}) \\\\\n\\beta &\\sim \\mathcal{N}(0, \\sigma_{\\beta}) \\\\\n\\sigma &\\sim \\text{Unif}(0, 50) \n\\end{aligned}\n\\]\nLa verosimiglianza indica che ciascun valore \\(Y_i\\) (la nostra variabile dipendente) segue una distribuzione Normale. Tuttavia, ciascuna \\(Y_i\\) segue una distribuzione Normale avente una media diversa (come indicato dal pedice \\(\\) usato per \\(\\mu_i\\)). questa formulazione del modello, tutte le distribuzioni Normali relative alla \\(Y\\) hanno la stessa deviazione standard (\\(\\sigma\\), il che corrisponde ’assunzione di omoschedasticità). La cosa importante è che la media \\(\\mu\\) non è più il parametro che deve essere stimato – come avveniva invece nel caso del modello Normale che abbiamo discusso nel caso dei valori BDI-II. Nel modello statistico della regressione lineare, invece, \\(\\mu_i\\) è espresso nei termini di due altri due parametri, \\(\\alpha\\) e \\(\\beta\\), e nei termini di una quantità osservabile chiamata \\(X\\) (la nostra variabile indipendente), come indicato nella seconda riga della descrizione del modello statistico.La seconda riga della specificazione del modello statistico non esprime una relazione stocastica (non viene usato il segno \\(\\sim\\)), ma bensì una relazione deterministica (come indicato dall’uso del segno di uguale). Ciò significa che, una volta fissati parametri \\(\\alpha\\) e \\(\\beta\\), il valore \\(\\mu_i\\) è determinato maniera univoca (questa è la componente deterministica del modello di regressione lineare). Se il modello di regressione lineare è espresso come \\(\\mu_i = \\alpha + \\beta(X_i - \\bar{X})\\), allora possiamo assegnare ai parametri \\(\\alpha\\) e \\(\\beta\\) l’interpretazione che abbiamo già incontrato precedenza.Il parametro \\(\\alpha\\) è uguale alla media della \\(Y\\). precedenza abbiamo detto che, dal punto di vista geometrico, \\(\\alpha\\) corrisponde ’ordinata del punto cui la retta di regressione interseca l’asse verticale di un sistema di coordinate cartesiane. Nel caso presente abbiamo espresso dati come scarti dalla media: \\(X_i - \\bar{X}\\), da cui segue che la media di \\(X\\) avrà valore zero. Da ciò deriva l’interpretazione di \\(\\alpha\\) che abbiamo fornito sopra.Il parametro \\(\\alpha\\) è uguale alla media della \\(Y\\). precedenza abbiamo detto che, dal punto di vista geometrico, \\(\\alpha\\) corrisponde ’ordinata del punto cui la retta di regressione interseca l’asse verticale di un sistema di coordinate cartesiane. Nel caso presente abbiamo espresso dati come scarti dalla media: \\(X_i - \\bar{X}\\), da cui segue che la media di \\(X\\) avrà valore zero. Da ciò deriva l’interpretazione di \\(\\alpha\\) che abbiamo fornito sopra.Il parametro \\(\\beta\\) ci dice di quanto varia, media, il valore \\(Y\\) per ogni variazione unitaria della \\(X\\). Abbiamo visto precedenza che il parametro \\(\\beta\\) viene chiamato “pendenza” perché, termini geometrici, il modello di regressione lineare assume la forma di una retta che, ’interno di un diagramma dispersione, approssima quanto meglio è possibile la nube di punti \\((X, Y)\\).Il parametro \\(\\beta\\) ci dice di quanto varia, media, il valore \\(Y\\) per ogni variazione unitaria della \\(X\\). Abbiamo visto precedenza che il parametro \\(\\beta\\) viene chiamato “pendenza” perché, termini geometrici, il modello di regressione lineare assume la forma di una retta che, ’interno di un diagramma dispersione, approssima quanto meglio è possibile la nube di punti \\((X, Y)\\).Nel modello statistico presentato sopra, la nostra incertezza su \\(\\beta\\) è stata quantificata mediante una distribuzione priori centrata sullo zero e con una deviazione standard pari \\(\\sigma_{\\beta}\\). La nostra incertezza su \\(\\alpha\\) è stata quantificata mediante una distribuzione priori centrata sullo zero e con una deviazione standard pari \\(\\sigma_{\\alpha}\\). Infine, la nostra incertezza su \\(\\sigma\\) è stata quantificata mediante una distribuzione uniforme compresa tra 0 e 50.Per ora ci siamo limitati descrivere la formulazione Bayesiana del modello di regressione mediante la sintassi di rethinking, così come fa McElreath (2020). Nelle sezioni seguenti vedremo come sia possibile svolgere l’analisi di regressione termini Bayesiani. Inizieremo considerando il caso più semplice, ovvero quello nel quale la variabile \\(X\\) è una variabile dicotomica; considereremo poi il caso cui \\(X\\) è una variabile continua.","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"sec_var_ind_dico","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.4.1 Variabile indipendente dicotomica","text":"Solitamente, un modello di regressione lineare come quello descritto nella sezione Il modello di regressione lineare, la variabile \\(X\\) è una variabile continua. Tuttavia, è anche possibile che \\(X\\) sia una variabile discreta. Consideriamo qui il caso più semplice, ovvero quello cui \\(X\\) assume solo due valori: 0 e 1. tali circostanze, il modello lineare può essere usato per il confronto tra le medie di due gruppi. Vediamo perché.Se \\(X\\) è una variabile dicotomica (con valori 0 e 1), allora per il modello lineare \\(\\mu_i = \\alpha + \\beta x_i\\) abbiamo quanto segue. Quando \\(X=0\\), il modello diventa\\[\\mu_i = \\alpha\\]\nmentre, quando \\(X=1\\), il modello diventa\\[\\mu_i = \\alpha + \\beta.\\]\nCiò significa che il parametro \\(\\alpha\\) è uguale alla media del gruppo codificato con \\(X=0\\) e il parametro \\(\\beta\\) è uguale alla differenza tra le medie dei due gruppi (essendo la media del secondo gruppo uguale \\(\\alpha + \\beta\\)).tali circostanze, il parametro \\(\\beta\\) risulta particolarmente utile quanto, nel caso di due gruppi, codifica direttamente l’effetto di una manipolazione sperimentale o di un trattamento (ovvero, esprime la differenza tra le medie di due gruppi). Per “effetto di un trattamento” si intende appunto la differenza tra le medie di due gruppi (per esempio, il gruppo “sperimentale” e il gruppo “di controllo”). L’inferenza su \\(\\beta\\) può dunque aiutarci capire quanto può essere considerato “robusto” l’effetto di un trattamento o di una manipolazione sperimentale.","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"sec_var_ind_dico","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.4.2 Un esempio pratico","text":"Esaminiamo un sottoinsieme di dati tratto dal National Longitudinal Survey Youth quali fanno parte di un esempio discusso da Gelman et al. (2020). soggetti sono bambini di 3 e 4 anni. La variabile dipendente, kid_score, è il punteggio totale del Peabody Individual Achievement Test (PIAT) costituito dalla somma dei punteggi di tre sottoscale (Mathematics, Reading comprehension, Reading recognition). La variabile indipendente, mom_hs, è il livello di istruzione della madre, codificato con due livelli: scuola media superiore completata oppure . La domanda della ricerca è se il QI del figlio (misurato con la scala PIAT) risulta o meno associato al livello di istruzione della madre.Codifichiamo il livello di istruzione della madre (\\(X\\)) con una variabile indicatrice (ovvero, una variabile che assume solo valori 0 e 1) tale per cui:\\(X=0\\): la madre non ha completato la scuola secondaria di secondo grado (scuola media superiore);\\(X=0\\): la madre non ha completato la scuola secondaria di secondo grado (scuola media superiore);\\(X=1\\): la madre ha completato la scuola media superiore.\\(X=1\\): la madre ha completato la scuola media superiore.Supponiamo che dati siano contenuti nel data.frame df.Calcoliamo le statistiche descrittive per due gruppi:Il punteggio PIAT medio è 77.5 per bambini la cui madre non ha il diploma di scuola media superiore, e pari 89.3 per bambini la cui madre ha completato la scuola media superiore. Questa differenza suggerisce un’associazione tra le variabili, ma tale differenza potrebbe essere soltanto la conseguenza della variabilità campionaria, senza riflettere una caratteristica generale della popolazione. Come possiamo usare il modello statistico lineare per fare inferenza sulla differenza osservata tra due gruppi?Per rispondere questa domanda specifichiamo il modello statistico che descrive la differenza tra punteggi PIAT dei due gruppi mediante un modello di regressione lineare:Si noti che abbiamo specificato tale modello statistico seguendo la stessa logica descritta ’inizio della sezione Il modello di regressione lineare. Ovvero, abbiamo esplicitato:la verosimiglianza dei dati;la verosimiglianza dei dati;il modello statistico della regressione lineare che esprime il valore atteso della variabile dipendente come una funzione lineare della variabile indipendente;il modello statistico della regressione lineare che esprime il valore atteso della variabile dipendente come una funzione lineare della variabile indipendente;la distribuzione priori di ciascuno dei parametri del modello, ovvero \\(\\), \\(b\\) e \\(\\sigma\\).la distribuzione priori di ciascuno dei parametri del modello, ovvero \\(\\), \\(b\\) e \\(\\sigma\\).Nel caso presente, abbiamo specificato due distribuzioni priori “debolmente informative” per parametri \\(\\) e \\(b\\). La distribuzione priori del parametro \\(\\) è una distribuzione Normale centrata sulla media di tutti dati (calcolata ignorando la suddivisione gruppi), con una deviazione standard relativamente grande. Ciò significa che, priori, per il parametro \\(\\) riteniamo plausibili valori che sono inclusi nell’intervallo \\(86.8 \\pm 2 \\times 100\\) punti PIAT, anche se riteniamo più probabili valori prossimi 86.8. Così facendo, prima di avere osservato dati campionari, esprimiamo una generale incertezza su quello che potrebbe essere il valore della media del gruppo codificato con \\(X = 0\\).maniera simile, caratterizziamo il possibile valore della differenza tra le medie tra due gruppi (ciò cui siamo interessati) maniera molto vaga: affermiamo che potrebbe essere un valore qualsiasi, probabilmente contenuto nell’intervallo \\(0 \\pm 2 \\times 100\\), assegnando priori una plausibilità maggiore ai valori prossimi allo zero (positivi e negativi). Per il parametro \\(b\\), specifichiamo dunque una distribuzione priori Normale centrata sullo zero. Così facendo, la distribuzione priori non favorisce né l’ipotesi secondo cui il parametro \\(b\\) sia maggiore di zero (ovvero, che la media dei punteggi PIAT sia maggiore nel gruppo codificato con \\(X = 0\\) rispetto al gruppo codificato\ncon \\(X = 1\\)), né l’ipotesi opposta. Specificando per \\(b\\) una distribuzione priori simmetrica centrata sullo zero non introduciamo dunque alcuna distorsione nella distribuzione posteriori: non favoriamo né l’ipotesi cui potremmo essere interessati (ad esempio, \\(> 0\\)), né l’ipotesi opposta.Infine, specifichiamo distribuzione priori uniforme nell’intervallo (0, 100) per il parametro \\(\\sigma\\) che descrive la distribuzione dei dati attorno al loro valore atteso (ovvero, attorno alla retta di regressione).Adattiamo il modello ai dati utilizzando la funzione quap():Estraiamo alcuni campioni dalla distribuzione posteriori:Esaminiamo la distribuzione posteriori dei parametri mediante le istruzioni seguenti.\nFigura 18.1: Distribuzioni posteriori dei parametri , b e \\(\\sigma\\) del modello statistico lineare che descrive punteggi del Peabody Individual Achievement Test come funzione del gruppo di appartenenza: bambini la cui madre non ha completato la scuola media superiore e bambini la cui madre ha completato la scuola media superiore. dati sono tratti da Gelman et al. (2020).\nrisultati possono anche essere esaminati mediante la funzione precis() che fornisce la stima posteriori del parametro e l’intervallo di credibilità al livello desiderato:risultati confermano ciò che ci aspettavamo: il coefficiente \\(\\) corrisponde alla media del gruppo codificato con \\(X = 0\\), ovvero la media dei punteggi PIAT per bambini la cui madre non ha completato la scuola media superiore; il coefficiente \\(b\\) corrisponde alla differenza tra le medie dei due gruppi, ovvero 89.32 - 77.55 = 11.77. Il coefficiente \\(b\\) ci dice dunque che bambini la cui madre ha completato la scuola superiore ottengono media 12 punti più rispetto ai bambini la cui madre non ha completato la scuola superiore. Per ora non consideriamo l’interpretazione del parametro \\(\\sigma\\) (si veda più sotto). Una rappresentazione grafica dell’interpretazione che abbiamo fornito ai parametri del modello lineare è fornita nella figura 18.2.\nFigura 18.2: Distribuzione dei punteggi del Peabody Individual Achievement Test due gruppi di bambini facenti parte del campione discusso da Gelman et al. (2020): bambini la cui madre non hanno completato la scuola media superiore (\\(X\\) = 0) e bambini la cui madre ha completato la scuola media superiore (\\(X\\) = 1).\nAbbiamo visto sopra che il parametro \\(b\\) = 11.77 riflette semplicemente la differenza tra le medie dei due gruppi. Ma il modello statistico lineare ci dice qualcosa più: esso quantifica la nostra incertezza relativamente tale differenza, al di là delle caratteristiche specifiche del particolare campione che abbiamo esaminato. È ovvio chiedersi: se esaminassimo un altro campione, quanto sarebbe grande questa differenza? E un altro campione ancora? Il modello statistico lineare ci dice che, indipendentemente da quale campione di dati verrà esaminiamo, ci possiamo aspettare, con un grado di certezza del 95%, che la differenza tra le medie dei due gruppi sia compresa nell’intervallo tra 7.2 e 16.3 punti PIAT. Questo è il significato dell’intervallo di credibilità al 95% che è stato calcolato e che ci viene fornito dalla funzione precis().L’intervallo di credibilità al 95% rappresenta una stima dell’intervallo di valori che contengono il 95% dell’area della distribuzione posteriori del parametro \\(b\\). Nella figura 18.1 si vedono le distribuzioni posteriori dei parametri \\(\\) e \\(b\\). Tali distribuzioni sono state generate estraendo un numero molto grande di campioni dalle\ndistribuzioni posteriori di \\(\\) e \\(b\\). Solitamente tali stime sono ottenute mediante una variante dell’algoritmo di Metropolis; l’approssimazione quadratica qui usata fornisce un’approssimazione questo processo. Dalla figura 18.1 vediamo che le distribuzioni posteriori tendono ad essere Normali. Vediamo inoltre che valori più plausibili per il parametro \\(b\\) sono compresi tra 7.2 e 16.3, come ci dice l’intervallo di credibilità al 95%. Il problema di come sia possibile specificare un intervallo di credibilità sulla base delle informazioni fornite dalla distribuzione posteriori è\ndiscusso nel capitolo XX. metodi di stima MCMC costituiscono la modalità usuale per generare la distribuzione posteriori nell’analisi Bayesiana. queste dispense, però, ci limitiamo ai metodi di stima basati sull’approssimazione quadratica. Abbiamo deciso di svolgere gli esercizi mediante l’approssimazione quadratica piuttosto che con il metodo MCMC perché l’installazione sul proprio computer del software necessario per le analisi MCMC costituisce un problema di tipo informatico che esula dagli scopi di questo insegnamento.conclusione, il modello statistico lineare riassume la differenza nei punteggi medi del test PIAT nei due gruppi di bambini: bambini la cui madre ha completato la scuola media superiore e bambini la cui madre non ha completato la scuola media superiore. Il modello statistico lineare ci consente inoltre di fare inferenza sulla differenza nei punteggi medi\ndel test PIAT nei due gruppi di bambini. Viene infatti definito un livello di credibilità che descrive, ad un determinato grado di certezza, quelli che sono valori plausibili di tale differenza, al di là delle idiosincrasie del particolare campione che abbiamo esaminato, ovvero tenendo considerazione il fenomeno della variabilità campionaria. Questo è il processo di inferenza Bayesiana che viene svolta mediante l’uso di un modello statistico lineare.","code":"\nlibrary(\"foreign\")\ndf <- read.dta(here(\"data\", \"kidiq.dta\"))\nhead(df)\n#>   kid_score mom_hs    mom_iq mom_work mom_age\n#> 1        65      1 121.11753        4      27\n#> 2        98      1  89.36188        4      25\n#> 3        85      1 115.44316        4      27\n#> 4        83      1  99.44964        3      25\n#> 5       115      1  92.74571        4      27\n#> 6        98      0 107.90184        1      18\ndf %>% \n  group_by(mom_hs) %>% \n  summarise(\n    mean_kid_score = mean(kid_score),\n    std = sqrt(var(kid_score))\n  )\n#> # A tibble: 2 x 3\n#>   mom_hs mean_kid_score   std\n#> *  <dbl>          <dbl> <dbl>\n#> 1      0           77.5  22.6\n#> 2      1           89.3  19.0\nflist <- alist(\n  kid_score ~ dnorm(mu, sigma),\n  mu ~ a + b * mom_hs,\n  a ~ dnorm(86.8, 100),\n  b ~ dnorm(0, 100),\n  sigma ~ dunif(0, 100)\n)\nm1 <- quap(\n  flist,\n  data = df\n)\npost <- extract.samples(m1)\npost[1:5, ]\n#>          a         b    sigma\n#> 1 80.07328  9.006764 19.73586\n#> 2 77.26278 11.935567 19.77395\n#> 3 76.66754 11.448966 19.31510\n#> 4 77.23189 10.742990 19.97299\n#> 5 80.54546  9.255061 19.83349\npar(mfrow = c(1, 3))\n\ndens(\n  post$a, lwd = 2.5, xlab = \"\",\n  ylab = \"Densità\", main = \"p(a | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\ndens(\n  post$b, lwd = 2.5, xlab = \"\",\n  ylab = \"\", main = \"p(b | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\ndens(\n  post$sigma, lwd = 2.5, xlab = \"\",\n  ylab = \"\", main = \"p(sigma | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\n    \npar(mfrow = c(1, 1))\nprecis(m1, prob = 0.95)\n#>           mean        sd      2.5%    97.5%\n#> a     77.55837 2.0528231 73.534910 81.58183\n#> b     11.75868 2.3158682  7.219661 16.29770\n#> sigma 19.80505 0.6721414 18.487679 21.12243"},{"path":"la-modellizzazione-bayesiana.html","id":"quale-distribuzione-a-priori-è-corretta","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.4.2.1 Quale distribuzione a priori è corretta?","text":"Un grave problema che è emerso negli anni recenti relativamente ’analisi dei dati psicologici (e non solo) è il cosiddetto “\\(p\\)-hacking,” ovvero la pratica di adattare il modello e dati allo scopo di ottenere il risultato desiderato. Il risultato desiderato è\ngeneralmente un valore-\\(p\\) inferiore al 5%. Il problema è che quando il modello statistico viene modificato alla luce dei dati osservati, valori-\\(p\\) non mantengono più il loro significato originario: altre parole, il “\\(p\\)-hacking” aumenta la probabilità di ottenere dei risultati falsi. L’approccio Bayesiano non prevede valori-\\(p\\), ma il rischio rimane se scegliamo le distribuzioni priori base alle proprietà del campione allo scopo di ottenere il risultato desiderato. La procedura che invece deve essere seguita è quella di scegliere le distribuzioni priori sulla base di considerazioni generali, indipendentemente dalle specifiche caratteristiche del campione.","code":""},{"path":"la-modellizzazione-bayesiana.html","id":"sec_var_ind_dico","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"18.5 Una variabile indipendente continua","text":"Continuiamo ora con l’esempio relativo al National Longitudinal Survey Youth discusso da Gelman et al. (2020) e poniamoci ora il problema di descrivere l’associazione tra\nl’intelligenza del bambino, kid_score (ovvero il punteggio totale del Peabody Individual Achievement Test,PIAT), e mom_iq (ovvero il quoziente di intelligenza della madre), che rappresenta una variabile continua.\nFigura 18.3: Punteggio del test PIAT come funzione del QI materno con sovrapposta la retta di regressione. Ogni punto sulla retta di regressione può essere concepito come un punteggio il punteggio predetto per un bambino la cui madri ha il QI corrispondente o come il punteggio PIAT medio per una sottopopolazione di bambini le cui madri hanno tutte quel particolare valore di QI. dati sono tratti da Gelman et al. (2020).)\nIl modello statistico lineare diventa:\\[\n\\begin{aligned}\nY_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta(X_i - \\bar{X}) \\\\\n\\alpha &\\sim \\mathcal{N}(0, \\sigma_{\\alpha}) \\\\\n\\beta &\\sim \\mathcal{N}(0, \\sigma_{\\beta}) \\\\\n\\sigma &\\sim \\text{Unif}(0, 50) \n\\end{aligned}\n\\]Abbiamo descritto \\(X\\) nei termini degli scostamenti dalla media (\\(X_i - \\bar{X}\\)) per fare modo che il coefficiente \\(\\alpha\\) corrisponda al valore atteso della \\(Y\\) corrispondenza della media di \\(X\\) (quoziente di intelligenza della madre) – questa è una conseguenza del fatto che la retta di regressione passa per il punto \\((\\bar{X}, \\bar{Y})\\). Infatti, avrebbe poco senso chiederci qual è il valore atteso del punteggio PIAT quando il quoziente d’intelligenza della madre è uguale zero.Specifichiamo dunque il modello statistico lineare con la sintassi richiesta da rethinking:Adattiamo il modello di regressione ai datiEsaminiamo il risultato ottenuto mediante la funzione precis():Troviamo così che\n\\[\n\\mathbb{E}(\\text{kid_score}) = 86 + 0.61 \\cdot x,\n\\]\nladdove \\(x\\) è la variabile kid_score espressa come scostamento rispetto al suo valore medio. Tale retta di regressione stimata è mostrata assieme ai dati nella figura 18.3.Il coefficiente \\(b\\) ci dice che, ’aumentare di un punto del quoziente d’intelligenza della madre, la media dei punteggi PIAT cresce di 0.61 unità. Il parametro \\(\\sigma\\) ci dice che la deviazione standard che quantifica la dispersione dei dati attorno alla retta di regressione è pari 18.22.L’intervallo di credibilità di questo coefficiente ci dice che, con un livello di certezza del 95%, possiamo essere sicuri che, ’aumentare di un punto del quoziente d’intelligenza della madre, la media dei punteggi PIAT crescerà, come minimo, di 0.50 punti e, come massimo, di 0.72 punti. La differenza 0.72 - 0.50 esprime il nostro grado di incertezza rispetto alla stima del parametro, quando vogliamo che la nostra stima sia “credibile” al livello di 0.95. Ma non c’è niente di “magico” o necessario relativamente al livello di 0.95. Infatti, il default della funzione precis() è 0.89. Ciascuno di questi valori è\narbitrario. Sono possibili tantissime soglie per quantificare la nostra incertezza: alcuni ricercatori usano il livello di 0.5. Il nostro obiettivo è quello di descrivere il livello della nostra incertezza relativamente alla stima del parametro. E la nostra incertezza è\ndescritta dall’intera distribuzione posteriori. Per cui il metodo più semplice, più diretto e più completo per descrivere la nostra incertezza rispetto alla stima dei parametri è quello di riportare graficamente tutta la distribuzione posteriori, come indicato per esempio nella figura 18.4.\nFigura 18.4: Distribuzione posteriori dei parametri \\(\\), \\(b\\) e \\(sigma\\) del modello statistico lineare che descrive punteggi del Peabody Individual Achievement Test come funzione del quoziente d’intelligenza della madre espresso come scostamento rispetto al suo valore medio. dati sono tratti da Gelman et al. (2020).\n","code":"\ndf %>% \n  ggplot(aes(x = mom_iq, y = kid_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    x = \"QI della madre\",\n    y = \"Peabody Individual Achievement Test\"\n  )\n#> `geom_smooth()` using formula 'y ~ x'\nflist <- alist(\n  kid_score ~ dnorm(mu, sigma),\n  mu ~ a + b * (mom_iq - mean(mom_iq)),\n  a ~ dnorm(mean(kid_score), 100),\n  b ~ dnorm(0, 100),\n  sigma ~ dunif(0, 100)\n)\nm2 <- quap(\n  flist,\n  data = df\n)\nprecis(m2, prob = 0.95)\n#>             mean         sd       2.5%      97.5%\n#> a     86.7973283 0.87475112 85.0828476 88.5118090\n#> b      0.6099751 0.05838627  0.4955402  0.7244101\n#> sigma 18.2240956 0.61857167 17.0117174 19.4364738\npost <- extract.samples(m2)\npar(mfrow = c(1, 3))\n\ndens(\n  post$a, lwd = 2.5, xlab = \"\",\n  ylab = \"Densità\", main = \"p(a | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\ndens(\n  post$b, lwd = 2.5, xlab = \"\",\n  ylab = \"\", main = \"p(b | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\ndens(\n  post$sigma, lwd = 2.5, xlab = \"\",\n  ylab = \"\", main = \"p(sigma | x, y)\",\n  cex.lab = 1.5, cex.axis = 1.35, cex.main = 1.5,\n  cex.sub = 1.5\n)\n    \npar(mfrow = c(1, 1))"},{"path":"la-modellizzazione-bayesiana.html","id":"conclusioni","chapter":"Capitolo 18 La modellizzazione Bayesiana","heading":"Conclusioni","text":"Questo capitolo ha introdotto il modello bivariato di regressione lineare, ovvero un metodo che ci consente di stimare l’associazione tra una variabile indipendente e una variabile dipendente. La distribuzione Normale può essere usata per specificare la funzione di verosimiglianza modelli statistici di questo tipo perché può essere concepita come un\nmodo di contare quanti diversi modi diverse combinazioni di \\(\\mu\\) e \\(\\sigma\\) possono produrre un’osservazione. Per adattare questi modelli ai dati, il presente capitolo ha introdotto un’approssimazione quadratica della distribuzione posteriori tramite la funzione quap(). Sono state inoltre introdotte nuove procedure per visualizzare le distribuzioni posteriori e per descrivere le stime posteriori.Si noti che, per definire la distribuzione priori del parametro \\(p\\) di una distribuzione di Bernoulli, abbiamo usato una distribuzione uniforme (priori non informativa), ovvero una Beta(1, 1). tali circostanze, l’intervallo di credibilità è praticamente identico ’intervallo di confidenza di tipo frequentista. L’intervallo di credibilità, invece, differisce dall’intervallo di confidenza frequentista quando il modello statistico Bayesiano include una distribuzione priori informativa o debolmente informativa. L’analisi dei dati Bayesiana fa quasi sempre uso di distribuzioni priori debolmente informative, mentre le distribuzioni priori informative sono più rare. Le distribuzioni priori debolmente informative hanno quale scopo la regolarizzazione, cioè, l’obiettivo di mantenere le inferenze una gamma ragionevole di valori; ciò contribuisce nel contempo limitare l’influenza eccessiva delle osservazioni estreme (valori anomali).","code":""},{"path":"appendici.html","id":"appendici","chapter":"Appendici","heading":"Appendici","text":"","code":""},{"path":"appendici.html","id":"simbologia-di-base","chapter":"Appendici","heading":"Simbologia di base","text":"Per una scrittura più sintetica possono essere utilizzati alcuni simboli\nmatematici.L’operatore logico booleano \\(\\land\\) significa “e” (congiunzione\nforte) mentre il connettivo di disgiunzione \\(\\lor\\) significa “o”\n(oppure) (congiunzione debole).L’operatore logico booleano \\(\\land\\) significa “e” (congiunzione\nforte) mentre il connettivo di disgiunzione \\(\\lor\\) significa “o”\n(oppure) (congiunzione debole).Il quantificatore esistenziale \\(\\exists\\) vuol dire “esiste almeno\nun” e indica l’esistenza di almeno una istanza del concetto/oggetto\nindicato. Il quantificatore esistenziale di unicità \\(\\exists!\\)\n(“esiste soltanto un”) indica l’esistenza di esattamente una istanza\ndel concetto/oggetto indicato. Il quantificatore esistenziale\n\\(\\nexists\\) nega l’esistenza del concetto/oggetto indicato.Il quantificatore esistenziale \\(\\exists\\) vuol dire “esiste almeno\nun” e indica l’esistenza di almeno una istanza del concetto/oggetto\nindicato. Il quantificatore esistenziale di unicità \\(\\exists!\\)\n(“esiste soltanto un”) indica l’esistenza di esattamente una istanza\ndel concetto/oggetto indicato. Il quantificatore esistenziale\n\\(\\nexists\\) nega l’esistenza del concetto/oggetto indicato.Il quantificatore universale \\(\\forall\\) vuol dire “per ogni.”Il quantificatore universale \\(\\forall\\) vuol dire “per ogni.”L’implicazione logica “\\(\\Rightarrow\\)” significa “implica” (se\n…allora). \\(P \\Rightarrow Q\\) vuol dire che \\(P\\) è condizione\nsufficiente per la verità di \\(Q\\) e che \\(Q\\) è condizione necessaria\nper la verità di \\(P\\).L’implicazione logica “\\(\\Rightarrow\\)” significa “implica” (se\n…allora). \\(P \\Rightarrow Q\\) vuol dire che \\(P\\) è condizione\nsufficiente per la verità di \\(Q\\) e che \\(Q\\) è condizione necessaria\nper la verità di \\(P\\).L’equivalenza matematica “\\(\\iff\\)” significa “se e solo se” e indica\nuna condizione necessaria e sufficiente, o corrispondenza biunivoca.L’equivalenza matematica “\\(\\iff\\)” significa “se e solo se” e indica\nuna condizione necessaria e sufficiente, o corrispondenza biunivoca.Il simbolo \\(\\vert\\) si legge “tale che.”Il simbolo \\(\\vert\\) si legge “tale che.”Il simbolo \\(\\triangleq\\) (o \\(:=\\)) si legge “uguale per definizione.”Il simbolo \\(\\triangleq\\) (o \\(:=\\)) si legge “uguale per definizione.”Il simbolo \\(\\Delta\\) indica la differenza fra due valori della\nvariabile scritta destra del simbolo.Il simbolo \\(\\Delta\\) indica la differenza fra due valori della\nvariabile scritta destra del simbolo.Il simbolo \\(\\propto\\) si legge “proporzionale .”Il simbolo \\(\\propto\\) si legge “proporzionale .”Il simbolo \\(\\approx\\) si legge “circa.”Il simbolo \\(\\approx\\) si legge “circa.”Il simbolo \\(\\\\) della teoria degli insiemi vuol dire “appartiene” e\nindica l’appartenenza di un elemento ad un insieme. Il simbolo\n\\(\\notin\\) vuol dire “non appartiene.”Il simbolo \\(\\\\) della teoria degli insiemi vuol dire “appartiene” e\nindica l’appartenenza di un elemento ad un insieme. Il simbolo\n\\(\\notin\\) vuol dire “non appartiene.”Il simbolo \\(\\subseteq\\) si legge “è un sottoinsieme di” (può\ncoincidere con l’insieme stesso). Il simbolo \\(\\subset\\) si legge “è\nun sottoinsieme proprio di.”Il simbolo \\(\\subseteq\\) si legge “è un sottoinsieme di” (può\ncoincidere con l’insieme stesso). Il simbolo \\(\\subset\\) si legge “è\nun sottoinsieme proprio di.”Il simbolo \\(\\#\\) indica la cardinalità di un insieme.Il simbolo \\(\\#\\) indica la cardinalità di un insieme.Il simbolo \\(\\cap\\) indica l’intersezione di due insiemi. Il simbolo\n\\(\\cup\\) indica l’unione di due insiemi.Il simbolo \\(\\cap\\) indica l’intersezione di due insiemi. Il simbolo\n\\(\\cup\\) indica l’unione di due insiemi.Il simbolo \\(\\emptyset\\) indica l’insieme vuoto o evento impossibile.Il simbolo \\(\\emptyset\\) indica l’insieme vuoto o evento impossibile.","code":""},{"path":"appendici.html","id":"numeri-binari-interi-razionali-irrazionali-e-reali","chapter":"Appendici","heading":"Numeri binari, interi, razionali, irrazionali e reali","text":"","code":""},{"path":"appendici.html","id":"numeri-binari","chapter":"Appendici","heading":"Numeri binari","text":"più semplici sono numeri binari, cioè zero o uno. Useremo spesso\nnumeri binari per rappresentare se qualcosa è vero o falso, o presente o\nassente.Supponiamo di chiedere 10 studenti “Ti piacciono mirtilli?” Poniamo\nche le risposte siano le seguenti:Tali risposte possono essere ricodificate nei termini di valori di\nverità, ovvero, vero e falso, generalmente denotati rispettivamente come\n1 e 0. tale ricodifica può essere effettuata mediante l’operatore\n== che è un test per l’uguaglianza e restituisce il valore logico VERO\nse le due cose sono uguali e FALSO se non lo sono:R considera valori di verità e numeri binari modo equivalente, con\nTRUE uguale 1 e FALSE uguale zero. Di conseguenza, possiamo\neffettuare operazioni algebriche sui valori logici VERO e FALSO.\nNell’esempio, possiamo sommare valori di verità, dividere per 10e concludere che 7 risposte su 10 sono positive.","code":"\nopinion <- c('Yes','No','Yes','No','Yes','No','Yes','Yes','Yes','Yes')\nopinion\n#>  [1] \"Yes\" \"No\"  \"Yes\" \"No\"  \"Yes\" \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\"\nopinion <- opinion == \"Yes\"\nopinion\n#>  [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\nsum(opinion) / length(opinion)\n#> [1] 0.7"},{"path":"appendici.html","id":"numeri-interi","chapter":"Appendici","heading":"Numeri interi","text":"Un numero intero è un numero senza decimali. Si dicono naturali \nnumeri che servono contare, come 1, 2, … L’insieme dei numeri\nnaturali si indica con il simbolo \\(\\mathbb{N}\\). È anche necessario\nintrodurre numeri con il segno per poter trattare grandezze negative.\nSi ottengono così l’insieme numerico dei numeri interi relativi:\n\\(\\mathbb{Z} = \\{0, \\pm 1, \\pm 2, \\dots \\}\\)","code":""},{"path":"appendici.html","id":"numeri-razionali","chapter":"Appendici","heading":"Numeri razionali","text":"numeri razionali sono numeri frazionari \\(m/n\\), dove \\(m, n \\N\\),\ncon \\(n \\neq 0\\). Si ottengono così numeri razionali:\n\\(\\mathbb{Q} = \\{\\frac{m}{n} \\,\\vert\\, m, n \\\\mathbb{Z}, n \\neq 0\\}\\).\nÈ evidente che \\(\\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{Q}\\).\nAnche questo caso è necessario poter trattare grandezze negative. \nnumeri razionali non negativi sono indicati con\n\\(\\mathbb{Q^+} = \\{q \\\\mathbb{Q} \\,\\vert\\, q \\geq 0\\}\\).","code":""},{"path":"appendici.html","id":"numeri-irrazionali","chapter":"Appendici","heading":"Numeri irrazionali","text":"Tuttavia, non tutti punti di una retta \\(r\\) possono essere\nrappresentati mediante numeri interi e razionali. È dunque necessario\nintrodurre un’altra classe di numeri. Si dicono irrazionali, e sono\ndenotati con \\(\\mathbb{R}\\), numeri che possono essere scritti come una\nfrazione \\(/ b\\), con \\(\\) e \\(b\\) interi e \\(b\\) diverso da 0. numeri\nirrazionali sono numeri illimitati e non periodici che quindi non\npossono essere espressi sotto forma di frazione. Per esempio,\n\\(\\sqrt{2}\\), \\(\\sqrt{3}\\) e \\({\\displaystyle \\pi =3,141592\\ldots}\\) sono\nnumeri irrazionali.","code":""},{"path":"appendici.html","id":"numeri-reali","chapter":"Appendici","heading":"Numeri reali","text":"punti della retta \\(r\\) sono quindi “di più” dei numeri razionali. Per\npoter rappresentare tutti punti della retta abbiamo dunque bisogno dei\nnumeri reali. numeri reali possono essere positivi, negativi o nulli\ne comprendono, come casi particolari, numeri interi, numeri\nrazionali e numeri irrazionali. Spesso statisticac il numero dei\ndecimali indica il grado di precisione della misurazione.","code":""},{"path":"appendici.html","id":"intervalli","chapter":"Appendici","heading":"Intervalli","text":"Un intervallo si dice chiuso se gli estremi sono compresi\nnell’intervallo, aperto se gli estremi non sono compresi. Le\ncaratteristiche degli intervalli sono riportate nella tabella seguente.","code":""},{"path":"appendici.html","id":"cap:insiemi","chapter":"Appendici","heading":"Insiemi","text":"Un insieme (o collezione, classe, gruppo, …) è un concetto primitivo,\novvero è un concetto che già possediamo. Georg Cantor l’ha definito nel\nmodo seguente:un insieme è una collezione di oggetti, determinati e distinti, della nostra percezione o del nostro pensiero, concepiti come un tutto unico; tali oggetti si dicono elementi dell’insieme.Mentre non è rilevante la natura degli oggetti che costituiscono\nl’insieme, ciò che importa è distinguere se un dato oggetto appartenga o\nmeno ad un insieme. Deve essere vera una delle due possibilità: il dato\noggetto è un elemento dell’insieme considerato oppure non è elemento\ndell’insieme considerato. Due insiemi \\(\\) e \\(B\\) si dicono uguali se sono\nformati dagli stessi elementi, anche se disposti ordine diverso:\n\\(=B\\). Due insiemi \\(\\) e \\(B\\) si dicono diversi se non contengono gli\nstessi elementi: \\(\\neq B\\). Ad esempio, seguenti insiemi sono uguali:\n\\[\\{1, 2, 3\\} = \\{3, 1, 2\\} = \\{1, 3, 2\\}= \\{1, 1, 1, 2, 3, 3, 3\\}.\\]\nGli insiemi sono denotati da una lettera maiuscola, mentre le lettere\nminuscole, di solito, designano gli elementi di un insieme. Per esempio,\nun generico insieme \\(\\) si indica con\n\\[= \\{a_1, a_2, \\dots, a_n\\}, \\quad \\text{con~} n > 0.\\]La scrittura \\(\\\\) dice che \\(\\) è un elemento di \\(\\). Per dire che\n\\(b\\) non è un elemento di \\(\\) si scrive \\(b \\notin .\\)Per quegli insiemi cui elementi soddisfano una certa proprietà che li\ncaratterizza, tale proprietà può essere usata per descrivere più\nsinteticamente l’insieme:\n\\[\n= \\{x ~\\vert~ \\text{proprietà posseduta da~} x\\},\n\\]\nche si legge come “\\(\\) è l’insieme degli elementi \\(x\\) per cui è vera la proprietà\nindicata.” Per esempio, per indicare l’insieme \\(\\) delle coppie di\nnumeri reali \\((x,y)\\) che appartengono alla parabola \\(y = x^2 + 1\\) si può\nscrivere:\n\\[\n= \\{(x,y) ~\\vert~ y = x^2 + 1\\}.\n\\]\nDati due insiemi \\(\\) e \\(B\\), diremo che \\(\\) è un sottoinsieme di \\(B\\) se\ne solo se tutti gli elementi di \\(\\) sono anche elementi di \\(B\\):\n\\[\\subseteq B \\iff (\\forall x \\\\Rightarrow x \\B).\\] Se esiste\nalmeno un elemento di \\(B\\) che non appartiene ad \\(\\) allora diremo che\n\\(\\) è un sottoinsieme proprio di \\(B\\):\n\\[\n\\subset B \\iff (\\subseteq B, \\exists~ x \\B ~\\vert~ x \\notin ).\n\\]\nUn altro insieme, detto insieme delle parti, o insieme potenza, che si\nassocia ’insieme \\(\\) è l’insieme di tutti sottoinsiemi di \\(\\),\ninclusi l’insieme vuoto e \\(\\) stesso. Per esempio, per l’insieme\n\\(= \\{, b, c\\}\\), l’insieme delle parti è:\n\\[\n\\mathcal{P}() = \\{\n\\emptyset, \\{\\}, \\{b\\}, \\{c\\},\n \\{, b\\}, \\{, c\\}, \\{c, b\\},\n \\{, b, c\\}\n\\}.\n\\]","code":""},{"path":"appendici.html","id":"operazioni-tra-insiemi","chapter":"Appendici","heading":"Operazioni tra insiemi","text":"Si definisce intersezione di \\(\\) e \\(B\\) l’insieme \\(\\cap B\\) di tutti\ngli elementi \\(x\\) che appartengono ad \\(\\) e contemporaneamente \\(B\\):\n\\[\\cap B = \\{x ~\\vert~ x \\\\land x \\B\\}.\\]Si definisce unione di \\(\\) e \\(B\\) l’insieme \\(\\cup B\\) di tutti gli\nelementi \\(x\\) che appartengono ad \\(\\) o \\(B\\), cioè\n\\[\n\\cup B = \\{x ~\\vert~ x \\\\lor x \\B\\}.\n\\]Differenza. Si indica con \\(\\setminus B\\) l’insieme degli elementi di\n\\(\\) che non appartengono \\(B\\):\n\\[\\setminus B = \\{x ~\\vert~ x \\\\land x \\notin B\\}.\\]Insieme complementare. Nel caso che sia \\(B \\subseteq \\), l’insieme\ndifferenza \\(\\setminus B\\) è detto insieme complementare di \\(B\\) \\(\\) e\nsi indica con \\(B^C\\).Dato un insieme \\(S\\), una partizione di \\(S\\) è una collezione di\nsottoinsiemi di \\(S\\), \\(S_1, \\dots, S_k\\), tali che\n\\[S = S_1 \\cup S_2 \\cup \\dots S_k\\] e\n\\[S_i \\cap S_j, \\quad \\text{con~} \\neq j.\\]La relazione tra unione, intersezione e insieme complementare è data\ndalle leggi di DeMorgan: \\[(\\cup B)^c = ^c \\cap B^c,\\]\n\\[(\\cap B)^c = ^c \\cup B^c.\\]","code":""},{"path":"appendici.html","id":"diagrammi-di-eulero-venn","chapter":"Appendici","heading":"Diagrammi di Eulero-Venn","text":"molte situazioni è utile servirsi dei cosiddetti diagrammi di\nEulero-Venn per rappresentare gli insiemi e verificare le proprietà\ndelle operazioni tra insiemi (si veda la figura 18.5.\ndiagrammi di Venn sono così nominati onore del matematico inglese del diciannovesimo secolo John Venn anche se Leibnitz e Eulero avevano già precedenza utilizzato rappresentazioni simili.\ntale rappresentazione, gli insiemi sono individuati da regioni del piano delimitate da una curva chiusa. Nel caso di insiemi finiti, è possibile evidenziare esplicitamente alcuni elementi di un insieme mediante punti, quando si\npossono anche evidenziare tutti gli elementi degli insiemi considerati.\nFigura 18.5: tutte le figure \\(S\\) è la regione delimitata dal rettangolo, \\(L\\) è la regione ’interno del cerchio di sinistra e \\(R\\) è la regione ’interno del cerchio di destra. La regione evidenziata mostra l’insieme indicato sotto ciascuna figura.\ndiagrammi di Eulero-Venn che forniscono una dimostrazione delle leggi\ndi DeMorgan sono forniti nella figura 18.6.\nFigura 18.6: Dimostrazione delle leggi di DeMorgan.\n","code":""},{"path":"appendici.html","id":"sec:prod_cartesiano","chapter":"Appendici","heading":"Coppie ordinate e prodotto cartesiano","text":"Una coppia ordinata \\((x,y)\\) è l’insieme cui elementi sono \\(x \\\\) e\n\\(y \\B\\) e nella quale \\(x\\) è la prima componente (o prima coordinata),\n\\(y\\) la seconda. L’insieme di tutte le coppie ordinate costruite \npartire dagli insiemi \\(\\) e \\(B\\) viene detto prodotto cartesiano:\n\\[\\times B = \\{(x, y) ~\\vert~ x \\\\land y \\B\\}.\\] Ad esempio,\nsia \\(= \\{1, 2, 3\\}\\) e \\(B = \\{, b\\}\\). Allora,\n\\[\\{1, 2\\} \\times \\{, b, c\\} = \\{(1, ), (1, b), (1, c), (2, ), (2, b), (2, c)\\}.\\]","code":""},{"path":"appendici.html","id":"cardinalità","chapter":"Appendici","heading":"Cardinalità","text":"Si definisce cardinalità (o potenza) di un insieme finito il numero\ndegli elementi dell’insieme. Viene indicata con \\(\\vert \\vert, \\#()\\) o\n\\(\\text{c}()\\).","code":""},{"path":"appendici.html","id":"propr_coef_min_quad","chapter":"Appendici","heading":"Proprietà degli stimatori dei minimi quadrati","text":"Il coefficiente dei minimi quadrati \\(b\\) è una combinazione lineare delle\nosservazioni \\(y_i\\). Tale proprietà è importante perché consente di\nderivare la distribuzione di \\(b\\) dalla distribuzione delle \\(y_i\\). Può\nessere dimostrato che la formula per il calcolo di \\(b\\) si può scrivere\nnel modo seguente: \\[\\begin{aligned}\nb &= \\sum_i \\left[\\frac{x_i-\\bar{x}}{\\sum_j(x_j-\\bar{x})^2}\\right]y_i = \\textstyle\\sum m_i y_i,\\end{aligned}\\]\ndove \\(m_i \\triangleq (x_i-\\bar{x}) / \\sum (x_j-\\bar{x})^2\\) è il peso\nassociato ciascun valore \\(y_i\\). Dato che valori \\(x_i\\) sono fissi e\n\\(m_i\\) dipende solo da \\(x_i\\), anche pesi \\(m_i\\) sono fissi.Il valore atteso di \\(b\\) è uguale \\[\\begin{aligned}\nE(b) &= \\textstyle\\sum m_i E(y_i)\\notag\\\\ \n&= \\textstyle\\sum m_i (\\alpha + \\beta x_i)\\notag\\\\ \n&= \\textstyle\\alpha\\sum m_i + \\beta \\sum m_i x_i\\notag\\\\\n&= \\frac{\\alpha \\sum(x_i-\\bar{x})}{\\sum(x_i-\\bar{x})^2} + \\beta \\frac{\\sum(x_i-\\bar{x})x_i}{\\sum(x_i-\\bar{x})^2}\\notag\\\\\n&= 0 + \\beta \\frac{\\sum x_i^2 -\\bar{x}\\sum x_i}{\\sum(x_i-\\bar{x})^2}\\notag\\\\ \n&= \\beta \\frac{\\sum x_i^2 - n\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\notag\\\\ \n&= \\beta.\\end{aligned}\\] Il coefficiente dei minimi quadrati \\(b\\) è\ndunque uno stimatore corretto di \\(\\beta\\). maniera equivalente si può\ndimostrare che \\(E() = \\alpha\\).Sotto le ipotesi di omoschedasticità\n\\(\\big[ \\var(y_i) = \\var(\\varepsilon_i)=\\sigma^2_{\\varepsilon}\\big]\\) e\nindipendenza, la varianza di \\(b\\) è \\[\\begin{aligned}\n\\var(b) &= \\textstyle\\var\\big(\\sum m_i y_i\\big)\\notag\\\\\n&= \\textstyle\\mathop{\\sum m_i^2} \\var(y_i)\\notag\\\\ \n&= \\textstyle\\mathop{\\sum m_i^2} \\sigma^2_{\\varepsilon}\\notag\\\\\n&= \\frac{\\mathop{\\sigma^2_{\\varepsilon}} \\textstyle\\sum(x_i-\\bar{x})^2}{\\big[\\textstyle\\sum(x_i-\\bar{x})^2\\big]^2}\\notag\\\\\n&= \\frac{\\sigma^2_{\\varepsilon}}{\\sum(x_i-\\bar{x})^2}.\\end{aligned}\\] \nmaniera simile si dimostra che la varianza di \\(\\) è\n\\[\\var()= \\frac{\\sigma^2_{\\varepsilon} \\textstyle\\sum x_i^2}{n \\textstyle\\sum (x_i-\\bar{x})^2}.\\]Dato che sia \\(\\) che \\(b\\) sono funzioni lineari di \\(y_i\\), se valori\n\\(y_i\\) seguono la distribuzione gaussiana, allora anche \\(\\) e \\(b\\) saranno\ndistribuiti secondo una distribuzione normale. conclusione,\n\\[\\begin{aligned}\nb &\\sim \\mathcal{N}\\bigg(\\beta,  \\frac{\\sigma^2_{\\varepsilon}}{\\sum(x_i-\\bar{x})^2}\\bigg),\\\\\n&\\sim \\mathcal{N}\\bigg(\\alpha, \\frac{\\sigma^2_{\\varepsilon}\\textstyle\\sum x_i^2}{n \\textstyle\\sum (x_i-\\bar{x})^2} \\bigg).\\end{aligned}\\]","code":""},{"path":"bibliografia.html","id":"bibliografia","chapter":"Bibliografia","heading":"Bibliografia","text":"","code":""}]
