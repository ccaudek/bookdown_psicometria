<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capitolo 13 Il calcolo delle probabilità | Data Science per psicologi</title>
<meta name="author" content="Corrado Caudek">
<<<<<<< HEAD
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.7/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
=======
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.7.3/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science per psicologi</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Benvenuti</a></li>
<li class="book-part">Obiettivi formativi</li>
<li><a class="" href="conoscenza-dichiarativa-e-imperativa.html"><span class="header-section-number">1</span> Conoscenza dichiarativa e imperativa</a></li>
<li class="book-part">Introduzione al linguaggio R</li>
<li><a class="" href="introduzione.html">Introduzione</a></li>
<li><a class="" href="chapter-pacchetti.html"><span class="header-section-number">2</span> Pacchetti</a></li>
<li><a class="" href="chapter-install-r.html"><span class="header-section-number">3</span> Per cominciare</a></li>
<li><a class="" href="chapter-sintassi.html"><span class="header-section-number">4</span> Sintassi di base</a></li>
<li><a class="" href="chapter-strutture-dati.html"><span class="header-section-number">5</span> Strutture di dati</a></li>
<li><a class="" href="chapter-strut-contr.html"><span class="header-section-number">6</span> Strutture di controllo</a></li>
<li><a class="" href="chapter-input-output.html"><span class="header-section-number">7</span> Input/Output</a></li>
<li><a class="" href="manipolazione-dei-dati.html"><span class="header-section-number">8</span> Manipolazione dei dati</a></li>
<li><a class="" href="flusso-di-lavoro-riproducibile.html"><span class="header-section-number">9</span> Flusso di lavoro riproducibile</a></li>
<li class="book-part">Statistica descrittiva ed analisi esplorativa dei dat̀i</li>
<li><a class="" href="introduzione-1.html">Introduzione</a></li>
<li><a class="" href="terminologia.html"><span class="header-section-number">10</span> Terminologia</a></li>
<li><a class="" href="chapter-misurazione.html"><span class="header-section-number">11</span> La misurazione in psicologia</a></li>
<li><a class="" href="chapter-descript.html"><span class="header-section-number">12</span> Statistica descrittiva</a></li>
<li class="book-part">Nozioni di base</li>
<li><a class="" href="introduzione-2.html">Introduzione</a></li>
<li><a class="active" href="il-calcolo-delle-probabilit%C3%A0.html"><span class="header-section-number">13</span> Il calcolo delle probabilità</a></li>
<li><a class="" href="chapter-prob-cond.html"><span class="header-section-number">14</span> Probabilità condizionata</a></li>
<li><a class="" href="chapter-teo-bayes.html"><span class="header-section-number">15</span> Il teorema di Bayes</a></li>
<li><a class="" href="chapter-prob-congiunta.html"><span class="header-section-number">16</span> Probabilità congiunta</a></li>
<<<<<<< HEAD
<li><a class="" href="la-distribuzione-binomiale.html"><span class="header-section-number">17</span> La distribuzione binomiale</a></li>
<li><a class="" href="funzioni-di-densit%C3%A0-di-probabilit%C3%A0.html"><span class="header-section-number">18</span> Funzioni di densità di probabilità</a></li>
<li><a class="" href="la-funzione-di-verosimiglianza.html"><span class="header-section-number">19</span> La funzione di verosimiglianza</a></li>
<li class="book-part">Inferenza frequentista</li>
<li><a class="" href="introduzione-3.html">Introduzione</a></li>
<li><a class="" href="distribuzione-campionaria.html"><span class="header-section-number">20</span> Distribuzione campionaria</a></li>
<li><a class="" href="significativit%C3%A0-statistica.html"><span class="header-section-number">21</span> Significatività statistica</a></li>
<li><a class="" href="inferenza-sulle-medie.html"><span class="header-section-number">22</span> Inferenza sulle medie</a></li>
<li><a class="" href="critiche-e-difese.html"><span class="header-section-number">23</span> Critiche e difese</a></li>
<li class="book-part">Inferenza Bayesiana</li>
<li><a class="" href="introduzione-4.html">Introduzione</a></li>
<li><a class="" href="modellistica-bayesiana.html"><span class="header-section-number">24</span> Modellistica bayesiana</a></li>
<li><a class="" href="stima-della-funzione-a-posteriori.html"><span class="header-section-number">25</span> Stima della funzione a posteriori</a></li>
<li><a class="" href="sintesi-a-posteriori.html"><span class="header-section-number">26</span> Sintesi a posteriori</a></li>
<li><a class="" href="una-breve-introduzione-al-modello-di-regressione.html"><span class="header-section-number">27</span> Una breve introduzione al modello di regressione</a></li>
<li><a class="" href="il-modello-statistico-della-regressione-lineare.html"><span class="header-section-number">28</span> Il modello statistico della regressione lineare</a></li>
<li><a class="" href="inferenza-bayesiana.html"><span class="header-section-number">29</span> Inferenza Bayesiana</a></li>
=======
<li><a class="" href="valore-atteso-e-varianza-di-variabili-aleatorie-discrete.html"><span class="header-section-number">17</span> Valore atteso e varianza di variabili aleatorie discrete</a></li>
<li><a class="" href="covarianza-e-correlazione-di-variabili-aleatorie.html"><span class="header-section-number">18</span> Covarianza e correlazione di variabili aleatorie</a></li>
<li><a class="" href="la-distribuzione-binomiale.html"><span class="header-section-number">19</span> La distribuzione binomiale</a></li>
<li><a class="" href="funzioni-di-densit%C3%A0-di-probabilit%C3%A0.html"><span class="header-section-number">20</span> Funzioni di densità di probabilità</a></li>
<li><a class="" href="la-funzione-di-verosimiglianza.html"><span class="header-section-number">21</span> La funzione di verosimiglianza</a></li>
<li class="book-part">Inferenza frequentista</li>
<li><a class="" href="introduzione-3.html">Introduzione</a></li>
<li><a class="" href="distribuzione-campionaria.html"><span class="header-section-number">22</span> Distribuzione campionaria</a></li>
<li><a class="" href="significativit%C3%A0-statistica.html"><span class="header-section-number">23</span> Significatività statistica</a></li>
<li><a class="" href="inferenza-sulle-medie.html"><span class="header-section-number">24</span> Inferenza sulle medie</a></li>
<li><a class="" href="critiche-e-difese.html"><span class="header-section-number">25</span> Critiche e difese</a></li>
<li class="book-part">Inferenza Bayesiana</li>
<li><a class="" href="introduzione-4.html">Introduzione</a></li>
<li><a class="" href="modellistica-bayesiana.html"><span class="header-section-number">26</span> Modellistica bayesiana</a></li>
<li><a class="" href="stima-della-funzione-a-posteriori.html"><span class="header-section-number">27</span> Stima della funzione a posteriori</a></li>
<li><a class="" href="sintesi-a-posteriori.html"><span class="header-section-number">28</span> Sintesi a posteriori</a></li>
<li><a class="" href="una-breve-introduzione-al-modello-di-regressione.html"><span class="header-section-number">29</span> Una breve introduzione al modello di regressione</a></li>
<li><a class="" href="il-modello-statistico-della-regressione-lineare.html"><span class="header-section-number">30</span> Il modello statistico della regressione lineare</a></li>
<li><a class="" href="inferenza-bayesiana.html"><span class="header-section-number">31</span> Inferenza Bayesiana</a></li>
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
<li class="book-part">Informazioni generali</li>
<li><a class="" href="citazione.html">Citazione</a></li>
<li class="book-part">Appendici</li>
<li><a class="" href="un-piccolo-ripasso.html">Un piccolo ripasso</a></li>
<li><a class="" href="bibliografia.html">Bibliografia</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="il-calcolo-delle-probabilità" class="section level1" number="13">
<h1>
<span class="header-section-number">Capitolo 13</span> Il calcolo delle probabilità<a class="anchor" aria-label="anchor" href="#il-calcolo-delle-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h1>
<p>È normale fare delle congetture rispetto a ciò di cui non siamo sicuri.
Ma perché facciamo questo? Molto spesso perché, anche se sappiamo che le
nostre conoscenze sono incomplete, dobbiamo comunque prendere delle
decisioni. Ad esempio: “non so se tra qualche ora pioverà; devo o non
devo prendere l’ombrello?” In maniera simile, anche se uno psicologo non
sa in maniera certa quali sono i meccanismi che regolano i fenomeni
psicologi, deve comunque decidere tra diverse alternative. Per esempio,
deve fornire un parere, relativamente a chi, tra due genitori, sia più
adatto per ottenere l’affidamento del figlio in caso di divorzio, oppure
quale sia, in un caso specifico, l’approccio più efficace per il
trattamento dei disturbi dell’alimentazione. Ovviamente la qualità delle
congetture varia, così come varia la qualità delle decisioni che
prendiamo. La teoria delle probabilità ci fornisce gli strumenti per
prendere decisioni “razionali” in condizioni di incertezza, ovvero per
formulare le migliori congetture possibili.</p>
<p>La teoria delle probabilità ci consente di descrivere in maniera
quantitativa quei fenomeni che, pur essendo altamente variabili,
rivelano comunque una qualche coerenza a lungo termine. Il lancio
ripetuto di una moneta è uno di questi fenomeni. È anche l’esempio
tipico che viene usato per introdurre una discussione sulle probabilità.
Sapere se una moneta sia onesta o meno, o calcolare la probabilità di
ottenere testa un certo numero di volte può essere interessante nel
mondo delle scommesse, ma nella vita quotidiana non ci capita spesso di
lanciare una moneta per prendere una decisione. Allora perché ci
preoccupiamo di studiare le proprietà statistiche dei lanci di una
moneta? A questa domanda si può rispondere dicendo che l’esperimento
(chiamato “casuale”) che corrisponde al lancio di una moneta è il
surrogato di una molteplicità di eventi che, della vita reale, sono
molto importanti. Per esempio: qual è la probabilità di successo di un
intervento psicologico? Qual è la probabilità che un test per l’HIV dia
esito positivo in una persona che non ha l’HIV? Qual è la probabilità di
essere occupato entro un anno dalla laurea? I lanci di una moneta
costituiscono una rappresentazione generica di molteplici altri eventi
che hanno un grande significato nella nostra vita. Questa è la ragione
per cui studiamo le proprietà statistiche dei fenomeni aleatori usando
il lancio di una moneta quale esempio generico.</p>
<p>La discussione della teoria della probabilità è certamente l’argomento
più impegnativo affrontato in queste dispense. Fare uno sforzo di
comprensione per chiarire i concetti di base della teoria della
probabilità è però necessario per mettersi nelle condizioni di capire le
caratteristiche dell’inferenza statistica che verranno discusse in
seguito.</p>
<div id="probabilità-nel-linguaggio-naturale" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Probabilità nel linguaggio naturale<a class="anchor" aria-label="anchor" href="#probabilit%C3%A0-nel-linguaggio-naturale"><i class="fas fa-link"></i></a>
</h2>
<p>In un articolo pubblicato su Harward Business Review nel 2018,
Mauboussin e Mauboussin ci ricordano come, nel marzo del 1951, l’<em>Office
of National Estimates</em> della CIA pubblicò un documento che suggeriva che
un attacco sovietico alla Jugoslavia nel corso dell’anno fosse una
“seria possibilità.” Sherman Kent, un professore di storia a Yale che fu
chiamato a Washington, D.C. per dirigere l’<em>Office of National
Estimates</em>, espresse perplessità sull’esatto significato
dell’espressione “seria possibilità.” Lo interpretò nel senso che la
probabilità di un attacco era di circa il 65%. Ma quando chiese ai
membri del <em>Board of National Estimates</em> cosa ne pensassero, gli furono
riferite cifre che andavano dal 20% all’80%. Una gamma così ampia
rappresentava chiaramente un problema, poiché le implicazioni politiche
di quegli estremi erano nettamente diverse. Kent riconobbe che la
soluzione di tale problema era quella di usare i numeri per esprimere il
nostro grado di certezza, notando mestamente:</p>
<blockquote>
<p>Non abbiamo usato i numeri… e sembra chiaro che abbiamo abusando delle parole.</p>
</blockquote>
<p>Da allora non è cambiato molto. Ancora oggi le persone nel mondo della
politica, degli affari e nella vita quotidiana continuano a usare parole
vaghe per descrivere i possibili risultati degli eventi. Perché? Phil
Tetlock, professore di psicologia all’Università della Pennsylvania, che
ha studiato a fondo il fenomeno psicologico della previsione, suggerisce
che “una vaga verbosità conferisce sicurezza.” Quando usiamo una parola
per descrivere la probabilità di un evento incerto, cerchiamo di porci
nelle condizioni di non essere smentiti dopo che il risultato
dell’evento verrà rivelato. Se si verifica l’evento che abbiamo
previsto, è facile dire: “Ti avevo detto che probabilmente sarebbe
successo questo.” Se la nostra predizione fallisce, possiamo sempre
dire: “Ho solo detto che probabilmente sarebbe successo.” Parole così
ambigue non solo consentono all’oratore di evitare di essere smentito,
ma consentono anche al destinatario di interpretare il messaggio in modo
coerente con le sue nozioni preconcette. Ovviamente, da tale ambiguità
linguistica deriva una cattiva comunicazione. È dunque necessario
procedere in modo diverso nel linguaggio scientifico. Vedremo in questo
capitolo come sia possibile assegnare al termine “probabilità” un
significato preciso.</p>
</div>
<div id="probabilità-nel-linguaggio-scientifico" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Probabilità nel linguaggio scientifico<a class="anchor" aria-label="anchor" href="#probabilit%C3%A0-nel-linguaggio-scientifico"><i class="fas fa-link"></i></a>
</h2>
<p>La teoria della probabilità nasce nel 1654. Fu infatti in questa data
che Antoine Gombaud Cavalier De Méré, un nobile francese, nonché
accanito giocatore d’azzardo scrisse una lettera al suo amico Pascal per
cercare di comprendere il motivo delle sue continue perdite nel gioco
dei dadi. De Méré descrisse due diverse scommesse:</p>
<dl>
<dt>scommessa A</dt>
<dd>
<p>si lancia un dado per 4 volte di seguito e si vince se esce almeno
una volta il 6;</p>
</dd>
<dt>scommessa B</dt>
<dd>
<p>si lanciano due dadi per 24 volte di seguito e si vince se esce
almeno una volta il doppio 6.</p>
</dd>
</dl>
<p>Il cavaliere De Méré pose a Pascal il seguente quesito: le possibilità
di vittoria sono maggiori nella scommessa A o nella scommessa B? Il
problema di De Méré divenne un motivo di scambio epistolare tra Pascal e
Fermat, i due più grandi matematici del tempo, e viene considerato come
la motivazione iniziale dello sviluppo della teoria della probabilità.</p>
<p>Ma come può essere risolto il problema di De Méré? Una strategia
possibile è quella di seguire l’esempio di De Méré, ovvero, giocare
questo gioco molte volte. Così facendo, De Méré si rese conto che le
possibilità di vittoria erano leggermente migliori nel caso della
scommessa A.</p>
<p>Utilizzando una simulazione al computer possiamo facilmente giungere a
questa stessa conclusione senza perdere tutto il tempo che De Méré ha
dedicato a questa materia. Una simulazione al computer ci consente
infatti di ripetere il gioco di De Méré moltissime volte e di annotare
il risultato ottenuto ad ogni ripetizione del gioco. Vedremo in seguito
perché, utilizzando un computer, è possibile ottenere un risultato
diverso ogni volta che si ripete una certa operazione, in modo tale da
rappresentare il grado di casualità che si osserva quando si lancia di
un dado. Per ora ci limitiamo ad esaminare i risultati che vengono
prodotti in questo modo e che sono illustrati nella
figura <a href="il-calcolo-delle-probabilit%C3%A0.html#fig:demere">13.1</a>.</p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Game A: Throw a fair die at most four times, and win if you get a six.</span>
<span class="va">experiment_a</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span><span class="op">{</span>
  <span class="va">rolls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, size <span class="op">=</span> <span class="fl">4</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="va">condition</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">rolls</span> <span class="op">==</span> <span class="fl">6</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">condition</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Game B: Throw two fair dice at most twenty-four times, and win if you get a double-six.</span>
<span class="va">experiment_b</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span><span class="op">{</span>
  <span class="va">first.die</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, size <span class="op">=</span> <span class="fl">24</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="va">second.die</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, size <span class="op">=</span> <span class="fl">24</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="va">condition</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">first.die</span> <span class="op">==</span> <span class="va">second.die</span><span class="op">)</span> <span class="op">&amp;</span> <span class="op">(</span><span class="va">first.die</span> <span class="op">==</span> <span class="fl">6</span><span class="op">)</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">condition</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># number of replications</span>
<span class="va">nrep</span> <span class="op">&lt;-</span> <span class="fl">1e4</span>
<span class="co"># Play game A nrep times. We get a vector of nrep elements. Eeach element of </span>
<span class="co"># of the simsA vector is the outcome obtained by playing game A once: TRUE if</span>
<span class="co"># the output is a win, FALSE if the output of the game is a loss. Remember than</span>
<span class="co"># TRUE = 1 and FALSE = 0.</span>
<span class="va">sims_a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">nrep</span>, <span class="fu">experiment_a</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="co"># The proportion of wins in game A </span>
<span class="va">prop_wins_a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">sims_a</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">sims_a</span><span class="op">)</span>
<span class="va">prop_wins_a</span>
<span class="co">#&gt; [1] 0.5193</span>
<span class="co"># To plot the results, we compute the </span>
<span class="va">nwins_a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">sims_a</span><span class="op">)</span>
<span class="va">ntrials</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="va">nrep</span>

<span class="va">sims_b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">nrep</span>, <span class="fu">experiment_b</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="va">prop_wins_b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">sims_b</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">sims_b</span><span class="op">)</span> 
<span class="va">prop_wins_b</span>
<span class="co">#&gt; [1] 0.493</span>

<span class="va">nwins_b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">sims_b</span><span class="op">)</span>

<span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>
  n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">ntrials</span>, <span class="va">ntrials</span><span class="op">)</span>, 
  pwin <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">nwins_a</span><span class="op">/</span><span class="va">ntrials</span>, <span class="va">nwins_b</span><span class="op">/</span><span class="va">ntrials</span><span class="op">)</span>,
  game <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Scommessa A"</span>, <span class="st">"Scommessa B"</span><span class="op">)</span>, each <span class="op">=</span> <span class="va">nrep</span><span class="op">)</span>
<span class="op">)</span>

<span class="va">d</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">n</span>, y <span class="op">=</span> <span class="va">pwin</span>, col <span class="op">=</span> <span class="va">game</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">10</span>, <span class="fl">50</span>, <span class="fl">200</span>, <span class="fl">1000</span>, <span class="fl">3000</span>,  <span class="fl">10000</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.title <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_blank</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x<span class="op">=</span><span class="st">"Numero di ripetizioni del gioco di De Méré"</span>, 
    y<span class="op">=</span><span class="st">"Proporzione di vincite"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gray80"</span>, <span class="st">"skyblue"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span id="fig:demere"></span>
<img src="Data-Science-per-psicologi_files/figure-html/demere-1.png" alt="Risultati ottenuti da 10000 ripetizioni delle due scommesse di De Méré." width="80%"><p class="caption">
Figura 13.1: Risultati ottenuti da 10000 ripetizioni delle due scommesse di De Méré.
</p>
</div>
<p>La figura <a href="il-calcolo-delle-probabilit%C3%A0.html#fig:demere">13.1</a> riportata la proporzione di vittorie in funzione
del numero di ripetizioni di ciascuna scommessa e rivela che, a lungo
termine (ovvero, se consideriamo un grande numero di ripetizioni del
gioco di De Méré), la scommessa A risulta più conveniente della
scommessa B. Nel caso di 10000 ripetizioni del gioco di De Méré, la
proporzione di vittorie è risultata essere pari a 0.5182 per la
scommessa A e pari a 0.4909 per la scommessa B. Se ripetiamo la stessa
simulazione altre 10000 volte, otteniamo una proporzione di vittorie
uguale a 0.5180 per la scommessa A e a 0.4878 per la scommessa B.</p>
<p>Vedremo in questo capitolo come ciascuna di queste proporzioni possa
essere considerata come una <em>stima empirica</em> di ciò che chiamiamo
<em>probabilità</em>. Le proporzioni descritte sopra vengono sono delle “stime”
poiché approssimano il vero valore della probabilità; infatti, ripetendo
la simulazione due volte abbiamo ottenuto dei risultati leggermente
diversi. Ma allora qual è il “vero” valore della probabilità? Un modo
semplice per rispondere a questa domanda è quello di dire che,
utilizzando la procedura descritta sopra, il vero valore della
probabilità si otterrebbe se il gioco di De Méré venisse ripetuto
infinite volte. Ma ovviamente, per qualunque applicazione concreta, non
abbiamo bisogno di ripetere la simulazione infinite volte, in quanto un
grande numero di ripetizioni ci fornisce un’approssimazione sufficiente.</p>
<p>In conclusione, le considerazioni precedenti ci fanno capire che il
concetto di probabilità sia legato a quello di incertezza. La
probabilità può infatti essere definita come la quantificazione del
livello di “casualità” di un evento, laddove viene detto casuale ciò che
non è noto o non può essere predetto con certezza.</p>
</div>
<div id="terminologia-1" class="section level2" number="13.3">
<h2>
<span class="header-section-number">13.3</span> Terminologia<a class="anchor" aria-label="anchor" href="#terminologia-1"><i class="fas fa-link"></i></a>
</h2>
<p>Come qualsiasi altra branca della matematica, la teoria delle
probabilità fa uso di una specifica terminologia i cui concetti di base
sono descritti di seguito.</p>
<ul>
<li>Il calcolo delle probabilità si occupa di un generico <em>esperimento casuale</em>. Si dice <em>esperimento casuale</em> qualsiasi attività che produce un risultato osservabile. L’esecuzione di un esperimento casuale è chiamata <em>prova</em> dell’esperimento. Esempi sono: lanciare una moneta, lanciare un dado a 6 facce, provare un nuovo percorso per andare al lavoro per vedere se è più veloce di quello che usiamo di solito, o giocare al gioco di De Méré.</li>
<li>Il risultato (o esito) di una prova si indica con <span class="math inline">\(\omega\)</span> ed è detto <em>evento elementare</em>.</li>
<li>Prima che l’esperimento casuale venga eseguito non sappiamo quale esito verrà prodotto; dopo che l’esperimento casuale è stato eseguito, l’esito dell’esperimento si “cristallizza” nel risultato osservato.</li>
<li>Si dice <em>spazio campionario</em> <span class="math inline">\(\Omega\)</span> (probability space) l’insieme di tutti i possibili esiti di un esperimento casuale. Lo spazio campionario può essere finito, infinito o infinito numerabile. Eseguire un esperimento casuale significa scegliere in maniera casuale uno dei possibili eventi elementari dello spazio campionario.</li>
<li>Si dice <em>evento composto</em> (o non-elementare) un sottoinsieme dello spazio campionario, ovvero un insieme che può essere a sua volta scomposto in più eventi elementari. Per esempio, il numero 4 è un evento elementare dello spazio campionario finito <span class="math inline">\(\Omega = \{1, 2, 3, 4, 5, 6\}\)</span> che corrisponde all’esperimento casuale del lancio di un dado. L’evento composto <span class="math inline">\(A\)</span> “il risultato è pari” è <span class="math inline">\(A = \{2, 4, 6\}\)</span>.</li>
</ul>
</div>
<div id="le-diverse-definizioni-della-probabilità" class="section level2" number="13.4">
<h2>
<span class="header-section-number">13.4</span> Le diverse definizioni della probabilità<a class="anchor" aria-label="anchor" href="#le-diverse-definizioni-della-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h2>
<p>Ma, nello specifico, che cos’è la probabilità? A questa domanda si può rispondere in modi diversi.</p>
<div id="una-definizione-ingenua-della-probabilità" class="section level3" number="13.4.1">
<h3>
<span class="header-section-number">13.4.1</span> Una definizione ingenua della probabilità<a class="anchor" aria-label="anchor" href="#una-definizione-ingenua-della-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h3>
<p>Storicamente, la prima definizione della probabilità di un evento è
stata quella che richiede di contare il numero di modi nei quali un
evento può manifestarsi e di dividere tale numero per il numero totale
di eventi dello spazio campionario <span class="math inline">\(\Omega\)</span>.</p>

<div class="definition">
<span id="def:def-ing-prob" class="definition"><strong>Definizione 13.1  </strong></span>Dato uno spazio campionario finito, la definizione ingenua della probabilità dell’evento <span class="math inline">\(A\)</span> è
<span class="math display">\[
\begin{aligned}
P_{\text{ing}} = \frac{|A|}{|\Omega|}
= \frac{\text{numero eventi elementari favorevoli all'evento }A}{\text{numero totale  eventi elementari dello spazio campionario }\Omega}.\notag
\end{aligned}
\]</span>
</div>
<p>La definizione <a href="il-calcolo-delle-probabilit%C3%A0.html#def:def-ing-prob">13.1</a> rende chiaro che il calcolo delle probabilità richiede di contare il numero di modi in cui un evento può realizzarsi.
Per esempio, nell’esperimento casuale corrispondente al lancio di due dadi equilibrati, l’evento <span class="math inline">\(A\)</span> = “la somma dei due dati è 5” si può realizzare in 4 modi diversi: <span class="math inline">\(A = \{ (1, 4), (2, 3), (3, 2), (4, 1) \}\)</span>.
Contare il numero di modi in cui un evento può realizzarsi può essere semplice, nel caso di alcuni eventi (come il presente), oppure estremamente complesso, nel caso di altri eventi. In questo secondo caso, per contare il numero di modi in cui un evento può realizzarsi, al fine di calcolare la probabilità definita come indicato sopra, è necessario fare uso del calcolo combinatorio. In queste dispense ci accontenteremo di presentare alcune nozioni di base del calcolo combinatorio, ma non entreremo nei dettagli di questo argomento.</p>
</div>
<div id="una-definizione-non-ingenua-della-probabilità" class="section level3" number="13.4.2">
<h3>
<span class="header-section-number">13.4.2</span> Una definizione non ingenua della probabilità<a class="anchor" aria-label="anchor" href="#una-definizione-non-ingenua-della-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h3>
<p>Il calcolo combinatorio ci consente di contare il numero di casi nello spazio campionario e di applicare la definizione “ingenua” di probabilità descritta nella
definizione <a href="il-calcolo-delle-probabilit%C3%A0.html#def:def-ing-prob">13.1</a>. È però facile rendersi conto che tale definizione di probabilità ha un grosso problema: non può essere applicata al caso di uno spazio campionario infinito. Dobbiamo dunque trovare una definizione che risolva un tale problema.</p>
<p>L’attuale nozione matematica di probabilità ha impiegato diverse centinaia di anni per cristallizzarsi. Non cerca di rispondere a difficili domande filosofiche come “Che cos’è la casualità?” “Da dove viene?” “Qual è il significato della probabilità nel mondo reale?” ecc., ma ci offre un modello matematico che si rivela estremamente efficace nella modellazione dei fenomeni del mondo reale.</p>
<p>Nella maggior parte dei trattamenti matematici contemporanei della probabilità, la nozione di base è quella uno <em>spazio di probabilità</em>, che è un modello matematico di un esperimento casuale. Esempi di un esperimento casuale sono tre lanci successivi di una moneta equilibrata o la scelta di un punto casuale in un quadrato di area unitaria.</p>
<p>Uno spazio di probabilità è una tripla (<span class="math inline">\(\Omega\)</span>, <span class="math inline">\(\mathcal{F}\)</span>, <span class="math inline">\(P\)</span>). La prima componente viene chiamata <em>spazio campionario</em> ed è un insieme costituito da tutti i possibili risultati dell’esperimento casuale. Ogni elemento <span class="math inline">\(\omega \in \Omega\)</span> è chiamato <em>evento elementare</em>. Per l’esempio dei tre lanci di una moneta, lo spazio campionario consiste di tutte le possibili sequenze di tre lettere costituite da H (testa) e T (croce): <span class="math inline">\(\Omega\)</span> = {HHH, HHT, …, TTT}. Per l’esempio della scelta di un punto casuale in un quadrato, lo spazio campionario è costituito da tutti i punti contenuti nell’area del quadrato.</p>
<p>La seconda componente <span class="math inline">\(\mathcal{F}\)</span> di uno spazio di probabilità è un insieme di sottoinsiemi di <span class="math inline">\(\Omega\)</span>. Ogni insieme <span class="math inline">\(E \in \mathcal{F}\)</span> è chiamata evento. Un esempio concreto di evento nell’esperimento dei tre lanci di una moneta è “numero dispari di risultati croce,” ovvero <span class="math inline">\(E = {HHT, HTH, THH, TTT}\)</span>. Nell’esempio della scelta di un punto casuale in un quadrato, tutte le figure geometriche all’interno del quadrato sono degli eventi.</p>
<p>L’ultimo componente <span class="math inline">\(P\)</span> di uno spazio di probabilità è una funzione che
assegna un numero reale <span class="math inline">\(P(E)\)</span>, chiamato probabilità di <span class="math inline">\(E\)</span>, a ogni
evento <span class="math inline">\(E \in \mathcal{F}\)</span>. Nell’esempio dei tre lanci di una moneta, consideriamo tutti gli eventi elementari egualmente probabili, e la probabilità di un evento <span class="math inline">\(E\)</span> è definita come il rapporto tra il numero degli eventi elementari in <span class="math inline">\(E\)</span> e il numero degli eventi elementari nello spazio campionario. Nell’esempio della scelta di un punto casuale in un quadrato, la probabilità dell’evento <span class="math inline">\(E\)</span> che corrisponde ad una qualche figura geometrica inscritta nel quadrato è data dal rapporto tra l’area di tale figura geometrica e l’area totale del quadrato.</p>
<p>La tripla (<span class="math inline">\(\Omega\)</span>, <span class="math inline">\(\mathcal{F}\)</span>, <span class="math inline">\(P\)</span>) deve soddisfare i seguenti assiomi, che
furono presentati per la prima volta in questa forma da Kolmogorov negli anni ’30:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(P(E) ≥ 0\)</span> per ciascun <span class="math inline">\(E \in \mathcal{F}\)</span>.</li>
<li>
<span class="math inline">\(P(\Omega) = 1\)</span> (l’esperimento produce sempre qualche risultato).</li>
<li>
<span class="math inline">\(P\)</span> se <span class="math inline">\(E_1, E_2, \dots\)</span> è una sequenza di eventi mutuamente disgiunti, ne segue che
<span class="math display">\[
P\left(\bigcup\limits_{i=1}^{\infty} E_i \right) = \sum_{i=1}^\infty P(E_i)
\]</span>
</li>
</ol>
<p>Gli assiomi (1) e (2) sono molto intuitivi, così come l’additività nel caso di un numero finito di eventi. L’additività numerabile non può essere supportata dall’intuizione, ma è uno strumento fondamentale nella teoria della probabilità e della statistica.</p>
<p>La precedente definizione di uno spazio di probabilità corrisponde al cosiddetto <em>approccio assiomatico</em> messo a punto da Kolmogorov intorno al 1930, il quale è alla base della moderna teoria della probabilità. Gli assiomi di Kolmogorov sono necessari per evitare i paradossi che si possono creare quando si manipolano gli insiemi – ad esempio, l’utilizzo dell’“l’insieme di tutti gli insiemi” tipicamente conduce ad un paradosso.</p>
</div>
</div>
<div id="assegnare-le-probabilità-agli-eventi" class="section level2" number="13.5">
<h2>
<span class="header-section-number">13.5</span> Assegnare le probabilità agli eventi<a class="anchor" aria-label="anchor" href="#assegnare-le-probabilit%C3%A0-agli-eventi"><i class="fas fa-link"></i></a>
</h2>
<p>È importante capire che l’approccio assiomatico non ci dice però come sia possibile assegnare un valore di probabilità a un evento <span class="math inline">\(E \in \Omega\)</span>. A questo proposito esistono due diverse scuole di pensiero.</p>
<div id="approccio-frequentista" class="section level3" number="13.5.1">
<h3>
<span class="header-section-number">13.5.1</span> Approccio frequentista<a class="anchor" aria-label="anchor" href="#approccio-frequentista"><i class="fas fa-link"></i></a>
</h3>
<p>Una prima possibilità è di definire la nozione di probabilità in termini
empirici. La probabilità di un evento <span class="math inline">\(A\)</span> può essere concepita come il
limite cui tende la frequenza relativa dell’evento, al tendere
all’infinito del numero delle prove effettuate, ossia
<span class="math display">\[\begin{equation}
P_A = \lim_{n \to \infty} \frac{n_A}{n}.
\end{equation}\]</span>
Questo è l’approccio che abbiamo utilizzato in precedenza, quando abbiamo discusso il gioco di De Méré.</p>
<p>Tale definizione assume che l’esperimento possa essere ripetuto più
volte, idealmente infinite volte, sotto le medesime condizioni, e
corrisponde alla definizione <em>frequentista</em> di probabilità. Per
l’approccio frequentista, dire che la probabilità di ottenere testa è
0.5 significa affermare che l’evento “testa” verrebbe ottenuto nel 50%
dei casi, se ripetessimo tantissime volte l’esperimento casuale del
lancio di una moneta.</p>
<p>Se non abbiamo a disposizione informazioni empiriche a proposito del
verificarsi di un evento possiamo attribuire le probabilità agli eventi
usando la nostra conoscenza della situazione. Tale approccio è seguito
dalla definizione <em>classica</em> di probabilità in base alla quale la
probabilità di un evento è il rapporto tra il numero di casi favorevoli
e quelli possibili, supposto che tutti gli eventi siano equiprobabili,
ossia
<span class="math display">\[
P_A = \frac{n_A}{n},
\]</span>
dove <span class="math inline">\(n\)</span> è il numero di casi possibili e <span class="math inline">\(n_A\)</span> è il numero di casi favorevoli per l’evento <span class="math inline">\(A\)</span>. L’assunzione di equiprobabilità degli eventi elementari ha senso soprattutto nel caso dei giochi d’azzardo.</p>
<p>In base all’approccio frequentista, la probabilità è il limite a cui
tende una frequenza relativa empirica al crescere del numero di
ripetizioni dell’esperimento casuale. È molto facile utilizzare   per
calcolare una tale probabilità. Per esempio, se vogliamo calcolare la
probabilità di ottenere 3 nel lancio di un dado equilibrato, possiamo
eseguire la seguente simulazione.</p>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1e5</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="va">n</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">x_01</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">==</span> <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x_01</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.1676</span></code></pre></div>
<p>Il risultato è ovviamente molto simile a <span class="math inline">\(1/6\)</span>.</p>
</div>
<div id="approccio-bayesiano" class="section level3" number="13.5.2">
<h3>
<span class="header-section-number">13.5.2</span> Approccio Bayesiano<a class="anchor" aria-label="anchor" href="#approccio-bayesiano"><i class="fas fa-link"></i></a>
</h3>
<p>Esistono però degli eventi per i quali non è possibile calcolare una frequenza relativa, ovvero quelli che si verificano una volta soltanto. Che cos’è allora la probabilità in questi casi? In base all’approccio Bayesiano la probabilità è una misura del grado di plausibilità di una proposizione. Questa definizione è applicabile a qualsiasi evento. Ciò consente di assegnare una probabilità anche a proposizioni quali “il candidato <span class="math inline">\(A\)</span> vincerà le elezioni” oppure “l’accusato è innocente,” anche se non è possibile ripetere più volte un’elezione o un evento criminoso.</p>
<p>Per assegnare le probabilità agli eventi, nell’approccio Bayesiano si
utilizzano considerazioni “soggettive” che derivano dalle informazioni
di cui il soggetto è in possesso. Il teorema di Bayes consente di
aggiustare, alla luce dei dati osservati, tali credenze “a priori” per
arrivare alla probabilità a posteriori. Quindi, tramite l’approccio
Bayesiano, si usa una stima del grado di plausibilità di una
proposizione prima dell’osservazione dei dati, al fine di associare un
valore numerico al grado di plausibilità di quella stessa proposizione
successivamente all’osservazione dei dati. Questo processo di
“aggiornamento Bayesiano” corrisponde all’inferenza statistica e verrà
discusso in dettaglio nel seguito delle dispense.</p>
</div>
</div>
<div id="proprietà-elementari-della-probabilità" class="section level2" number="13.6">
<h2>
<span class="header-section-number">13.6</span> Proprietà elementari della probabilità<a class="anchor" aria-label="anchor" href="#propriet%C3%A0-elementari-della-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h2>
<p>Indipendentemente da come decidiamo di interpretare la probabilità (in
termini frequentisti o Bayesiani), alla probabilità possono essere
assegnate le seguenti proprietà.</p>
<ol style="list-style-type: decimal">
<li><p>La probabilità dell’evento impossibile è zero:
<span class="math display">\[P(\emptyset) = 1 - P(\Omega) = 0.\]</span></p></li>
<li><p>Se consideriamo due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> tali che <span class="math inline">\(A \subseteq B\)</span>, cioè
che <span class="math inline">\(A\)</span> è contenuto o coincidente con <span class="math inline">\(B\)</span>, da ciò segue che
<span class="math display">\[P(A) \leq P(B).\]</span></p></li>
<li><p>Se <span class="math inline">\(A^c\)</span> è il complementare dell’evento <span class="math inline">\(A\)</span>, allora
<span class="math display">\[P(A^c) = 1 - P(A).\]</span></p></li>
<li><p>Dati <span class="math inline">\(n\)</span> eventi <span class="math inline">\(A_i\)</span> per <span class="math inline">\(i= 1, \cdots, n\)</span>, gli eventi si dicono
<em>indipendenti</em> se risulta
<span class="math display">\[P(A_i \cap A_j \cap \cdots \cap A_k) = P(A_i) P(A_j) \cdots P(A_k).\]</span></p></li>
<li><p>Se due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> non sono disgiunti, allora quando sommiamo
le loro probabilità dobbiamo evitare che la loro parte comune
<span class="math inline">\(A \cap B\)</span> venga contata due volte. Dati due eventi non
necessariamente disgiunti, dunque, la probabilità dell’unione è pari
alla somma delle singole probabilità dei due eventi meno la
probabilità dell’intersezione:
<span class="math display" id="eq:probunione">\[\begin{equation}
P(A \text{ o } B) = P(A \cup B) = P(A) + P(B) - P(A \cap B).
\tag{13.1}
\end{equation}\]</span></p></li>
</ol>
<div class="exercise">
<<<<<<< HEAD
<span id="exr:ex-parlamento-londra" class="exercise"><strong>Exercizio 13.1  </strong></span>Nel 2012, a 97 deputati al Parlamento di Londra è stato chiesto: “Se lanci una moneta due volte, qual è la probabilità di ottenere due volte testa?” La maggioranza, 60 su 97, non ha saputo dare la risposta corretta. Come possiamo dare a questo problema una risposta migliore di quella fornita da questi politici?
=======
<span id="exr:ex-parlamento-londra" class="exercise"><strong>Esercizio 13.1  </strong></span>Nel 2012, a 97 deputati al Parlamento di Londra è stato chiesto: “Se lanci una moneta due volte, qual è la probabilità di ottenere due volte testa?” La maggioranza, 60 su 97, non ha saputo dare la risposta corretta. Come possiamo dare a questo problema una risposta migliore di quella fornita da questi politici?
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
</div>

<div class="solution">
 <span class="solution"><em>Soluzione. </em></span> In base alla regola 4 elencata sopra, la risposta corretta è <span class="math inline">\(0.5 \times 0.5 = 0.25\)</span>.
</div>

<div class="exercise">
<<<<<<< HEAD
<span id="exr:ex-urna-10b-10r-10g" class="exercise"><strong>Exercizio 13.2  </strong></span>Un’urna contiene <span class="math inline">\(30\)</span> palline: <span class="math inline">\(10\)</span> bianche numerate da <span class="math inline">\(1\)</span> a <span class="math inline">\(10\)</span>, <span class="math inline">\(10\)</span>
=======
<span id="exr:ex-urna-10b-10r-10g" class="exercise"><strong>Esercizio 13.2  </strong></span>Un’urna contiene <span class="math inline">\(30\)</span> palline: <span class="math inline">\(10\)</span> bianche numerate da <span class="math inline">\(1\)</span> a <span class="math inline">\(10\)</span>, <span class="math inline">\(10\)</span>
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
rosse e <span class="math inline">\(10\)</span> gialle numerate allo stesso modo. Qual è la probabilità
che, estraendo una pallina a caso, venga estratta una pallina gialla o
una pallina pari?
</div>

<div class="solution">
 <span class="solution"><em>Soluzione. </em></span> Il numero totale di palline è <span class="math inline">\(30\)</span>. La probabilità che venga estratta
una gialla è <span class="math inline">\(P(G) = \frac{10}{30} = \frac{1}{3}\)</span>. Le palline con numero
pari sono <span class="math inline">\(5\)</span> per ogni colore, quindi <span class="math inline">\(15\)</span>. La probabilità che venga
estratto un numero pari è <span class="math inline">\(P(P) = \frac{15}{30} = \frac{1}{2}\)</span>. Gli
eventi sono compatibili: i casi favorevoli a entrambi gli eventi
(pallina gialla e pari) sono <span class="math inline">\(5\)</span>. La probabilità dell’evento cercato è
dunque
<span class="math inline">\(P(\text{gialla} \cup \text{pari}) = \frac{1}{3} + \frac{1}{2} - \frac{5}{30} = \frac{2}{3}\)</span>.
</div>
</div>
<div id="variabili-aleatorie" class="section level2" number="13.7">
<h2>
<span class="header-section-number">13.7</span> Variabili aleatorie<a class="anchor" aria-label="anchor" href="#variabili-aleatorie"><i class="fas fa-link"></i></a>
</h2>
<p>Tutte le nozioni che abbiamo discusso in precedenza sono necessarie per potere definire il concetto di “variabile aleatoria.” Le variabili aleatorie sono un concetto fondamentale della teoria statistica e delle sue applicazioni. Infatti, le variabili aleatorie sono lo strumento che usiamo per valutare, per esempio, l’efficacia di un intervento psicologico. Un intervento psicologico, infatti, può essere concepito come un “esperimento casuale” e le variabili aleatorie ci consentono di riassumere i risultati di un esperimento casuale e di quantificare il grado di certezza che possiamo assegnare all’esito osservato, nel contesto di tutti gli esiti possibili che, in linea di principio, sarebbe stato possibile osservare. Il significato di “variabile aleatoria” è semplice; meno semplice è capire come manipolare le variabili aleatorie. Ma iniziamo con una definizione.</p>

<div class="definition">
<span id="def:def-var-aleatoria" class="definition"><strong>Definizione 13.2  </strong></span>Una variabile aleatoria è una funzione sullo spazio campionario <span class="math inline">\(\Omega\)</span>
che associa ad ogni evento elementare <span class="math inline">\(\omega_i\)</span> un unico numero
<span class="math inline">\(X(\omega_i) = x_i\)</span>, ovvero <span class="math inline">\(X: \Omega \rightarrow \Re\)</span>.
</div>
<p>Il dominio della variabile aleatoria <span class="math inline">\(X\)</span> (che è una funzione) è dato dai
punti dello spazio campionario <span class="math inline">\(\Omega\)</span>. Ad ogni evento elementare
<span class="math inline">\(\omega_i\)</span> attribuiamo il numero <span class="math inline">\(X(\omega_i)\)</span>, ovvero il valore che la
variabile aleatoria assume sul risultato <span class="math inline">\(\omega_i\)</span> dell’esperimento
casuale. L’attributo “aleatoria” si riferisce al fatto che la variabile
considerata trae origine da un esperimento casuale di cui non siamo in grado di
prevedere l’esito con certezza.</p>
<p>Mediante una variabile aleatoria trasformiamo lo spazio campionario
<span class="math inline">\(\Omega\)</span>, che in genere è complesso, in uno spazio campionario più
semplice formato da un insieme di numeri. Il maggior vantaggio di questa
sostituzione è che molte variabili aleatorie, definite su spazi
campionari anche molto diversi tra loro, danno luogo ad una stessa
“distribuzione” di probabilità sull’asse reale. Le variabili aleatorie
si indicano con le lettere maiuscole ed i valori da esse assunti con le
lettere minuscole.</p>
<p>Ci sono due classi di variabili aleatorie: variabili aleatorie discrete
e variabili aleatorie continue. Consideriamo innanzitutto il caso delle
variabili aleatorie discrete.</p>

<div class="definition">
<span id="def:def-var-aleatoria-discr" class="definition"><strong>Definizione 13.3  </strong></span>Una variabile aleatoria <span class="math inline">\(X\)</span> viene detta discreta se può assumere un
insieme discreto (finito o numerabile) di numeri reali.
</div>
<p>Se <span class="math inline">\(X\)</span> è una variabile aleatoria discreta allora l’insieme dei possibili
valori <span class="math inline">\(x\)</span>, tali per cui <span class="math inline">\(P(X = x) &gt; 0\)</span>, viene detto “supporto” di <span class="math inline">\(X\)</span>.</p>
<p>Alcuni esempi di variabili aleatorie discrete sono i seguenti: il numero
di intrusioni di pensieri, immagini, impulsi indesiderabili in un
paziente OCD, il voto all’esame di Psicometria, la durata di vita di un
individuo, il numero dei punti che si osservano nel lancio di due dadi e
il guadagno (la perdita) che un giocatore realizzerà in <span class="math inline">\(n\)</span> partite. Si
noti che, in tutti questi casi, la variabile aleatoria considerata viene
rappresentata mediante un numero.</p>
<div id="a-cosa-servono-le-variabili-aleatorie" class="section level3" number="13.7.1">
<h3>
<span class="header-section-number">13.7.1</span> A cosa servono le variabili aleatorie?<a class="anchor" aria-label="anchor" href="#a-cosa-servono-le-variabili-aleatorie"><i class="fas fa-link"></i></a>
</h3>
<p>Facendo riferimento agli esempi elencati sopra, possiamo chiederci
perché questi numeri vengono considerati come “aleatori.” È ovvio che
noi non conosciamo, ad esempio, il voto di Psicometria di Mario Rossi
prima del momento in cui Mario Rossi avrà fatto l’esame. Le variabili
aleatorie si pongono il seguente problema: come possiamo descrivere le
nostre opinioni rispetto al voto (possibile) di Mario Rossi, prima che
lui abbia fatto l’esame. Prima dell’esame, il voto di Psicometria di
Mario Rossi si può solo descrivere facendo riferimento ad un insieme di
valori possibili. Inoltre, molto spesso, possiamo anche dire che tali
valori possibili non sono tutti egualmente verosimili: ci aspettiamo di
osservare più spesso alcuni di questi valori rispetto agli altri. Le
proprietà delle variabili aleatorie ci consentono di sistematizzare
questo tipo di opinioni. Ovviamente, una volta che Mario Rossi avrà
fatto l’esame, questa materia non avrà più alcuna componente aleatoria.</p>
</div>
<div id="funzione-di-massa-di-probabilità" class="section level3" number="13.7.2">
<h3>
<span class="header-section-number">13.7.2</span> Funzione di massa di probabilità<a class="anchor" aria-label="anchor" href="#funzione-di-massa-di-probabilit%C3%A0"><i class="fas fa-link"></i></a>
</h3>
<p>Per entrare nel merito di questa discussione, chiediamoci ora come sia
possibile associare delle probabilità ai valori che vengono assunti
dalle variabili aleatorie. Ad esempio, qual è la probabilità che Mario
Rossi ottenga 29 all’esame? Ci occuperemo qui del caso delle variabili
aleatorie discrete.</p>
<p>Alle variabili aleatorie discrete vengono assegnale le probabilità
mediante le cosiddette “distribuzioni di probabilità.” Una distribuzione
di probabilità è un modello matematico che collega ciascun valore di una
variabile aleatoria discreta alla probabilità di osservare un tale
valore in un esperimento casuale. In pratica, ad ognuno dei valori che
possono essere assunti da una variabile aleatoria discreta viene
associata una determinata probabilità. La funzione che associa ad ogni
valore della variabile aleatoria una probabilità corrispondente si
chiama “distribuzione di probabilità” oppure “legge di probabilità.”</p>
<p>Una descrizione intuitiva del concetto di distribuzione di probabilità
può essere formulata nei termini seguenti. Possiamo pensare alla
probabilità come ad una quantità positiva che viene “distribuita”
sull’insieme dei valori della variabile aleatoria. Tale “distribuzione”
(suddivisione, spartizione) viene scalata in maniera tale che ciascun
elemento di essa corrisponda ad una proporzione del totale, nel senso
che il valore totale della distribuzione è sempre pari a 1. Una
distribuzione di probabilità non è dunque altro che un modo per
suddividere la nostra certezza (cioè 1) tra i valori che la variabile
aleatoria può assumere. In modo più formale, possiamo dire quanto segue.</p>

<div class="definition">
<span id="def:def-distr-prob-va-discr" class="definition"><strong>Definizione 13.4  </strong></span>Se <span class="math inline">\(X\)</span> è una variabile aleatoria discreta, una distribuzione di
probabilità può essere rappresentata mediante una funzione di massa di
probabilità che associa a ciascuno dei valori <span class="math inline">\(x\)</span> che la variabile
aleatoria <span class="math inline">\(X\)</span> può assumere la corrispondente probabilità <span class="math inline">\(P_{\pi}(X=x)\)</span>.
</div>
<p>In maniera più semplice, una distribuzione di (massa) di probabilità è
formata dall’elenco di tutti i valori possibili di una variabile
aleatoria discreta e dalle probabilità loro associate. Si noti che
<span class="math inline">\(P_{\pi}(X=x)\)</span> è un numero positivo se il valore <span class="math inline">\(x\)</span> è compreso nel
supporto di <span class="math inline">\(X\)</span>, altrimenti vale 0.</p>
<p>Se <span class="math inline">\(A\)</span> è un sottoinsieme della variabile aleatoria <span class="math inline">\(X\)</span>, allora denotiamo
con <span class="math inline">\(P_{\pi}(A)\)</span> la probabilità assegnata ad <span class="math inline">\(A\)</span> dalla distribuzione
<span class="math inline">\(P_{\pi}\)</span>. Mediante una distribuzione di probabilità <span class="math inline">\(P_{\pi}\)</span> è
possibile determinare la probabilità di ciascun sottoinsieme
<span class="math inline">\(A \subset X\)</span> come <span class="math display">\[P_{\pi}(A) = \sum_{x \in A} P_{\pi}(x).\]</span> Qui non
facciamo altro che applicare il terzo assioma di Kolmogorov.</p>

<div class="exercise">
<<<<<<< HEAD
<span id="exr:ex-va-discr-2dice" class="exercise"><strong>Exercizio 13.3  </strong></span>Consideriamo nuovamente lo spazio campionario <span class="math inline">\(\Omega\)</span> dell’esercizio precedente e definiamo la variabile aleatoria <span class="math inline">\(S(\omega)\)</span> come la somma dei puntini che si ottengono dal lancio di due dadi. Per esempio, <span class="math inline">\(S(\{(6, 3)\}) = 6 + 3 = 9\)</span>. Iniziamo a chiederci qual è la probabilità dell’evento <span class="math inline">\(S = 7\)</span>.
</div>

<div class="solution">
<p> <span class="solution"><em>Soluzione. </em></span> Per risolvere tale problema iniziamo a considerare il fatto che l’evento <span class="math inline">\(S = 7\)</span> si verifica in corrispondenza di sei punti elementari dello spazio campionario <span class="math inline">\(\Omega\)</span>: {(1, 6), (2, 5), (3, 4), (4, 3), (2, 5), (6, 1)}. Dunque,
<span class="math display">\[\begin{equation}
P(S = 7) = P\{(1, 6)\} + P\{(2, 5)\} + P\{(3, 4)\} + P\{(4, 3)\} + P\{(2, 5)\} + P\{(6, 1)\}.
=======
<span id="exr:ex-va-discr-2dice" class="exercise"><strong>Esercizio 13.3  </strong></span>Consideriamo nuovamente lo spazio campionario <span class="math inline">\(\Omega\)</span> dell’esercizio precedente e definiamo la variabile aleatoria <span class="math inline">\(S(\omega)\)</span> come la somma dei puntini che si ottengono dal lancio di due dadi. Per esempio, <span class="math inline">\(S(\{(6, 3)\}) = 6 + 3 = 9\)</span>. Iniziamo a chiederci qual è la probabilità dell’evento <span class="math inline">\(S = 7\)</span>.
</div>

<div class="solution">
<p> <span class="solution"><em>Soluzione. </em></span> Per risolvere tale problema notiamo che l’evento <span class="math inline">\(S = 7\)</span> si verifica in corrispondenza di sei punti elementari dello spazio campionario <span class="math inline">\(\Omega\)</span>: {(1, 6), (2, 5), (3, 4), (4, 3), (2, 5), (6, 1)}. Dunque,
<span class="math display">\[\begin{equation}
P(S = 7) = \\
P\{(1, 6)\} + P\{(2, 5)\} + P\{(3, 4)\} + P\{(4, 3)\} + P\{(2, 5)\} + P\{(6, 1)\}.
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
\end{equation}\]</span>
Se possiamo assumere che i due dadi sono bilanciati, allora ciascun evento elementare dello spazio campionario ha probabilità <span class="math inline">\(\frac{1}{36}\)</span> e la probabilità cercata diventa <span class="math inline">\(\frac{1}{6}\)</span>. È facile estendere il ragionamento fatto sopra a tutti i valori che <span class="math inline">\(S\)</span> può assumere. In questo modo giungiamo alla funzione di massa di probabilità <span class="math inline">\(P_0\)</span> riportata nella prima riga della tabella seguente.</p>
<div id="tab:massa_prob_due_dadi_on_tr">
<div class="inline-table"><table class="table table-sm">
<caption>Distribuzione di massa di probabilità per la somma dei punti
prodotti dal lancio di due dadi bilanciati (<span class="math inline">\(P_0\)</span>) e di due dadi
truccati (<span class="math inline">\(P_1\)</span>).</caption>
<thead><tr class="header">
<th align="center">s</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
<th align="center">11</th>
<th align="center">12</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P_0(S = s)\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{6}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(P_1(S = s)\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{6}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{7}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{12}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{7}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{6}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{64}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{64}\)</span></td>
</tr>
</tbody>
</table></div>
</div>
<p>Per considerare un caso più generale, poniamoci ora il problema di
trovare la funzione di massa di probabilità di <span class="math inline">\(S\)</span> nel caso di due dadi
truccati aventi la seguente distribuzione di probabilità:
<span class="math display">\[
\begin{aligned}
P(\{1\}) = P(\{6\}) &amp;= \frac{1}{4};\notag\\
<<<<<<< HEAD
P(\{2\}) = P(\{3\}) = P(\{4\}) = P(\{5\}) = \frac{1}{8}\notag.
=======
P(\{2\}) = P(\{3\}) = P(\{4\}) = P(\{5\}) &amp;= \frac{1}{8}\notag.
>>>>>>> e415d868dca882176aeed1dfdb870336d1b10e3f
\label{eq:loaded_dice}
\end{aligned}
\]</span>
Nel caso dei due dadi truccati, la probabilità dell’evento elementare (1, 1) è 1/4 1/4. Dunque, <em>P</em>(<em>S</em> = 2) = 4/64. La probabilità dell’evento elementare (1, 2) è 1/4 1/8. Tale valore è uguale alla probabilità dell’evento elementare (2, 1). La probabilità che <em>S</em> sia uguale a 3 è 1/4 1/8 + 1/8 1/4 = 4/64, e così via. Svolgendo i calcoli per tutti i possibili valori di <em>S</em> otteniamo la funzione di massa di probabilità <span class="math inline">\(P_1\)</span> riportata nella seconda riga della tabella precedente.</p>
Si noti che, a partire dalla funzione di massa di probabilità di <em>S</em>, è possibile calcolare la probabilità di altri eventi. Per esempio, possiamo dire che l’evento <em>S</em> &gt; 10 ha una probabilità minore nel caso dei dadi bilanciati, ovvero 3/36 = 1/12, rispetto al caso dei dadi truccati considerati in precedenza dove, per lo stesso evento, abbiamo una probabilità di 8/64 = 1/8.
</div>
</div>
</div>
<div id="notazione" class="section level2" number="13.8">
<h2>
<span class="header-section-number">13.8</span> Notazione<a class="anchor" aria-label="anchor" href="#notazione"><i class="fas fa-link"></i></a>
</h2>
<p>Qui sotto è riportata la notazione che verrà usata per fare riferimento
ad eventi e probabilità, nel caso discreto e continuo, in maniera tale
che queste convenzioni siano elencate tutte in un posto solo.</p>
<ul>
<li>Gli eventi sono denotati da lettere maiuscole, es. <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>.</li>
<li>Una variabile aleatoria è denotata da una lettera maiuscola, ad
esempio <span class="math inline">\(X\)</span>, e assume valori denotati dalla stessa lettera
minuscola, ad esempio <span class="math inline">\(x\)</span>.</li>
<li>La connessione tra eventi e valori viene espressa nei termini
seguenti: “<span class="math inline">\(X = x\)</span>” significa che l’evento <span class="math inline">\(X\)</span> assume il valore <span class="math inline">\(x\)</span>.</li>
<li>La probabilità di un evento è denotata con <span class="math inline">\(P(A)\)</span>.</li>
<li>Una variabile aleatoria discreta ha una funzione di massa di
probabilità denotata con <span class="math inline">\(p(x)\)</span>. La relazione tra <span class="math inline">\(P\)</span> e <span class="math inline">\(p\)</span> è che
<span class="math inline">\(P(X=x) = p(x)\)</span>.</li>
</ul>
</div>
<div id="conclusioni-2" class="section level2 unnumbered">
<h2>Conclusioni<a class="anchor" aria-label="anchor" href="#conclusioni-2"><i class="fas fa-link"></i></a>
</h2>
<p>In questo capitolo abbiamo visto come si costruisce lo spazio
campionario di un esperimento casuale, quali sono le proprietà di base
della probabilità e come si assegnano le probabilità agli eventi
definiti sopra uno spazio campionario discreto. Abbiamo anche introdotto
le nozioni di “variabile aleatoria” e di “funzione di massa di
probabilità.” Le procedure di analisi dei dati psicologici che
discuteremo in seguito faranno un grande uso di questi concetti e della
notazione qui introdotta.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="introduzione-2.html">Introduzione</a></div>
<div class="next"><a href="chapter-prob-cond.html"><span class="header-section-number">14</span> Probabilità condizionata</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#il-calcolo-delle-probabilit%C3%A0"><span class="header-section-number">13</span> Il calcolo delle probabilità</a></li>
<li><a class="nav-link" href="#probabilit%C3%A0-nel-linguaggio-naturale"><span class="header-section-number">13.1</span> Probabilità nel linguaggio naturale</a></li>
<li><a class="nav-link" href="#probabilit%C3%A0-nel-linguaggio-scientifico"><span class="header-section-number">13.2</span> Probabilità nel linguaggio scientifico</a></li>
<li><a class="nav-link" href="#terminologia-1"><span class="header-section-number">13.3</span> Terminologia</a></li>
<li>
<a class="nav-link" href="#le-diverse-definizioni-della-probabilit%C3%A0"><span class="header-section-number">13.4</span> Le diverse definizioni della probabilità</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#una-definizione-ingenua-della-probabilit%C3%A0"><span class="header-section-number">13.4.1</span> Una definizione ingenua della probabilità</a></li>
<li><a class="nav-link" href="#una-definizione-non-ingenua-della-probabilit%C3%A0"><span class="header-section-number">13.4.2</span> Una definizione non ingenua della probabilità</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#assegnare-le-probabilit%C3%A0-agli-eventi"><span class="header-section-number">13.5</span> Assegnare le probabilità agli eventi</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#approccio-frequentista"><span class="header-section-number">13.5.1</span> Approccio frequentista</a></li>
<li><a class="nav-link" href="#approccio-bayesiano"><span class="header-section-number">13.5.2</span> Approccio Bayesiano</a></li>
</ul>
</li>
<li><a class="nav-link" href="#propriet%C3%A0-elementari-della-probabilit%C3%A0"><span class="header-section-number">13.6</span> Proprietà elementari della probabilità</a></li>
<li>
<a class="nav-link" href="#variabili-aleatorie"><span class="header-section-number">13.7</span> Variabili aleatorie</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-cosa-servono-le-variabili-aleatorie"><span class="header-section-number">13.7.1</span> A cosa servono le variabili aleatorie?</a></li>
<li><a class="nav-link" href="#funzione-di-massa-di-probabilit%C3%A0"><span class="header-section-number">13.7.2</span> Funzione di massa di probabilità</a></li>
</ul>
</li>
<li><a class="nav-link" href="#notazione"><span class="header-section-number">13.8</span> Notazione</a></li>
<li><a class="nav-link" href="#conclusioni-2">Conclusioni</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science per psicologi</strong>" was written by Corrado Caudek. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
