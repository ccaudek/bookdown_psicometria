<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capitolo 13 Il teorema di Bayes | PSICOMETRIA</title>
<meta name="author" content="Corrado Caudek">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6.4/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">PSICOMETRIA</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Benvenuti</a></li>
<li class="book-part">Introduzione</li>
<li><a class="" href="obiettivi-formativi.html"><span class="header-section-number">1</span> Obiettivi formativi</a></li>
<li class="book-part">Introduzione al linguaggio R</li>
<li><a class="" href="chapter-pacchetti.html"><span class="header-section-number">2</span> Pacchetti</a></li>
<li><a class="" href="chapter-install-r.html"><span class="header-section-number">3</span> Per cominciare</a></li>
<li><a class="" href="chapter-sintassi.html"><span class="header-section-number">4</span> Sintassi di base</a></li>
<li><a class="" href="chapter-strutture-dati.html"><span class="header-section-number">5</span> Strutture di dati</a></li>
<li><a class="" href="chapter-strut-contr.html"><span class="header-section-number">6</span> Strutture di controllo</a></li>
<li><a class="" href="chapter-input-output.html"><span class="header-section-number">7</span> Input/Output</a></li>
<li class="book-part">Misurazione</li>
<li><a class="" href="chapter-terminologia.html"><span class="header-section-number">8</span> Terminologia</a></li>
<li><a class="" href="chapter-misurazione.html"><span class="header-section-number">9</span> La misurazione in psicologia</a></li>
<li class="book-part">Descrizione</li>
<li><a class="" href="chapter-descript.html"><span class="header-section-number">10</span> Statistica descrittiva</a></li>
<li class="book-part">Elementi di teoria della probabilità</li>
<li><a class="" href="chapter-prob.html"><span class="header-section-number">11</span> Il calcolo delle probabilità</a></li>
<li><a class="" href="chapter-prob-cond.html"><span class="header-section-number">12</span> Probabilità condizionata</a></li>
<li><a class="active" href="chapter-teo-bayes.html"><span class="header-section-number">13</span> Il teorema di Bayes</a></li>
<li><a class="" href="chapter-prob-congiunta.html"><span class="header-section-number">14</span> Probabilità congiunta</a></li>
<li class="book-part">Inferenza frequentista</li>
<li><a class="" href="distribuzione-campionaria-della-media-dei-campioni.html"><span class="header-section-number">15</span> Distribuzione campionaria della media dei campioni</a></li>
<li class="book-part">Statistica Bayesiana</li>
<li><a class="" href="chapter-modellistica-bayesiana.html"><span class="header-section-number">16</span> Modellistica bayesiana</a></li>
<li><a class="" href="chapter-stima-distr-posteriori.html"><span class="header-section-number">17</span> Stima della funzione a posteriori</a></li>
<li><a class="" href="una-breve-introduzione-al-modello-di-regressione.html"><span class="header-section-number">18</span> Una breve introduzione al modello di regressione</a></li>
<li><a class="" href="il-modello-statistico-della-regressione-lineare.html"><span class="header-section-number">19</span> Il modello statistico della regressione lineare</a></li>
<li><a class="" href="inferenza-bayesiana.html"><span class="header-section-number">20</span> Inferenza Bayesiana</a></li>
<li><a class="" href="appendici.html">Appendici</a></li>
<li><a class="" href="bibliografia.html">Bibliografia</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-teo-bayes" class="section level1" number="13">
<h1>
<span class="header-section-number">Capitolo 13</span> Il teorema di Bayes<a class="anchor" aria-label="anchor" href="#chapter-teo-bayes"><i class="fas fa-link"></i></a>
</h1>
<p>Il teorema di Bayes ha un ruolo centrale nella statistica Bayesiana,
anche se viene utilizzato anche dall’approccio frequentista. Prima di
esaminare il teorema di Bayes introdurremo una sua componente, ovvero il
teorema della probabilità totale.</p>
<div id="il-teorema-della-probabilità-totale" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Il teorema della probabilità totale<a class="anchor" aria-label="anchor" href="#il-teorema-della-probabilit%C3%A0-totale"><i class="fas fa-link"></i></a>
</h2>
<p>Il teorema della probabilità totale fa uso della legge della probabilità
composta <a href="chapter-prob.html#eq:probcomposte">(11.1)</a> per calcolare le probabilità di casi più
complessi di quelli considerati fino ad ora. La notazione sembra
complessa, ma l’idea sottostante è semplice. Discutiamo qui il teorema
della probabilità totale considerando il caso di una partizione dello
spazio campionario in tre sottoinsiemi. È facile estendere tale
situazione al caso di una partizione in un qualunque numero di
sottoinsiemi.</p>

<div class="theorem">
<span id="thm:theo-prob-tot" class="theorem"><strong>Teorema 13.1  </strong></span>Sia <span class="math inline">\(\{A_1, A_2, A_3\}\)</span> una partizione dello spazio campionario
<span class="math inline">\(\Omega\)</span>. Se <span class="math inline">\(E\)</span> è un qualunque altro evento, allora:
<span class="math display" id="eq:prob-total-1a">\[\begin{equation}
P(E) = P(E \cap A_1) + P(E \cap A_2) + P(E \cap A_3) \notag
\tag{13.1}
\end{equation}\]</span>
ovvero
<span class="math display" id="eq:prob-total-1b">\[\begin{equation}
P(E) = P(E \mid A_1) P(A_1) + P (E \mid A_2) P(A_2) + P(E \mid A_3) P(A_3).
\tag{13.2}
\end{equation}\]</span>
</div>
<p>Il teorema della probabilità totale afferma che, se l’evento <span class="math inline">\(E\)</span> è
costituito da tutti gli eventi elementari in <span class="math inline">\(E \cap A_1\)</span>, <span class="math inline">\(E \cap A_2\)</span>
e <span class="math inline">\(E \cap A_3\)</span>, allora la probabilità <span class="math inline">\(P(E)\)</span> è data dalla somma delle
probabilità di queti tre eventi. Ciò è illustrato nella figura seguente.</p>
<div class="inline-figure"><img src="Psicometria_files/figure-html/tikz-prob-tot-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>Esercizio.</strong></p>
<p>Si considerino tre urne, ciascuna delle quali contiene 100 palline:</p>
<ul>
<li>Urna 1: 75 palline rosse e 25 palline blu,</li>
<li>Urna 2: 60 palline rosse e 40 palline blu,</li>
<li>Urna 3: 45 palline rosse e 55 palline blu.</li>
</ul>
<p>Una pallina viene estratta a caso da un’urna anch’essa scelta a caso.
Qual è la probabilità che la pallina estratta sia di colore rosso?</p>
<p><em>Soluzione.</em>
Sia <span class="math inline">\(R\)</span> l’evento “la pallina estratta è rossa” e sia <span class="math inline">\(U_i\)</span> l’evento che
corrisponde alla scelta dell’<span class="math inline">\(i\)</span>-esima urna. Sappiamo che
<span class="math display">\[
P(R \mid U_1) = 0.75, \qquad P(R \mid U_2) = 0.60, \qquad P(R \mid U_3) = 0.45.
\]</span>
Gli eventi <span class="math inline">\(U_1\)</span>, <span class="math inline">\(U_2\)</span> e <span class="math inline">\(U_3\)</span> costituiscono una partizione dello
spazio campionario in quanto <span class="math inline">\(U_1\)</span>, <span class="math inline">\(U_2\)</span> e <span class="math inline">\(U_3\)</span> sono eventi
mutualmente esclusivi ed esaustivi, <span class="math inline">\(P(U_1 \cup U_2 \cup U_3) = 1.0\)</span>. In
base al teorema della probabilità totale, la probabilità di estrarre una
pallina rossa è
<span class="math display">\[
\begin{aligned}
P(R) &amp;= P(R \mid U_1)P(U_1)+P(R \mid U_2)P(U_2)+P(R \mid U_3)P(U_3)\notag\\
&amp;= 0.75 \cdot \frac{1}{3}+0.60 \cdot \frac{1}{3}+0.45 \cdot \frac{1}{3} =0.60.\notag
\end{aligned}
\]</span></p>
<p><strong>Esercizio.</strong>
Consideriamo un’urna che contiene 5 palline rosse e 2 palline verdi. Due
palline vengono estratte, una dopo l’altra. Vogliamo sapere la
probabilità dell’evento “la seconda pallina estratta è rossa.”</p>
<p><em>Soluzione.</em>
Lo spazio campionario è <span class="math inline">\(\Omega = \{RR, RV, VR, VV\}\)</span>. Chiamiamo <span class="math inline">\(R_1\)</span>
l’evento “la prima pallina estratta è rossa,” <span class="math inline">\(V_1\)</span> l’evento “la prima
pallina estratta è verde,” <span class="math inline">\(R_2\)</span> l’evento “la seconda pallina estratta è
rossa” e <span class="math inline">\(V_2\)</span> l’evento “la seconda pallina estratta è verde.” Dobbiamo
trovare <span class="math inline">\(P(R_2)\)</span> e possiamo risolvere il problema usando il teorema
della probabilità
totale <a href="chapter-teo-bayes.html#eq:prob-total-1b">(13.2)</a>:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
P(R_2) &amp;= P(R_2 \mid R_1) P(R_1) + P(R_2 \mid V_1)P(V_1)\notag\\
&amp;= \frac{4}{6} \cdot \frac{5}{7} + \frac{5}{6} \cdot \frac{2}{7} = \frac{30}{42} = \frac{5}{7}.\notag
\end{aligned}
\end{equation}\]</span>
Se la prima estrazione è quella di una pallina rossa, nell’urna restano
4 palline rosse e due verdi, dunque, la probabilità che la seconda
estrazione produca una pallina rossa è uguale a 4/6. La probabilità di
una pallina rossa nella prima estrazione è 5/7. Se la prima estrazione è
quella di una pallina verde, nell’urna restano 5 palline rosse e una
pallina verde, dunque, la probabilità che la seconda estrazione produca
una pallina rossa è uguale a 5/6. La probabilità di una pallina verde
nella prima estrazione è 2/7.</p>
</div>
<div id="il-teorema-della-probabilità-delle-cause" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Il teorema della probabilità delle cause<a class="anchor" aria-label="anchor" href="#il-teorema-della-probabilit%C3%A0-delle-cause"><i class="fas fa-link"></i></a>
</h2>
<p>Il teorema di Bayes rappresenta uno dei fondamenti della teoria della
probabilità e della statistica. Lo presentiamo qui considerando prima un
caso specifico per poi descriverlo nella sua forma più generale.</p>
<p>Sia <span class="math inline">\(\{A_1, A_2\}\)</span> una partizione dello spazio campionario <span class="math inline">\(\Omega\)</span>.
Consideriamo un terzo evento <span class="math inline">\(E \subset \Omega\)</span> con probabilità non
nulla di cui si conoscono le probabilità condizionate rispetto ad <span class="math inline">\(A_1\)</span>
e a <span class="math inline">\(A_2\)</span>, ovvero <span class="math inline">\(P(E \mid A_1)\)</span> e <span class="math inline">\(P(E \mid A_2)\)</span>. È chiaro per le
ipotesi fatte che se si verifica <span class="math inline">\(E\)</span> deve anche essersi verificato
almeno uno degli eventi <span class="math inline">\(A_1\)</span> e <span class="math inline">\(A_2\)</span>. Supponendo che si sia verificato
l’evento <span class="math inline">\(E\)</span>, ci chiediamo: qual è la probabilità che si sia verificato
<span class="math inline">\(A_1\)</span> piuttosto che <span class="math inline">\(A_2\)</span>?</p>
<div class="inline-figure"><img src="Psicometria_files/figure-html/unnamed-chunk-139-1.png" width="45%" style="display: block; margin: auto;"></div>
<p>Per rispondere alla domanda precedente scriviamo:
<span class="math display">\[\begin{equation}
\begin{aligned}
P(A_1 \mid E) &amp;= \frac{P(E \cap A_1)}{P(E)}\notag\\ 
&amp;= \frac{P(E \mid A_1)P(A_1)}{P(E)}\notag.\end{aligned}
\end{equation}\]</span>
Sapendo che <span class="math inline">\(E = (E \cap A_1) \cup (E \cap A_2)\)</span> e che <span class="math inline">\(A_1\)</span> e <span class="math inline">\(A_2\)</span> sono eventi
disgiunti, ovvero <span class="math inline">\(A_1 \cap A_2 = \emptyset\)</span>, ne segue che possiamo
calcolare <span class="math inline">\(P(E)\)</span> utilizzando il teorema della probabilità totale:
<span class="math display">\[\begin{equation}
\begin{aligned}
P(E) &amp;= P(E \cap A_1) + P(E \cap A_2)\notag\\ 
     &amp;= P(E \mid A_1)P(A_1) + P(E \mid A_2)P(A_2).\notag
\end{aligned}
\end{equation}\]</span>
Sostituendo il risultato precedente nella formula della probabilità condizionata <span class="math inline">\(P(A_1 \mid E)\)</span> otteniamo:
<span class="math display" id="eq:bayes1">\[\begin{equation}
P(A_1 \mid E) = \frac{P(E \mid A_1)P(A_1)}{P(E \mid A_1)P(A_1) + P(E \mid A_2)P(A_2)}.
\tag{13.3}
\end{equation}\]</span>
La <a href="chapter-teo-bayes.html#eq:bayes1">(13.3)</a> si generalizza facilmente al caso di più di due eventi disgiunti, come indicato di seguito.</p>

<div class="theorem">
<span id="thm:theo-bayes" class="theorem"><strong>Teorema 13.2  (Teorema di Bayes)  </strong></span>Siano <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2,\)</span> …, <span class="math inline">\(A_n\)</span> <span class="math inline">\(n\)</span> eventi disgiunti con <span class="math inline">\(P(A_i) &gt; 0\)</span> e
tali che <span class="math inline">\(\bigcup_{i=1}^{n} A_i = \Omega\)</span>. Per l’evento
<span class="math inline">\(E \subset \Omega\)</span> con <span class="math inline">\(P(E) &gt; 0\)</span>, abbiamo
<span class="math display" id="eq:bayes2">\[\begin{equation}
P(A_j \mid E) = \frac{P(E \mid A_j)P(A_j)}{\sum_{i=1}^{n}P(E \mid A_i)P(A_i)}.
\tag{13.4}
\end{equation}\]</span>
</div>
<p>La formula <a href="chapter-teo-bayes.html#eq:bayes2">(13.4)</a> prende il nome di <em>Teorema di Bayes</em> e mostra che
la conoscenza del verificarsi dell’evento <span class="math inline">\(E\)</span> modifica la probabilità
che abbiamo attribuito all’evento <span class="math inline">\(A_j\)</span>.</p>
<div id="aggiornamento-bayesiano" class="section level3" number="13.2.1">
<h3>
<span class="header-section-number">13.2.1</span> Aggiornamento Bayesiano<a class="anchor" aria-label="anchor" href="#aggiornamento-bayesiano"><i class="fas fa-link"></i></a>
</h3>
<p>Consideriamo ora un’altra applicazione del teorema di Bayes che ci fa
capire come l’applicazione di questo teorema ci consente di modificare
una credenza a priori in maniera dinamica, via via che nuove evidenze
vengono raccolta, in modo tale da formulare una credenza a posteriori la
quale non è mai definitiva, ma può essere sempre aggiornata in base alle
nuove evidenze disponibili. Questo processo si chiama <em>aggiornamento
Bayesiano</em>.</p>
<p>Supponiamo che, per qualche strano errore di produzione, una fabbrica
produca due tipi di monete. Il primo tipo di monete ha la caratteristica
che, quando una moneta viene lanciata, la probabilità di osservare
l’esito “testa” è 0.6. Per semplicità, sia <span class="math inline">\(\theta\)</span> la probabilità di
osservare l’esito “testa.” Per una moneta del primo tipo, dunque,
<span class="math inline">\(\theta = 0.6\)</span>. Per una moneta del secondo tipo, invece, la probabilità
di produrre l’esito “testa” è 0.4. Ovvero, <span class="math inline">\(\theta = 0.4\)</span>. Noi
possediamo una moneta, ma non sappiamo se è del primo tipo o del secondo
tipo. Sappiamo solo che il 75% delle monete sono del primo tipo e il 25%
sono del secondo tipo. Sulla base di questa conoscenza <em>a priori</em> –
ovvero sulla base di una conoscenza ottenuta senza avere eseguito
l’esperimento che consiste nel lanciare la moneta una serie di volte per
osservare gli esiti prodotti – possiamo dire che la probabilità di una
prima ipotesi, secondo la quale <span class="math inline">\(\theta = 0.6\)</span>, è 3 volte più grande
della probabilità di una seconda ipotesi, secondo la quale
<span class="math inline">\(\theta = 0.4\)</span>. Senza avere eseguito alcun esperimento casuale con la
moneta, questo è quello che sappiamo.</p>
<p>Ora immaginiamo di lanciare una moneta due volte e di ottenere il
risultato seguente: <span class="math inline">\(\{T, C\}\)</span>. Quello che ci chiediamo è: sulla base di
questa evidenza, come cambiano le probabilità che associamo alle due
ipotesi? In altre parole, ci chiediamo qual è la probabilità di ciascuna
ipotesi alla luce dei dati che sono stati osservati: <span class="math inline">\(P(H \mid x)\)</span>,
laddove <span class="math inline">\(x\)</span> sono i dati osservati. Tale probabilità si chiama
probabilità a posteriori. Inoltre, se confrontiamo le due ipotesi, ci
chiediamo quale valore assuma il rapporto
<span class="math inline">\(\frac{P(H_1 \mid x)}{P(H_2 \mid x)}\)</span>. Tale rapporto ci dice quanto è
più probabile <span class="math inline">\(H_1\)</span> rispetto ad <span class="math inline">\(H_2\)</span>, alla luce dei dati osservati.
Infine, ci chiediamo come cambia il rapporto definito sopra, quando
osserviamo via via nuovi risultati prodotti dal lancio della moneta.</p>
<p>Definiamo il problema in maniera più chiara. Conosciamo le probabilità a
priori, ovvero <span class="math inline">\(P(H_1) = 0.75\)</span> e <span class="math inline">\(P(H_1) = 0.25\)</span>. Quello che vogliamo
conoscere sono le probabilità a posteriori <span class="math inline">\(P(H_1 \mid x)\)</span> e
<span class="math inline">\(P(H_2 \mid x)\)</span>. Per trovare le probabilità a posteriori applichiamo il
teorema di Bayes: <span class="math display">\[P(H_1 \mid x) = \frac{P(x \mid H_1) P(H_1)}{P(x)} = 
\frac{P(x \mid H_1) P(H_1)}{P(x \mid H_1) P(H_1) + P(x \mid H_2) P(H_2)},\]</span>
laddove lo sviluppo del denominatore deriva da un’applicazione del
teorema della probabilità totale. Inoltre, <span class="math display">\[P(H_2 \mid x) = 
\frac{P(x \mid H_2) P(H_2)}{P(x \mid H_1) P(H_1) + P(x \mid H_2) P(H_2)}.\]</span></p>
<p>La probabilità <span class="math inline">\(P(x \mid H_1)\)</span> si chiama <em>verosimiglianza</em> e descrive la
plausibilità dei dati osservati in base all’ipotesi considerata. Se
consideriamo l’ipotesi <span class="math inline">\(H_1\)</span> = “la probabilità di testa è 0.6,” allora
la verosimiglianza dei dati <span class="math inline">\(\{T, C\}\)</span> è <span class="math inline">\(0.6 \times 0.4 = 0.24.\)</span>
Dunque, <span class="math inline">\(P(x \mid H_1) = 0.24\)</span>. Se invece consideriamo l’ipotesi <span class="math inline">\(H_2\)</span> =
“la probabilità di testa è 0.4,” allora la verosimiglianza dei dati
<span class="math inline">\(\{T, C\}\)</span> è <span class="math inline">\(0.4 \times 0.6 = 0.24\)</span>, ovvero, <span class="math inline">\(P(x \mid H_2) = 0.24\)</span>. In
base alle due ipotesi <span class="math inline">\(H_1\)</span> e <span class="math inline">\(H_2\)</span>, dunque, i dati osservati hanno la
medesima plausibilità. Per semplicità, calcoliamo anche
<span class="math display">\[\begin{aligned}
P(x) &amp;= P(x \mid H_1) P(H_1) + P(x \mid H_2) P(H_2) = 0.24 \cdot 0.75 + 0.24 \cdot 0.25 = 0.24.\notag\end{aligned}\]</span></p>
<p>Le probabilità a posteriori diventano: <span class="math display">\[\begin{aligned}
P(H_1 \mid x) &amp;= \frac{P(x \mid H_1) P(H_1)}{P(x)} = \frac{0.24 \cdot 0.75}{0.24} = 0.75,\notag\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
P(H_2 \mid x) &amp;= \frac{P(x \mid H_2) P(H_2)}{P(x)} = \frac{0.24 \cdot 0.25}{0.24} = 0.25.\notag\end{aligned}\]</span>
Possiamo dunque concludere dicendo che, sulla base dei dati osservati,
l’ipotesi <span class="math inline">\(H_1\)</span> ha una probabilità 3 volte maggiore di essere vera
dell’ipotesi <span class="math inline">\(H_2\)</span>.</p>
<p>È tuttavia possibile raccogliere più evidenze e, sulla base di esse, le
probabilità a posteriori cambieranno. Supponiamo di lanciare la moneta
una terza volta e di osservare croce. I nostri dati saranno dunque
<span class="math inline">\(\{T, C, C\}\)</span>. Di conseguenza,
<span class="math inline">\(P(x \mid H_1) = 0.6 \cdot 0.4 \cdot 0.4 = 0.096\)</span> e
<span class="math inline">\(P(x \mid H_2) = 0.4 \cdot 0.6 \cdot 0.6 = 0.144\)</span>. Ne segue che le
probabilità a posteriori diventano: <span class="math display">\[\begin{aligned}
P(H_1 \mid x) &amp;= \frac{P(x \mid H_1) P(H_1)}{P(x)} = \frac{0.096 \cdot 0.75}{0.096 \cdot 0.75 + 0.144 \cdot 0.25} = 0.667,\notag\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
P(H_2 \mid x) &amp;= \frac{P(x \mid H_2) P(H_2)}{P(x)} = \frac{0.144 \cdot 0.25}{0.096 \cdot 0.75 + 0.144 \cdot 0.25} = 0.333.\notag\end{aligned}\]</span>
In queste circostanze, le evidenze che favoriscono <span class="math inline">\(H_1\)</span> nei confronti
di <span class="math inline">\(H_2\)</span> sono pari solo ad un fattore di 2.</p>
<p>Se otteniamo ancora croce in un quarto lancio della moneta, i nostri
dati saranno: <span class="math inline">\(\{T, C, C, C\}\)</span>. Ripetendo il ragionamento fatto sopra,
<span class="math inline">\(P(x \mid H_1) = 0.6 \cdot 0.4 \cdot 0.4 \cdot 0.4 = 0.0384\)</span> e
<span class="math inline">\(P(x \mid H_2) = 0.4 \cdot 0.6 \cdot 0.6 \cdot 0.6 = 0.0864\)</span>. Dunque
<span class="math display">\[\begin{aligned}
P(H_1 \mid x) &amp;= \frac{0.0384 \cdot 0.75}{0.0384 \cdot 0.75 + 0.0864 \cdot 0.25} = 0.571,\notag\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
P(H_2 \mid x) &amp;= \frac{0.0864 \cdot 0.25}{0.0384 \cdot 0.75 + 0.0864 \cdot 0.25} = 0.429.\notag\end{aligned}\]</span>
e le evidenze a favore di <span class="math inline">\(H_1\)</span> si riducono a 1.33. Se si ottenesse un
altro esito croce in un sesto lancio della moneta, l’ipotesi <span class="math inline">\(H2\)</span>
diventerebbe più probabile dell’ipotesi <span class="math inline">\(H_1\)</span>.</p>
<p>In conclusione, questo esercizio ci fa capire come sia possibile, sulla
base delle evidenze disponibili, passare da credenze a priori a credenze
a posteriori. Se prima di lanciare la moneta ritenevamo che l’ipotesi
<span class="math inline">\(H_1\)</span> fosse tre volte più plausibile dell’ipotesi <span class="math inline">\(H_2\)</span>, dopo avere
osservato uno specifico campione di dati siamo giunti alla conclusione
opposta. Il processo di aggiornamento Bayesiano ci fornisce dunque un
metodo per modificare il livello di fiducia in una data ipotesi, alla
luce di nuova informazione.</p>
</div>
</div>
<div id="conclusioni-4" class="section level2 unnumbered">
<h2>Conclusioni<a class="anchor" aria-label="anchor" href="#conclusioni-4"><i class="fas fa-link"></i></a>
</h2>
<p>Il teorema di Bayes costituisce il fondamento dell’approccio più moderno
della statistica, quello appunto detto Bayesiano. Chi usa il teorema di
Bayes non è, solo per questo motivo, “bayesiano.” Ci vuole ben altro. Ci
vuole un modo diverso per intendere il significato della probabilità e
un modo diverso per intendere gli obiettivi dell’inferenza statistica.
L’approccio bayesiano è stato, negli scorsi decenni, un approccio
piuttosto dogmatico a questi temi e, a causa di ciò, è stato considerato
da alcuni come un metodo un po’ troppo lontano dall’atteggiamento
critico e non dogmatico che costituisce il fondamento della comunità
scientifica. In anni recenti, questi aspetti più “ruvidi” dell’approccio
bayesiano sono stati abbandonati e una gran parte della comunità
scientifica riconosce all’approccio bayesiano il merito di consentire lo
sviluppo di modelli anche molto complessi senza, d’altra parte,
richiedere conoscenze matematiche troppo avanzate all’utente. Per questa
ragione l’approccio bayesiano sta prendendo sempre più piede, anche in
psicologia. Un introduzione a questi temi sarà presentata nell’ultima
parte di queste dispense.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-prob-cond.html"><span class="header-section-number">12</span> Probabilità condizionata</a></div>
<div class="next"><a href="chapter-prob-congiunta.html"><span class="header-section-number">14</span> Probabilità congiunta</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-teo-bayes"><span class="header-section-number">13</span> Il teorema di Bayes</a></li>
<li><a class="nav-link" href="#il-teorema-della-probabilit%C3%A0-totale"><span class="header-section-number">13.1</span> Il teorema della probabilità totale</a></li>
<li>
<a class="nav-link" href="#il-teorema-della-probabilit%C3%A0-delle-cause"><span class="header-section-number">13.2</span> Il teorema della probabilità delle cause</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#aggiornamento-bayesiano"><span class="header-section-number">13.2.1</span> Aggiornamento Bayesiano</a></li></ul>
</li>
<li><a class="nav-link" href="#conclusioni-4">Conclusioni</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>PSICOMETRIA</strong>" was written by Corrado Caudek. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
