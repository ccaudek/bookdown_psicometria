<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitolo 13 Stima della funzione a posteriori | PSICOMETRIA</title>
  <meta name="description" content="Queste dispense si propongono di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitolo 13 Stima della funzione a posteriori | PSICOMETRIA" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Queste dispense si propongono di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  <meta name="github-repo" content="ccaudek/bookdown_psicometria" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitolo 13 Stima della funzione a posteriori | PSICOMETRIA" />
  <meta name="twitter:site" content="@ccaudek" />
  <meta name="twitter:description" content="Queste dispense si propongono di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  

<meta name="author" content="Corrado Caudek" />


<meta name="date" content="2021-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon" />
<link rel="prev" href="chapter-distr-congiunta.html"/>
<link rel="next" href="chapter-reglin.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A.A. 2020/2021</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefazione</a></li>
<li class="part"><span><b>Introduzione</b></span></li>
<li class="chapter" data-level="" data-path="obiettivi.html"><a href="obiettivi.html"><i class="fa fa-check"></i>Obiettivi</a></li>
<li class="chapter" data-level="" data-path="perché-tanta-statistica-in-psicologia.html"><a href="perché-tanta-statistica-in-psicologia.html"><i class="fa fa-check"></i>Perché tanta statistica in psicologia?</a></li>
<li class="chapter" data-level="" data-path="come-studiare.html"><a href="come-studiare.html"><i class="fa fa-check"></i>Come studiare?</a></li>
<li class="part"><span><b>Introduzione al linguaggio R</b></span></li>
<li class="chapter" data-level="1" data-path="chapter-pacchetti.html"><a href="chapter-pacchetti.html"><i class="fa fa-check"></i><b>1</b> Pacchetti</a></li>
<li class="chapter" data-level="2" data-path="cha-install.html"><a href="cha-install.html"><i class="fa fa-check"></i><b>2</b> Per cominciare</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="2.1" data-path="cha-install.html"><a href="cha-install.html#installare-r-e-rstudio"><i class="fa fa-check"></i><b>2.1</b> Installare R e RStudio</a></li>
<li class="chapter" data-level="2.2" data-path="cha-install.html"><a href="cha-install.html#utilizzare-rstudio-per-semplificare-il-lavoro"><i class="fa fa-check"></i><b>2.2</b> Utilizzare RStudio per semplificare il lavoro</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="cha-install.html"><a href="cha-install.html#eseguire-il-codice"><i class="fa fa-check"></i><b>2.2.1</b> Eseguire il codice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html"><i class="fa fa-check"></i><b>3</b> Sintassi di base</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="3.1" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#utilizzare-la-console-r-come-calcolatrice"><i class="fa fa-check"></i><b>3.1</b> Utilizzare la console R come calcolatrice</a></li>
<li class="chapter" data-level="3.2" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#le-parentesi"><i class="fa fa-check"></i><b>3.2</b> Le parentesi</a></li>
<li class="chapter" data-level="3.3" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#i-nomi-degli-oggetti"><i class="fa fa-check"></i><b>3.3</b> I nomi degli oggetti</a></li>
<li class="chapter" data-level="3.4" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#permanenza-dei-dati-e-rimozione-di-oggetti"><i class="fa fa-check"></i><b>3.4</b> Permanenza dei dati e rimozione di oggetti</a></li>
<li class="chapter" data-level="3.5" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#chiudere-r"><i class="fa fa-check"></i><b>3.5</b> Chiudere R</a></li>
<li class="chapter" data-level="3.6" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#sec:editor"><i class="fa fa-check"></i><b>3.6</b> Creare ed eseguire uno script R con un editore</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#commentare-il-codice"><i class="fa fa-check"></i><b>3.6.1</b> Commentare il codice</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#sec:change_dir"><i class="fa fa-check"></i><b>3.7</b> Cambiare la cartella di lavoro</a></li>
<li class="chapter" data-level="3.8" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#loggetto-base-di-il-vettore"><i class="fa fa-check"></i><b>3.8</b> L’oggetto base di : il vettore</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#operazioni-vettorializzate"><i class="fa fa-check"></i><b>3.8.1</b> Operazioni vettorializzate</a></li>
<li class="chapter" data-level="3.8.2" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#vettori-aritmetici"><i class="fa fa-check"></i><b>3.8.2</b> Vettori aritmetici</a></li>
<li class="chapter" data-level="3.8.3" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#generazione-di-sequenze-regolari"><i class="fa fa-check"></i><b>3.8.3</b> Generazione di sequenze regolari</a></li>
<li class="chapter" data-level="3.8.4" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#sec:gen_rand_numbers"><i class="fa fa-check"></i><b>3.8.4</b> Generazione di numeri casuali</a></li>
<li class="chapter" data-level="3.8.5" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#vettori-logici"><i class="fa fa-check"></i><b>3.8.5</b> Vettori logici</a></li>
<li class="chapter" data-level="3.8.6" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#dati-mancanti"><i class="fa fa-check"></i><b>3.8.6</b> Dati mancanti</a></li>
<li class="chapter" data-level="3.8.7" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#vettori-di-caratteri-e-fattori"><i class="fa fa-check"></i><b>3.8.7</b> Vettori di caratteri e fattori</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#cap:sintassi_funzioni"><i class="fa fa-check"></i><b>3.9</b> Funzioni</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#scrivere-proprie-funzioni"><i class="fa fa-check"></i><b>3.9.1</b> Scrivere proprie funzioni</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#pacchetti"><i class="fa fa-check"></i><b>3.10</b> Pacchetti</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#istallazione-e-upgrade-dei-pacchetti"><i class="fa fa-check"></i><b>3.10.1</b> Istallazione e upgrade dei pacchetti</a></li>
<li class="chapter" data-level="3.10.2" data-path="chapter-sintassi-R.html"><a href="chapter-sintassi-R.html#caricare-un-pacchetto-in-r"><i class="fa fa-check"></i><b>3.10.2</b> Caricare un pacchetto in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html"><i class="fa fa-check"></i><b>4</b> Strutture di dati</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="4.1" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#classi-e-modi-degli-oggetti"><i class="fa fa-check"></i><b>4.1</b> Classi e modi degli oggetti</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#vettori"><i class="fa fa-check"></i><b>4.1.1</b> Vettori</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#matrici"><i class="fa fa-check"></i><b>4.2</b> Matrici</a></li>
<li class="chapter" data-level="4.3" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#array"><i class="fa fa-check"></i><b>4.3</b> Array</a></li>
<li class="chapter" data-level="4.4" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#operazioni-aritmetiche-su-vettori-matrici-e-array"><i class="fa fa-check"></i><b>4.4</b> Operazioni aritmetiche su vettori, matrici e array</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#operazioni-aritmetiche-su-vettori"><i class="fa fa-check"></i><b>4.4.1</b> Operazioni aritmetiche su vettori</a></li>
<li class="chapter" data-level="4.4.2" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#operazioni-aritmetiche-su-matrici"><i class="fa fa-check"></i><b>4.4.2</b> Operazioni aritmetiche su matrici</a></li>
<li class="chapter" data-level="4.4.3" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#operazioni-aritmetiche-su-array"><i class="fa fa-check"></i><b>4.4.3</b> Operazioni aritmetiche su array</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#liste"><i class="fa fa-check"></i><b>4.5</b> Liste</a></li>
<li class="chapter" data-level="4.6" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#data-frame"><i class="fa fa-check"></i><b>4.6</b> Data frame</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#selezione-di-elementi"><i class="fa fa-check"></i><b>4.6.1</b> Selezione di elementi</a></li>
<li class="chapter" data-level="4.6.2" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#giochi-di-carte"><i class="fa fa-check"></i><b>4.6.2</b> Giochi di carte</a></li>
<li class="chapter" data-level="4.6.3" data-path="chapter-strutture-dati-R.html"><a href="chapter-strutture-dati-R.html#variabili-locali"><i class="fa fa-check"></i><b>4.6.3</b> Variabili locali</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter-strutture-controllo.html"><a href="chapter-strutture-controllo.html"><i class="fa fa-check"></i><b>5</b> Strutture di controllo</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="5.1" data-path="chapter-strutture-controllo.html"><a href="chapter-strutture-controllo.html#il-ciclo-for"><i class="fa fa-check"></i><b>5.1</b> Il ciclo <code>for</code></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html"><i class="fa fa-check"></i><b>6</b> Input/Output</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="6.1" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html#la-funzione-read.table"><i class="fa fa-check"></i><b>6.1</b> La funzione <code>read.table()</code></a></li>
<li class="chapter" data-level="6.2" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html#file-di-dati-forniti-da-r"><i class="fa fa-check"></i><b>6.2</b> File di dati forniti da R</a></li>
<li class="chapter" data-level="6.3" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html#esportazione-di-un-file"><i class="fa fa-check"></i><b>6.3</b> Esportazione di un file</a></li>
<li class="chapter" data-level="6.4" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html#pacchetto-rio"><i class="fa fa-check"></i><b>6.4</b> Pacchetto <code>rio</code></a></li>
<li class="chapter" data-level="6.5" data-path="chapter-input-output-R.html"><a href="chapter-input-output-R.html#dove-sono-i-miei-file"><i class="fa fa-check"></i><b>6.5</b> Dove sono i miei file?</a></li>
</ul></li>
<li class="part"><span><b>Misurazione</b></span></li>
<li class="chapter" data-level="7" data-path="chapter-terminologia.html"><a href="chapter-terminologia.html"><i class="fa fa-check"></i><b>7</b> Terminologia</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter-terminologia.html"><a href="chapter-terminologia.html#metodi-e-procedure-della-psicologia"><i class="fa fa-check"></i><b>7.1</b> Metodi e procedure della psicologia</a></li>
<li class="chapter" data-level="7.2" data-path="chapter-terminologia.html"><a href="chapter-terminologia.html#variabili-e-costanti"><i class="fa fa-check"></i><b>7.2</b> Variabili e costanti</a></li>
<li class="chapter" data-level="7.3" data-path="chapter-terminologia.html"><a href="chapter-terminologia.html#variabili-indipendenti-e-variabili-dipendenti"><i class="fa fa-check"></i><b>7.3</b> Variabili indipendenti e variabili dipendenti</a></li>
<li class="chapter" data-level="7.4" data-path="chapter-terminologia.html"><a href="chapter-terminologia.html#la-matrice-dei-dati"><i class="fa fa-check"></i><b>7.4</b> La matrice dei dati</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html"><i class="fa fa-check"></i><b>8</b> La misurazione in psicologia</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#le-scale-di-misura"><i class="fa fa-check"></i><b>8.1</b> Le scale di misura</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#scala-nominale"><i class="fa fa-check"></i><b>8.1.1</b> Scala nominale</a></li>
<li class="chapter" data-level="8.1.2" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#scala-ordinale"><i class="fa fa-check"></i><b>8.1.2</b> Scala ordinale</a></li>
<li class="chapter" data-level="8.1.3" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#scala-ad-intervalli"><i class="fa fa-check"></i><b>8.1.3</b> Scala ad intervalli</a></li>
<li class="chapter" data-level="8.1.4" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#scala-di-rapporti"><i class="fa fa-check"></i><b>8.1.4</b> Scala di rapporti</a></li>
<li class="chapter" data-level="8.1.5" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#gerarchia-dei-livelli-di-scala-di-misura"><i class="fa fa-check"></i><b>8.1.5</b> Gerarchia dei livelli di scala di misura</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#sec:DiscreteVsContinuous"><i class="fa fa-check"></i><b>8.2</b> Variabili discrete vs. variabili continue</a></li>
<li class="chapter" data-level="8.3" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#perché-alcune-misurazioni-sono-migliori-di-altre"><i class="fa fa-check"></i><b>8.3</b> Perché alcune misurazioni sono migliori di altre?</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#sec:accuratezza_precisione"><i class="fa fa-check"></i><b>8.3.1</b> Tipologie di errori</a></li>
<li class="chapter" data-level="8.3.2" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#sec:reliability"><i class="fa fa-check"></i><b>8.3.2</b> Attendibilità</a></li>
<li class="chapter" data-level="8.3.3" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#sec:validity"><i class="fa fa-check"></i><b>8.3.3</b> Validità</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="part"><span><b>Elementi di teoria della probabilità</b></span></li>
<li class="chapter" data-level="9" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html"><i class="fa fa-check"></i><b>9</b> Che cos’è la probabilità?</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#probabilità-nel-linguaggio-naturale"><i class="fa fa-check"></i><b>9.1</b> Probabilità nel linguaggio naturale</a></li>
<li class="chapter" data-level="9.2" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#probabilità-nel-linguaggio-scientifico"><i class="fa fa-check"></i><b>9.2</b> Probabilità nel linguaggio scientifico</a></li>
<li class="chapter" data-level="9.3" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#terminologia"><i class="fa fa-check"></i><b>9.3</b> Terminologia</a></li>
<li class="chapter" data-level="9.4" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#le-diverse-definizioni-della-probabilità"><i class="fa fa-check"></i><b>9.4</b> Le diverse definizioni della probabilità</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#sec:def_ing_prob"><i class="fa fa-check"></i><b>9.4.1</b> Una definizione “ingenua” della probabilità</a></li>
<li class="chapter" data-level="9.4.2" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#una-definizione-non-ingenua-della-probabilità"><i class="fa fa-check"></i><b>9.4.2</b> Una definizione “non ingenua” della probabilità</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#assegnare-le-probabilità-agli-eventi"><i class="fa fa-check"></i><b>9.5</b> Assegnare le probabilità agli eventi</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#approccio-frequentista"><i class="fa fa-check"></i><b>9.5.1</b> Approccio frequentista</a></li>
<li class="chapter" data-level="9.5.2" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#approccio-bayesiano"><i class="fa fa-check"></i><b>9.5.2</b> Approccio Bayesiano</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#proprietà-elementari-della-probabilità"><i class="fa fa-check"></i><b>9.6</b> Proprietà elementari della probabilità</a></li>
<li class="chapter" data-level="9.7" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#variabili-aleatorie"><i class="fa fa-check"></i><b>9.7</b> Variabili aleatorie</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#a-cosa-servono-le-variabili-aleatorie"><i class="fa fa-check"></i><b>9.7.1</b> A cosa servono le variabili aleatorie?</a></li>
<li class="chapter" data-level="9.7.2" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#funzione-di-massa-di-probabilità"><i class="fa fa-check"></i><b>9.7.2</b> Funzione di massa di probabilità</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="chapter-prob-discreta.html"><a href="chapter-prob-discreta.html#notazione"><i class="fa fa-check"></i><b>9.8</b> Notazione</a></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter-prob-cond.html"><a href="chapter-prob-cond.html"><i class="fa fa-check"></i><b>10</b> Probabilità condizionata</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter-prob-cond.html"><a href="chapter-prob-cond.html#sec:prob_cond"><i class="fa fa-check"></i><b>10.1</b> Probabilità condizionata su altri eventi</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chapter-prob-cond.html"><a href="chapter-prob-cond.html#la-fallacia-del-pubblico-ministero"><i class="fa fa-check"></i><b>10.1.1</b> La fallacia del pubblico ministero</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chapter-prob-cond.html"><a href="chapter-prob-cond.html#legge-della-probabilità-composta"><i class="fa fa-check"></i><b>10.2</b> Legge della probabilità composta</a></li>
<li class="chapter" data-level="10.3" data-path="chapter-prob-cond.html"><a href="chapter-prob-cond.html#lindipendendenza-stocastica"><i class="fa fa-check"></i><b>10.3</b> L’indipendendenza stocastica</a></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter-bayes-theo.html"><a href="chapter-bayes-theo.html"><i class="fa fa-check"></i><b>11</b> Il teorema di Bayes</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter-bayes-theo.html"><a href="chapter-bayes-theo.html#sec:tot_prob_theorem"><i class="fa fa-check"></i><b>11.1</b> Il teorema della probabilità totale</a></li>
<li class="chapter" data-level="11.2" data-path="chapter-bayes-theo.html"><a href="chapter-bayes-theo.html#sec:bayes_theorem"><i class="fa fa-check"></i><b>11.2</b> Il teorema della probabilità delle cause</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter-bayes-theo.html"><a href="chapter-bayes-theo.html#aggiornamento-bayesiano"><i class="fa fa-check"></i><b>11.2.1</b> Aggiornamento Bayesiano</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html"><i class="fa fa-check"></i><b>12</b> Probabilità congiunta</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html#funzione-di-probabilità-congiunta"><i class="fa fa-check"></i><b>12.1</b> Funzione di probabilità congiunta</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html#proprietà"><i class="fa fa-check"></i><b>12.1.1</b> Proprietà</a></li>
<li class="chapter" data-level="12.1.2" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html#eventi"><i class="fa fa-check"></i><b>12.1.2</b> Eventi</a></li>
<li class="chapter" data-level="12.1.3" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html#funzioni-di-probabilità-marginali"><i class="fa fa-check"></i><b>12.1.3</b> Funzioni di probabilità marginali</a></li>
<li class="chapter" data-level="12.1.4" data-path="chapter-distr-congiunta.html"><a href="chapter-distr-congiunta.html#indipendenza-stocastica"><i class="fa fa-check"></i><b>12.1.4</b> Indipendenza stocastica</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html"><i class="fa fa-check"></i><b>13</b> Stima della funzione a posteriori</a>
<ul>
<li class="chapter" data-level="13.1" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:met_numerici_convenzionali"><i class="fa fa-check"></i><b>13.1</b> Metodi numerici convenzionali</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:appross_numerica"><i class="fa fa-check"></i><b>13.1.1</b> La procedura dell’approssimazione numerica</a></li>
<li class="chapter" data-level="13.1.2" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:es_pratico_zetsche"><i class="fa fa-check"></i><b>13.1.2</b> Un esempio pratico</a></li>
<li class="chapter" data-level="13.1.3" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:aspett_future_beta2_10"><i class="fa fa-check"></i><b>13.1.3</b> Un esempio pratico (versione 2)</a></li>
<li class="chapter" data-level="13.1.4" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sommario-della-funzione-a-posteriori"><i class="fa fa-check"></i><b>13.1.4</b> Sommario della funzione a posteriori</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#approssimazione-quadratica"><i class="fa fa-check"></i><b>13.2</b> Approssimazione quadratica</a></li>
<li class="chapter" data-level="13.3" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:integr_MC"><i class="fa fa-check"></i><b>13.3</b> Integrazione con metodo Monte Carlo</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:legge_forte_grandi_numeri"><i class="fa fa-check"></i><b>13.3.1</b> Legge forte dei grandi numeri</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:mmmc"><i class="fa fa-check"></i><b>13.4</b> Metodi MC basati su Catena di Markov</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:turista_viagg"><i class="fa fa-check"></i><b>13.4.1</b> Il problema del turista viaggiatore</a></li>
<li class="chapter" data-level="13.4.2" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#lalgoritmo-di-metropolis"><i class="fa fa-check"></i><b>13.4.2</b> L’algoritmo di Metropolis</a></li>
<li class="chapter" data-level="13.4.3" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:metropolis_one_mean"><i class="fa fa-check"></i><b>13.4.3</b> Una applicazione concreta</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-stima-funzione-aposteriori.html"><a href="chapter-stima-funzione-aposteriori.html#sec:pregi_inferenza_bayes"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chapter-reglin.html"><a href="chapter-reglin.html"><i class="fa fa-check"></i><b>14</b> Regressione lineare semplice</a>
<ul>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#obiettivi-di-apprendimento"><i class="fa fa-check"></i>Obiettivi di apprendimento</a></li>
<li class="chapter" data-level="" data-path="cha-install.html"><a href="cha-install.html#motivazione"><i class="fa fa-check"></i>Motivazione</a></li>
<li class="chapter" data-level="14.1" data-path="chapter-reglin.html"><a href="chapter-reglin.html#la-funzione-lineare"><i class="fa fa-check"></i><b>14.1</b> La funzione lineare</a></li>
<li class="chapter" data-level="14.2" data-path="chapter-reglin.html"><a href="chapter-reglin.html#lerrore-di-misurazione"><i class="fa fa-check"></i><b>14.2</b> L’errore di misurazione</a></li>
<li class="chapter" data-level="14.3" data-path="chapter-reglin.html"><a href="chapter-reglin.html#scopi-della-regressione-lineare"><i class="fa fa-check"></i><b>14.3</b> Scopi della regressione lineare</a></li>
<li class="chapter" data-level="14.4" data-path="chapter-reglin.html"><a href="chapter-reglin.html#quantificare-lassociazione-fra-due-caratteri-quantitativi"><i class="fa fa-check"></i><b>14.4</b> Quantificare l’associazione fra due caratteri quantitativi</a></li>
<li class="chapter" data-level="14.5" data-path="chapter-reglin.html"><a href="chapter-reglin.html#stime-dei-minimi-quadrati"><i class="fa fa-check"></i><b>14.5</b> Stime dei minimi quadrati</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="chapter-reglin.html"><a href="chapter-reglin.html#monotwinsiq"><i class="fa fa-check"></i><b>14.5.1</b> Un esempio concreto</a></li>
<li class="chapter" data-level="14.5.2" data-path="chapter-reglin.html"><a href="chapter-reglin.html#sec:beta_r"><i class="fa fa-check"></i><b>14.5.2</b> Coefficiente angolare e correlazione di Pearson</a></li>
<li class="chapter" data-level="14.5.3" data-path="chapter-reglin.html"><a href="chapter-reglin.html#regressione-verso-la-media"><i class="fa fa-check"></i><b>14.5.3</b> Regressione verso la media</a></li>
<li class="chapter" data-level="14.5.4" data-path="chapter-reglin.html"><a href="chapter-reglin.html#punti-influenti-e-valori-anomali"><i class="fa fa-check"></i><b>14.5.4</b> Punti influenti e valori anomali</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="chapter-reglin.html"><a href="chapter-reglin.html#bontà-delladattamento"><i class="fa fa-check"></i><b>14.6</b> Bontà dell’adattamento</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="chapter-reglin.html"><a href="chapter-reglin.html#errore-standard-della-stima"><i class="fa fa-check"></i><b>14.6.1</b> Errore standard della stima</a></li>
<li class="chapter" data-level="14.6.2" data-path="chapter-reglin.html"><a href="chapter-reglin.html#indice-di-determinazione"><i class="fa fa-check"></i><b>14.6.2</b> Indice di determinazione</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="chapter-reglin.html"><a href="chapter-reglin.html#inferenza-sullassociazione-tra-x-e-y-nella-popolazione"><i class="fa fa-check"></i><b>14.7</b> Inferenza sull’associazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nella popolazione</a>
<ul>
<li class="chapter" data-level="14.7.1" data-path="chapter-reglin.html"><a href="chapter-reglin.html#modello-statistico-di-regressione-lineare"><i class="fa fa-check"></i><b>14.7.1</b> Modello statistico di regressione lineare</a></li>
<li class="chapter" data-level="14.7.2" data-path="chapter-reglin.html"><a href="chapter-reglin.html#proprietà-degli-stimatori-dei-minimi-quadrati"><i class="fa fa-check"></i><b>14.7.2</b> Proprietà degli stimatori dei minimi quadrati</a></li>
<li class="chapter" data-level="14.7.3" data-path="chapter-reglin.html"><a href="chapter-reglin.html#le-inferenze-sul-modello-di-regressione"><i class="fa fa-check"></i><b>14.7.3</b> Le inferenze sul modello di regressione</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-reglin.html"><a href="chapter-reglin.html#considerazioni-conclusive"><i class="fa fa-check"></i>Considerazioni conclusive</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html"><i class="fa fa-check"></i><b>15</b> Il modello lineare</a>
<ul>
<li class="chapter" data-level="15.1" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#modello-binomiale"><i class="fa fa-check"></i><b>15.1</b> Modello Binomiale</a></li>
<li class="chapter" data-level="15.2" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#sec:idrossi"><i class="fa fa-check"></i><b>15.2</b> Il presidente Trump e l’idrossiclorochina</a></li>
<li class="chapter" data-level="15.3" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#modello-normale"><i class="fa fa-check"></i><b>15.3</b> Modello Normale</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#il-modello-normale-con-quap"><i class="fa fa-check"></i><b>15.3.1</b> Il modello normale con <code>quap()</code></a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#sec_mod_lin"><i class="fa fa-check"></i><b>15.4</b> Il modello di regressione lineare</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#sec_var_ind_dico"><i class="fa fa-check"></i><b>15.4.1</b> Variabile indipendente dicotomica</a></li>
<li class="chapter" data-level="15.4.2" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#sec_var_ind_dico"><i class="fa fa-check"></i><b>15.4.2</b> Un esempio pratico</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="chapter-stat-models.html"><a href="chapter-stat-models.html#sec_var_ind_dico"><i class="fa fa-check"></i><b>15.5</b> Una variabile indipendente continua</a></li>
<li class="chapter" data-level="" data-path="chapter-misurazione.html"><a href="chapter-misurazione.html#conclusioni"><i class="fa fa-check"></i>Conclusioni</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html"><i class="fa fa-check"></i>Appendici</a>
<ul>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#simbologia-di-base"><i class="fa fa-check"></i>Simbologia di base</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-binari-interi-razionali-irrazionali-e-reali"><i class="fa fa-check"></i>Numeri binari, interi, razionali, irrazionali e reali</a>
<ul>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-binari"><i class="fa fa-check"></i>Numeri binari</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-interi"><i class="fa fa-check"></i>Numeri interi</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-razionali"><i class="fa fa-check"></i>Numeri razionali</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-irrazionali"><i class="fa fa-check"></i>Numeri irrazionali</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#numeri-reali"><i class="fa fa-check"></i>Numeri reali</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#intervalli"><i class="fa fa-check"></i>Intervalli</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#cap:insiemi"><i class="fa fa-check"></i>Insiemi</a>
<ul>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#operazioni-tra-insiemi"><i class="fa fa-check"></i>Operazioni tra insiemi</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#diagrammi-di-eulero-venn"><i class="fa fa-check"></i>Diagrammi di Eulero-Venn</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#sec:prod_cartesiano"><i class="fa fa-check"></i>Coppie ordinate e prodotto cartesiano</a></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#cardinalità"><i class="fa fa-check"></i>Cardinalità</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendici.html"><a href="appendici.html#propr_coef_min_quad"><i class="fa fa-check"></i>Proprietà degli stimatori dei minimi quadrati</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSICOMETRIA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter:stima_funzione_aposteriori" class="section level1" number="13">
<h1><span class="header-section-number">Capitolo 13</span> Stima della funzione a posteriori</h1>
<p>Quando usiamo il teorema di Bayes per calcolare la distribuzione a posteriori del parametro di un modello statistico, al denominatore troviamo un integrale. Tale integrale, nella maggior parte dei casi, non si può risolvere per via analitica. L’inferenza bayesiana si sviluppa dunque mediante una stima numerica della funzione a posteriori. Dato che questi metodi sono “computazionalmente intensivi,” possono solo essere svolti mediante software. In anni recenti i metodi Bayesiani di analisi dei dati sono diventati sempre più popolari proprio perché la potenza di calcolo necessaria per svolgere tali calcoli è ora alla portata di tutti. Questo non era vero solo pochi decenni fa. Per capire come la distribuzione a posteriori possa essere approssimata per via numerica esamineremo qui tre diverse tecniche che possono essere utilizzate a questo scopo:</p>
<ol style="list-style-type: decimal">
<li><p>i metodi numerici convenzionali,</p></li>
<li><p>il metodo dell’approssimazione quadratica,</p></li>
<li><p>i metodi Monte Carlo basati su Catena di Markov (MCMC).</p></li>
</ol>
<div id="sec:met_numerici_convenzionali" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Metodi numerici convenzionali</h2>
<p>È possibile stimare l’intera distribuzione a posteriori mediante metodi
numerici convenzionali. Questo è l’approccio più semplice. Tuttavia,
anche se tali metodi possono fornire risultati accuratissimi, a causa
della “maledizione della dimensionalità,” tali procedure numeriche sono
utilizzabili solo nel caso di modelli statistici semplici, con non più
di due parametri. Nella pratica concreta tali metodi vengono sostituiti
da altre tecniche più efficienti in quanto, anche in comuni modelli
utilizzati in psicologia, vengono stimati centinaia se non migliaia di
parametri. Nell’esempio che faremo in questa sezione risulterà chiara la
ragione per cui, in tali circostanze, non è possibile usare una tale
procedura. I metodi numerici convenzionali sono invece utili come
strumento didattico in quanto ci forniscono una procedura molto diretta
e intuitiva che rende molto trasparente il processo dell’aggiornamento
Bayesiano. Per questa ragione esamineremo qui un esempio relativo a tale
procedura esaminando un modello statistico che dipende da un solo
parametro sconosciuto.</p>
<div id="sec:appross_numerica" class="section level3" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> La procedura dell’approssimazione numerica</h3>
<p>È molto semplice trovare una approssimazione numerica della
distribuzione a posteriori e ciò può essere fatto come indicato di
seguito. Anche se la maggior parte dei parametri è continua (ovvero, in
linea di principio ciascun parametro può assumere un numero infinito di
valori), possiamo ottenere un’eccellente approssimazione della
distribuzione a posteriori considerando solo una griglia finita di
valori dei parametri. Per calcolare la probabilità a posteriori in
corrispondenza di ciascun particolare valore del parametro, chiamiamolo
<span class="math inline">\(\theta&#39;\)</span>, è sufficiente moltiplicare la probabilità a priori di
<span class="math inline">\(\theta&#39;\)</span> per il valore della funzione di verosimiglianza in
corrispondenza di <span class="math inline">\(\theta&#39;\)</span>. Una stima della distribuzione a posteriori
si genera ripetendo questo procedimento per ciascun valore nella
griglia.</p>
</div>
<div id="sec:es_pratico_zetsche" class="section level3" number="13.1.2">
<h3><span class="header-section-number">13.1.2</span> Un esempio pratico</h3>
<p>Facciamo un esempio concreto consideriando nuovamente la ricerca di
<span class="citation"><a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al.</a> (<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">2019</a>)</span>. Questi autori si sono chiesti se gli individui
depressi manifestino delle aspettative accurate circa il loro umore
futuro, oppure se tali aspettative siano distorte negativamente.
Consideriamo qui i 30 partecipanti dello studio di <span class="citation"><a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al.</a> (<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">2019</a>)</span>
che hanno riportato la presenza di un episodio di depressione maggiore
in atto. All’inizio della settimana di test a questi pazienti è stato
chiesto di valutare l’umore che si aspettavano di sentire nei giorni
seguenti della settimana. Mediante una app, i partecipanti dovevano poi
valutare il proprio umore in cinque momenti diversi di ciascuno dei
cinque giorni successivi. Lo studio considera diverse emozioni, ma qui
ci concentriamo solo sulla tristezza.</p>
<p>Sulla base dei dati forniti dagli autori, abbiamo calcolato la media dei
giudizi relativi al livello di tristezza raccolti da ciascun
partecipante tramite la app. Tale media è stata poi sottratta
dall’aspettativa del livello di tristezza fornita all’inizio della
settimana. Per semplificare l’analisi abbiamo considerato la discrepanza
tra aspettative e realtà come un evento dicotomico: valori positivi di
tale differenza indicano che le aspettative circa il livello di
tristezza sono maggiori del livello di tristezza che in seguito viene
effettivamente esperito; ciò significa che le aspettative sono
negativamente distorte (evento codificato con “1”). Si può dire il
contrario (le aspettative sono positivamente distorte) se tale
differenza assume valori negativi (evento codificato con “0”). Nel
campione dei 30 partecipanti clinici qui esaminati, 23 partecipanti
manifestano delle aspettative negativamente distorte e 7 partecipanti
manifestano delle aspettative positivamente distorte. Chiamiamo <span class="math inline">\(\theta\)</span>
la probabilità dell’evento “le aspettative del partecipante sono
distorte negativamente.” Ci poniamo il problema di ottenere la stima a
posteriori di <span class="math inline">\(\theta\)</span>, dati i 23 "successi" in 30 prove che sono
stati osservati.</p>
<p>Per questo esempio considereremo 50 valori egualmente spaziati per il parametro <span class="math inline">\(\theta\)</span>: 0.000, 0.0204, …, 0.978, 1.000. In R abbiamo:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="chapter-stima-funzione-aposteriori.html#cb95-1" aria-hidden="true" tabindex="-1"></a>n_points <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb95-2"><a href="chapter-stima-funzione-aposteriori.html#cb95-2" aria-hidden="true" tabindex="-1"></a>p_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> n_points)</span>
<span id="cb95-3"><a href="chapter-stima-funzione-aposteriori.html#cb95-3" aria-hidden="true" tabindex="-1"></a>p_grid</span>
<span id="cb95-4"><a href="chapter-stima-funzione-aposteriori.html#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082 0.12244898 0.14285714</span></span>
<span id="cb95-5"><a href="chapter-stima-funzione-aposteriori.html#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [9] 0.16326531 0.18367347 0.20408163 0.22448980 0.24489796 0.26530612 0.28571429 0.30612245</span></span>
<span id="cb95-6"><a href="chapter-stima-funzione-aposteriori.html#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [17] 0.32653061 0.34693878 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776</span></span>
<span id="cb95-7"><a href="chapter-stima-funzione-aposteriori.html#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673 0.61224490 0.63265306</span></span>
<span id="cb95-8"><a href="chapter-stima-funzione-aposteriori.html#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [33] 0.65306122 0.67346939 0.69387755 0.71428571 0.73469388 0.75510204 0.77551020 0.79591837</span></span>
<span id="cb95-9"><a href="chapter-stima-funzione-aposteriori.html#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [41] 0.81632653 0.83673469 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367</span></span>
<span id="cb95-10"><a href="chapter-stima-funzione-aposteriori.html#cb95-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [49] 0.97959184 1.00000000</span></span></code></pre></div>
<div id="distribuzione-a-priori" class="section level4" number="13.1.2.1">
<h4><span class="header-section-number">13.1.2.1</span> Distribuzione a priori</h4>
<p>Supponiamo che le nostre credenze a priori sulla tendenza di un
individuo clinicamente depresso a manifestare delle aspettative distorte
negativamente circa il suo umore futuro siano molto scarse. Assumiamo
quindi per <span class="math inline">\(\theta\)</span> una distribuzione iniziale uniforme nell’intervallo
[0, 1]. Dato che consideriamo soltanto <span class="math inline">\(n = 50\)</span> valori del parametro
<span class="math inline">\(\theta\)</span>, creiamo un vettore di 50 elementi che conterrà i valori della distribuzione a priori scalando ciascun valore di questo vettore per <span class="math inline">\(n\)</span> in modo tale che la somma di tutti i
valori della distribuzione a priori (0.02, 0.02, …, 0.02, 0.02) sia
uguale a 1.0 (in questo modo viene definita una funzione di massa di
probabilità):</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="chapter-stima-funzione-aposteriori.html#cb96-1" aria-hidden="true" tabindex="-1"></a>prior1 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(p_grid, <span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(<span class="fu">dbeta</span>(p_grid, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb96-2"><a href="chapter-stima-funzione-aposteriori.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(prior1)</span>
<span id="cb96-3"><a href="chapter-stima-funzione-aposteriori.html#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>Stampo i valori:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="chapter-stima-funzione-aposteriori.html#cb97-1" aria-hidden="true" tabindex="-1"></a>prior1</span>
<span id="cb97-2"><a href="chapter-stima-funzione-aposteriori.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02</span></span>
<span id="cb97-3"><a href="chapter-stima-funzione-aposteriori.html#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [18] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02</span></span>
<span id="cb97-4"><a href="chapter-stima-funzione-aposteriori.html#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [35] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02</span></span></code></pre></div>
<p>La distribuzione a priori così costruita è riportata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr1">13.1</a>.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="chapter-stima-funzione-aposteriori.html#cb98-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(p_grid, prior1) <span class="sc">%&gt;%</span> </span>
<span id="cb98-2"><a href="chapter-stima-funzione-aposteriori.html#cb98-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>p_grid, <span class="at">xend=</span>p_grid, <span class="at">y=</span><span class="dv">0</span>, <span class="at">yend=</span>prior1)) <span class="sc">+</span></span>
<span id="cb98-3"><a href="chapter-stima-funzione-aposteriori.html#cb98-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb98-4"><a href="chapter-stima-funzione-aposteriori.html#cb98-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span> </span>
<span id="cb98-5"><a href="chapter-stima-funzione-aposteriori.html#cb98-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.17</span>) <span class="sc">+</span></span>
<span id="cb98-6"><a href="chapter-stima-funzione-aposteriori.html#cb98-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb98-7"><a href="chapter-stima-funzione-aposteriori.html#cb98-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Parametro \U03B8&quot;</span>,</span>
<span id="cb98-8"><a href="chapter-stima-funzione-aposteriori.html#cb98-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probabilità a priori&quot;</span>,</span>
<span id="cb98-9"><a href="chapter-stima-funzione-aposteriori.html#cb98-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;50 punti&quot;</span></span>
<span id="cb98-10"><a href="chapter-stima-funzione-aposteriori.html#cb98-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb98-11"><a href="chapter-stima-funzione-aposteriori.html#cb98-11" aria-hidden="true" tabindex="-1"></a>p1</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gridappr1"></span>
<img src="Psicometria_files/figure-html/gridappr1-1.png" alt="Rappresentazione della distribuzione a priori per il parametro $\theta$, ovvero la probabilità di aspettative future distorte negativamente [@zetsche_future_2019]." width="70%" />
<p class="caption">
Figura 13.1: Rappresentazione della distribuzione a priori per il parametro <span class="math inline">\(\theta\)</span>, ovvero la probabilità di aspettative future distorte negativamente <span class="citation">(<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al., 2019</a>)</span>.
</p>
</div>
</div>
<div id="funzione-di-verosimiglianza" class="section level4" number="13.1.2.2">
<h4><span class="header-section-number">13.1.2.2</span> Funzione di verosimiglianza</h4>
<p>Calcoliamo ora la funzione di verosimiglianza utilizzando i 50 valori
<span class="math inline">\(\theta\)</span> che abbiamo considerato. Per ciascun valore <span class="math inline">\(\theta\)</span> applichiamo la formula della probabilità binomiale tendendo sempre costanti i valori dei dati (ovvero 23 “successi” in 30 prove).</p>
<p>Per esempio, per il valore <span class="math inline">\(\theta = 0.816\)</span> l’ordinata della funzione di
verosimiglianza sarà
<span class="math display">\[\begin{aligned}
\binom{30}{23}&amp; \cdot 0.816^{23} \cdot (1 - 0.816)^{7} = 0.135\notag
\end{aligned}
\]</span>
e per <span class="math inline">\(\theta = 0.837\)</span> l’ordinata della funzione di verosimiglianza sarà
<span class="math display">\[\begin{aligned}
\binom{30}{23}&amp; \cdot 0.837^{23} \cdot (1 - 0.837)^{7} = 0.104.\notag
\end{aligned}
\]</span></p>
<p>Svolgendo tutti i calcoli con R otteniamo i valori seguenti:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="chapter-stima-funzione-aposteriori.html#cb99-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="dv">23</span>, <span class="at">size =</span> <span class="dv">30</span>, <span class="at">prob =</span> p_grid)</span>
<span id="cb99-2"><a href="chapter-stima-funzione-aposteriori.html#cb99-2" aria-hidden="true" tabindex="-1"></a>likelihood</span>
<span id="cb99-3"><a href="chapter-stima-funzione-aposteriori.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.000000e+00 2.352564e-33 1.703051e-26 1.644169e-22 1.053708e-19 1.525217e-17</span></span>
<span id="cb99-4"><a href="chapter-stima-funzione-aposteriori.html#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [7] 8.602222e-16 2.528440e-14 4.606907e-13 5.819027e-12 5.499269e-11 4.105534e-10</span></span>
<span id="cb99-5"><a href="chapter-stima-funzione-aposteriori.html#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [13] 2.520191e-09 1.311195e-08 5.919348e-08 2.362132e-07 8.456875e-07 2.749336e-06</span></span>
<span id="cb99-6"><a href="chapter-stima-funzione-aposteriori.html#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [19] 8.196948e-06 2.259614e-05 5.798673e-05 1.393165e-04 3.148623e-04 6.720574e-04</span></span>
<span id="cb99-7"><a href="chapter-stima-funzione-aposteriori.html#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [25] 1.359225e-03 2.611870e-03 4.778973e-03 8.340230e-03 1.390025e-02 2.214199e-02</span></span>
<span id="cb99-8"><a href="chapter-stima-funzione-aposteriori.html#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [31] 3.372227e-02 4.909974e-02 6.830377e-02 9.068035e-02 1.146850e-01 1.378206e-01</span></span>
<span id="cb99-9"><a href="chapter-stima-funzione-aposteriori.html#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [37] 1.568244e-01 1.681749e-01 1.688979e-01 1.575211e-01 1.348746e-01 1.043545e-01</span></span>
<span id="cb99-10"><a href="chapter-stima-funzione-aposteriori.html#cb99-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [43] 7.133007e-02 4.165680e-02 1.972669e-02 6.936821e-03 1.535082e-03 1.473375e-04</span></span>
<span id="cb99-11"><a href="chapter-stima-funzione-aposteriori.html#cb99-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [49] 1.868105e-06 0.000000e+00</span></span></code></pre></div>
<p>La funzione di verosimiglianza così ottenuta è riportata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr2">13.2</a>.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="chapter-stima-funzione-aposteriori.html#cb100-1" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(p_grid, likelihood) <span class="sc">%&gt;%</span> </span>
<span id="cb100-2"><a href="chapter-stima-funzione-aposteriori.html#cb100-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>p_grid, <span class="at">xend=</span>p_grid, <span class="at">y=</span><span class="dv">0</span>, <span class="at">yend=</span>likelihood)) <span class="sc">+</span></span>
<span id="cb100-3"><a href="chapter-stima-funzione-aposteriori.html#cb100-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span></span>
<span id="cb100-4"><a href="chapter-stima-funzione-aposteriori.html#cb100-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.17</span>) <span class="sc">+</span></span>
<span id="cb100-5"><a href="chapter-stima-funzione-aposteriori.html#cb100-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb100-6"><a href="chapter-stima-funzione-aposteriori.html#cb100-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Parametro \U03B8&quot;</span>,</span>
<span id="cb100-7"><a href="chapter-stima-funzione-aposteriori.html#cb100-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Verosimiglianza&quot;</span></span>
<span id="cb100-8"><a href="chapter-stima-funzione-aposteriori.html#cb100-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb100-9"><a href="chapter-stima-funzione-aposteriori.html#cb100-9" aria-hidden="true" tabindex="-1"></a>p2</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gridappr2"></span>
<img src="Psicometria_files/figure-html/gridappr2-1.png" alt="Rappresentazione della funzione di verosimiglianza per il parametro $\theta$, ovvero la probabilità di aspettative future distorte negativamente [@zetsche_future_2019]." width="70%" />
<p class="caption">
Figura 13.2: Rappresentazione della funzione di verosimiglianza per il parametro <span class="math inline">\(\theta\)</span>, ovvero la probabilità di aspettative future distorte negativamente <span class="citation">(<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al., 2019</a>)</span>.
</p>
</div>
</div>
<div id="la-stima-della-distribuzione-a-posteriori" class="section level4" number="13.1.2.3">
<h4><span class="header-section-number">13.1.2.3</span> La stima della distribuzione a posteriori</h4>
<p>La distribuzione a posteriori del parametro <span class="math inline">\(\theta\)</span> è data dal prodotto
della verosimiglianza e della distribuzione a priori, scalata per una
costante di normalizzazione. Quindi, facendo il prodotto dei valori
della distribuzione a priori e i valori della funzione di
verosimiglianza otteniamo la funzione a posteriori non standardizzata.
Dato che la distribuzione a priori è uniforme, per ottenere questo
risultato è sufficiente moltiplicare ciascun valore della funzione di
verosimiglianza per 0.02. Per esempio, per il primo valore della funzione di verosimiglianza che abbiamo calcolato sopra, avremo <span class="math inline">\(0.135 \cdot 0.02\)</span>; per il secondo valore della funzione di verosimiglianza che abbiamo calcolato sopra avremo <span class="math inline">\(0.104 \cdot 0.02\)</span>.</p>
<p>Usando R, la funzione a posteriori non standardizzata diventa:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="chapter-stima-funzione-aposteriori.html#cb101-1" aria-hidden="true" tabindex="-1"></a>unstd_posterior <span class="ot">&lt;-</span> likelihood <span class="sc">*</span> prior1</span>
<span id="cb101-2"><a href="chapter-stima-funzione-aposteriori.html#cb101-2" aria-hidden="true" tabindex="-1"></a>unstd_posterior</span>
<span id="cb101-3"><a href="chapter-stima-funzione-aposteriori.html#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.000000e+00 4.705127e-35 3.406102e-28 3.288337e-24 2.107415e-21 3.050433e-19</span></span>
<span id="cb101-4"><a href="chapter-stima-funzione-aposteriori.html#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [7] 1.720444e-17 5.056880e-16 9.213813e-15 1.163805e-13 1.099854e-12 8.211068e-12</span></span>
<span id="cb101-5"><a href="chapter-stima-funzione-aposteriori.html#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [13] 5.040382e-11 2.622390e-10 1.183870e-09 4.724263e-09 1.691375e-08 5.498671e-08</span></span>
<span id="cb101-6"><a href="chapter-stima-funzione-aposteriori.html#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [19] 1.639390e-07 4.519229e-07 1.159735e-06 2.786331e-06 6.297247e-06 1.344115e-05</span></span>
<span id="cb101-7"><a href="chapter-stima-funzione-aposteriori.html#cb101-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [25] 2.718450e-05 5.223741e-05 9.557946e-05 1.668046e-04 2.780049e-04 4.428398e-04</span></span>
<span id="cb101-8"><a href="chapter-stima-funzione-aposteriori.html#cb101-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [31] 6.744454e-04 9.819948e-04 1.366075e-03 1.813607e-03 2.293700e-03 2.756411e-03</span></span>
<span id="cb101-9"><a href="chapter-stima-funzione-aposteriori.html#cb101-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [37] 3.136488e-03 3.363497e-03 3.377958e-03 3.150422e-03 2.697491e-03 2.087091e-03</span></span>
<span id="cb101-10"><a href="chapter-stima-funzione-aposteriori.html#cb101-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [43] 1.426601e-03 8.331361e-04 3.945339e-04 1.387364e-04 3.070164e-05 2.946751e-06</span></span>
<span id="cb101-11"><a href="chapter-stima-funzione-aposteriori.html#cb101-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [49] 3.736209e-08 0.000000e+00</span></span></code></pre></div>
<p>Avendo svolto questo prodotto per tutti i 50 valori della funzione di verosimiglianza, dobbiamo poi dividere ciascuno dei 50 numeri così trovati per la costante di
normalizzazione. Nel caso discreto, trovare il denominatore del teorema
di Bayes è molto facile: esso è dato dalla somma di tutti i valori della
distribuzione a posteriori non normalizzata. Per i dati presenti, tale
costante di normalizzazione è uguale a 0.032.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="chapter-stima-funzione-aposteriori.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(unstd_posterior)</span>
<span id="cb102-2"><a href="chapter-stima-funzione-aposteriori.html#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.0316129</span></span></code></pre></div>
<p>Possiamo dunque standardizzare i due valori trovati sopra nel modo seguente:
<span class="math inline">\(0.135 \cdot 0.02 / 0.032\)</span> e <span class="math inline">\(0.104 \cdot 0.02 / 0.032\)</span>. Così facendo,
otterremo il risultato per cui la somma di tutti e 50 i valori della
distribuzione a posteriori normalizzata sarà uguale a 1.0.</p>
<p>Svolgiamo tutti i calcoli in R:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="chapter-stima-funzione-aposteriori.html#cb103-1" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> unstd_posterior <span class="sc">/</span> <span class="fu">sum</span>(unstd_posterior)</span>
<span id="cb103-2"><a href="chapter-stima-funzione-aposteriori.html#cb103-2" aria-hidden="true" tabindex="-1"></a>posterior</span>
<span id="cb103-3"><a href="chapter-stima-funzione-aposteriori.html#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.000000e+00 1.488357e-33 1.077440e-26 1.040188e-22 6.666313e-20 9.649330e-18</span></span>
<span id="cb103-4"><a href="chapter-stima-funzione-aposteriori.html#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [7] 5.442222e-16 1.599625e-14 2.914574e-13 3.681425e-12 3.479129e-11 2.597379e-10</span></span>
<span id="cb103-5"><a href="chapter-stima-funzione-aposteriori.html#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [13] 1.594406e-09 8.295316e-09 3.744893e-08 1.494410e-07 5.350268e-07 1.739376e-06</span></span>
<span id="cb103-6"><a href="chapter-stima-funzione-aposteriori.html#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [19] 5.185824e-06 1.429552e-05 3.668548e-05 8.813904e-05 1.991986e-04 4.251792e-04</span></span>
<span id="cb103-7"><a href="chapter-stima-funzione-aposteriori.html#cb103-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [25] 8.599178e-04 1.652408e-03 3.023432e-03 5.276472e-03 8.794033e-03 1.400820e-02</span></span>
<span id="cb103-8"><a href="chapter-stima-funzione-aposteriori.html#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [31] 2.133450e-02 3.106310e-02 4.321259e-02 5.736920e-02 7.255582e-02 8.719259e-02</span></span>
<span id="cb103-9"><a href="chapter-stima-funzione-aposteriori.html#cb103-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [37] 9.921545e-02 1.063963e-01 1.068538e-01 9.965619e-02 8.532881e-02 6.602021e-02</span></span>
<span id="cb103-10"><a href="chapter-stima-funzione-aposteriori.html#cb103-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [43] 4.512719e-02 2.635430e-02 1.248015e-02 4.388601e-03 9.711744e-04 9.321354e-05</span></span>
<span id="cb103-11"><a href="chapter-stima-funzione-aposteriori.html#cb103-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [49] 1.181862e-06 0.000000e+00</span></span></code></pre></div>
<p>Verifichiamo:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="chapter-stima-funzione-aposteriori.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(posterior)</span>
<span id="cb104-2"><a href="chapter-stima-funzione-aposteriori.html#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>In questo particolare esempio, la distribuzione a posteriori trovata come
descritto sopra non è altro che la versione normalizzata della funzione
di verosimiglianza: questo avviene perché la distribuzione a priori
uniforme non ha aggiunto altre informazioni oltre a quelle che erano già
fornite dalla funzione di verosimiglianza.</p>
<p>La funzione a posteriori che abbiamo calcolato con il metodo dell’approssimazione numerica è riportata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr3">13.3</a>.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="chapter-stima-funzione-aposteriori.html#cb105-1" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(p_grid, posterior) <span class="sc">%&gt;%</span> </span>
<span id="cb105-2"><a href="chapter-stima-funzione-aposteriori.html#cb105-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>p_grid, <span class="at">xend=</span>p_grid, <span class="at">y=</span><span class="dv">0</span>, <span class="at">yend=</span>posterior)) <span class="sc">+</span></span>
<span id="cb105-3"><a href="chapter-stima-funzione-aposteriori.html#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span></span>
<span id="cb105-4"><a href="chapter-stima-funzione-aposteriori.html#cb105-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.17</span>) <span class="sc">+</span></span>
<span id="cb105-5"><a href="chapter-stima-funzione-aposteriori.html#cb105-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb105-6"><a href="chapter-stima-funzione-aposteriori.html#cb105-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Parametro \U03B8&quot;</span>,</span>
<span id="cb105-7"><a href="chapter-stima-funzione-aposteriori.html#cb105-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probabilità a posteriori&quot;</span></span>
<span id="cb105-8"><a href="chapter-stima-funzione-aposteriori.html#cb105-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb105-9"><a href="chapter-stima-funzione-aposteriori.html#cb105-9" aria-hidden="true" tabindex="-1"></a>p3</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gridappr3"></span>
<img src="Psicometria_files/figure-html/gridappr3-1.png" alt="Rappresentazione della distribuzione a posteriori per il parametro $\theta$, ovvero la probabilità di aspettative future distorte negativamente [@zetsche_future_2019]." width="70%" />
<p class="caption">
Figura 13.3: Rappresentazione della distribuzione a posteriori per il parametro <span class="math inline">\(\theta\)</span>, ovvero la probabilità di aspettative future distorte negativamente <span class="citation">(<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al., 2019</a>)</span>.
</p>
</div>
<p>Le funzioni rappresentate nelle figure <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr1">13.1</a>, <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr2">13.2</a> e <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr3">13.3</a> sono state calcolate utilizzando 50 modalità equi-spaziate per il parametro <span class="math inline">\(\theta\)</span>. I segmenti verticali rappresentano l’intensità della funzione in corrispondenza di ciascuna modalità parametro <span class="math inline">\(\theta\)</span>. Nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr1">13.1</a> e nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr3">13.3</a> la somma delle lunghezze dei segmenti verticali è pari ad 1.0; ciò non si verifica, invece, nel caso della figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr3">13.3</a>.</p>
</div>
</div>
<div id="sec:aspett_future_beta2_10" class="section level3" number="13.1.3">
<h3><span class="header-section-number">13.1.3</span> Un esempio pratico (versione 2)</h3>
<p>Continuiamo la discussione dell’esempio precedente supponendo che la
letteratura precedente ci fornisca delle informazioni a proposito di
<span class="math inline">\(\theta\)</span>, ovvero sulla probabilità che le aspettative future di un
individuo clinicamente depresso siano distorte negativamente. In tali
circostanze, invece di utilizzare la distribuzione uniforme per
<span class="math inline">\(p(\theta)\)</span>, definiamo la distribuzione a priori per <span class="math inline">\(\theta\)</span> come una
distribuzione che ha la forma di una Beta di parametri <span class="math inline">\(\alpha = 2\)</span> e
<span class="math inline">\(\beta = 10\)</span>. In questo modo, la distribuzione a priori di <span class="math inline">\(\theta\)</span>
ritiene molto plausibili valori bassi di <span class="math inline">\(\theta\)</span>, mentre i valori
<span class="math inline">\(\theta\)</span> superiori a 0.5 vengono considerati impossibili. Questo è
equivalente a dire che ci aspettiamo che le aspettative relative
all’umore futuro siano distorte negativamente solo per pochissimi
individui clinicamente depressi – in altre parole, ci aspettiamo che la
maggioranza degli individui clinicamente depressi sia inguaribilmente
ottimista. Questa è, ovviamente, un’opinione a priori molto difficile da
giustificare. La esamino qui, non perché abbia senso nel contesto dei
dati di <span class="citation"><a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al.</a> (<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">2019</a>)</span>, ma soltanto per fare un esempio che mostra
come la distribuzione a posteriori fornisca una sorta di “compromesso”
tra la distribuzione a priori e la verosimiglianza.</p>
<p>Con calcoli del tutto simili a quelli descritti sopra si giunge alla distribuzione a posteriori rappresentata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr4">13.4</a>. Iniziamo a definire una griglia unidimensionale equispaziata di possibili valori del parametro <span class="math inline">\(\theta\)</span>. Anche in questo caso usiamo 50 valori possibili del parametro <span class="math inline">\(\theta\)</span>:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="chapter-stima-funzione-aposteriori.html#cb106-1" aria-hidden="true" tabindex="-1"></a>n_points <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb106-2"><a href="chapter-stima-funzione-aposteriori.html#cb106-2" aria-hidden="true" tabindex="-1"></a>p_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> n_points)</span></code></pre></div>
<p>Per la distribuzione a priori scelgo una Beta(2, 10).</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="chapter-stima-funzione-aposteriori.html#cb107-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb107-2"><a href="chapter-stima-funzione-aposteriori.html#cb107-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb107-3"><a href="chapter-stima-funzione-aposteriori.html#cb107-3" aria-hidden="true" tabindex="-1"></a>prior2 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(p_grid, alpha, beta) <span class="sc">/</span> <span class="fu">sum</span>(<span class="fu">dbeta</span>(p_grid, alpha, beta))</span>
<span id="cb107-4"><a href="chapter-stima-funzione-aposteriori.html#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(prior2)</span>
<span id="cb107-5"><a href="chapter-stima-funzione-aposteriori.html#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>Tale distribuzione a priori è rappresentata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr4">13.4</a>.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="chapter-stima-funzione-aposteriori.html#cb108-1" aria-hidden="true" tabindex="-1"></a>plot_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(p_grid, prior2) </span>
<span id="cb108-2"><a href="chapter-stima-funzione-aposteriori.html#cb108-2" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> plot_df <span class="sc">%&gt;%</span> </span>
<span id="cb108-3"><a href="chapter-stima-funzione-aposteriori.html#cb108-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>p_grid, <span class="at">xend=</span>p_grid, <span class="at">y=</span><span class="dv">0</span>, <span class="at">yend=</span>prior2)) <span class="sc">+</span></span>
<span id="cb108-4"><a href="chapter-stima-funzione-aposteriori.html#cb108-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-5"><a href="chapter-stima-funzione-aposteriori.html#cb108-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.17</span>) <span class="sc">+</span></span>
<span id="cb108-6"><a href="chapter-stima-funzione-aposteriori.html#cb108-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb108-7"><a href="chapter-stima-funzione-aposteriori.html#cb108-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb108-8"><a href="chapter-stima-funzione-aposteriori.html#cb108-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probabilità a priori&quot;</span>,</span>
<span id="cb108-9"><a href="chapter-stima-funzione-aposteriori.html#cb108-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;50 punti&quot;</span></span>
<span id="cb108-10"><a href="chapter-stima-funzione-aposteriori.html#cb108-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb108-11"><a href="chapter-stima-funzione-aposteriori.html#cb108-11" aria-hidden="true" tabindex="-1"></a>p4</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gridappr4"></span>
<img src="Psicometria_files/figure-html/gridappr4-1.png" alt="Rappresentazione di una funzione a priori informativa per il parametro $\theta$." width="70%" />
<p class="caption">
Figura 13.4: Rappresentazione di una funzione a priori informativa per il parametro <span class="math inline">\(\theta\)</span>.
</p>
</div>
<p>Calcoliamo il valore della funzione di verosimiglianza in corrispondenza di ciascun punto della griglia. La funzione di verosimiglianza è identica a quella considerata nell’esempio precedente.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="chapter-stima-funzione-aposteriori.html#cb109-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="dv">23</span>, <span class="at">size =</span> <span class="dv">30</span>, <span class="at">prob =</span> p_grid)</span></code></pre></div>
<p>Calcolo il prodotto tra la verosimiglianza e la distribuzione a priori, per ciascun punto della griglia:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="chapter-stima-funzione-aposteriori.html#cb110-1" aria-hidden="true" tabindex="-1"></a>unstd_posterior2 <span class="ot">&lt;-</span> likelihood <span class="sc">*</span> prior2</span></code></pre></div>
<p>Normalizzo la distribuzione a posteriori in modo tale che la somma sia 1.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="chapter-stima-funzione-aposteriori.html#cb111-1" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> unstd_posterior2 <span class="sc">/</span> <span class="fu">sum</span>(unstd_posterior2)</span>
<span id="cb111-2"><a href="chapter-stima-funzione-aposteriori.html#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(posterior2)</span>
<span id="cb111-3"><a href="chapter-stima-funzione-aposteriori.html#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>La nuova funzione a posteriori è rappresentata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr5">13.5</a>.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="chapter-stima-funzione-aposteriori.html#cb112-1" aria-hidden="true" tabindex="-1"></a>plot_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(p_grid, posterior2)</span>
<span id="cb112-2"><a href="chapter-stima-funzione-aposteriori.html#cb112-2" aria-hidden="true" tabindex="-1"></a>p5 <span class="ot">&lt;-</span> plot_df <span class="sc">%&gt;%</span></span>
<span id="cb112-3"><a href="chapter-stima-funzione-aposteriori.html#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p_grid, <span class="at">xend =</span> p_grid, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">yend =</span> posterior2)) <span class="sc">+</span></span>
<span id="cb112-4"><a href="chapter-stima-funzione-aposteriori.html#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span></span>
<span id="cb112-5"><a href="chapter-stima-funzione-aposteriori.html#cb112-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.17</span>) <span class="sc">+</span></span>
<span id="cb112-6"><a href="chapter-stima-funzione-aposteriori.html#cb112-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb112-7"><a href="chapter-stima-funzione-aposteriori.html#cb112-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Parametro \U03B8&quot;</span>,</span>
<span id="cb112-8"><a href="chapter-stima-funzione-aposteriori.html#cb112-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probabilità a posteriori&quot;</span></span>
<span id="cb112-9"><a href="chapter-stima-funzione-aposteriori.html#cb112-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb112-10"><a href="chapter-stima-funzione-aposteriori.html#cb112-10" aria-hidden="true" tabindex="-1"></a>p5</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gridappr5"></span>
<img src="Psicometria_files/figure-html/gridappr5-1.png" alt="Rappresentazione della funzione a posteriori per il parametro $\theta$ calcolata utilizzando una distribuzione a priori informativa." width="70%" />
<p class="caption">
Figura 13.5: Rappresentazione della funzione a posteriori per il parametro <span class="math inline">\(\theta\)</span> calcolata utilizzando una distribuzione a priori informativa.
</p>
</div>
<p>Facendo un confronto tra le figure <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr4">13.4</a> e <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr5">13.5</a> si nota come la distribuzione a priori per il parametro <span class="math inline">\(\theta\)</span> e la distribuzione a posteriori per il parametro <span class="math inline">\(\theta\)</span> sono molto diverse. In particolare, si noti che la distribuzione a posteriori rappresentata nella <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr5">13.5</a> risulta spostata verso destra su
posizioni più vicine a quelle della verosimiglianza, rappresentata nella figura <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr2">13.2</a>. Si noti anche, a causa dell’effetto della distribuzione a priori, le distribuzioni a posteriori riportate nelle figure <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr3">13.3</a> e <a href="chapter-stima-funzione-aposteriori.html#fig:gridappr5">13.5</a> sono molto diverse tra loro. Discuteremo in seguito l’influenza della distribuzione a priori sull’inferenza finale.</p>
</div>
<div id="sommario-della-funzione-a-posteriori" class="section level3" number="13.1.4">
<h3><span class="header-section-number">13.1.4</span> Sommario della funzione a posteriori</h3>
<p>Una volta calcolata la distribuzione a posteriori dobbiamo riassumerla in qualche modo. Nel caso in cui venga usato il metodo di approssimazione numerica, il problema del calcolo delle aree sottese alla funzione a posteriori in qualunque intervallo può essere risolto in vari modi. Tuttavia, questo problema trova una soluzione molto più semplice se viene utilizzato un metodo diverso per la stima della distribuzione a posteriori, come vedremo di seguito. Non discuteremo dunque la possibile soluzione di questo problema nel caso presente, in quanto il metodo dell’approssimazione numerica per il calcolo della distribuzione a posteriori è solo un esempio didattico.</p>
</div>
</div>
<div id="approssimazione-quadratica" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Approssimazione quadratica</h2>
<p>I metodi numerici convenzionali possono essere usati solo quando il numero di parametri da stimare è piccolo. La ragione di ciò sta nella cosiddetta “maledizione della dimensionalità.” Vediamo cosa significa. Nel caso di un solo parametro, supponiamo di utilizzare una griglia di
100 valori. Per due parametri avremo bisogno di <span class="math inline">\(100^2\)</span> valori. Ma già per 10 parametri avremo bisogno di <span class="math inline">\(10^{10}\)</span> valori – è facile capire che una tale quantità di valori è troppo grande anche per un computer potente come quello che utilizziamo normalmente. Dobbiamo dunque affrontare il problema in un altro modo.</p>
<p>Una possibile soluzione al nostro problema ci viene fornita dal metodo dell’approssimazione quadratica. La motivazione di tale metodo è la seguente. Sappiamo che, in generale, la regione della distribuzione a posteriori che si trova in prossimità del suo massimo può essere ben
approssimata dalla forma di una distribuzione Normale. Descrivere la distribuzione a posteriori mediante la distribuzione Normale significa utilizzare un’approssimazione che viene, appunto, chiamata “quadratica”<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<p>L’approssimazione quadratica si pone due obiettivi.</p>
<ol style="list-style-type: decimal">
<li><p>Trovare la moda della distribuzione a posteriori. Ci sono varie
procedure di ottimizzazione, implementate in R, in
grado di trovare il massimo di una distribuzione.</p></li>
<li><p>Stimare la curvatura della distribuzione in prossimità della moda.
Una stima della curvatura è sufficiente per trovare
un’approssimazione quadratica dell’intera distribuzione. In alcuni
casi, questi calcoli possono essere fatti seguendo una procedura
analitica, ma solitamente vengono usate delle tecniche numeriche.</p></li>
</ol>
<p>Una descrizione della distribuzione a posteriori ottenuta mediante l’approssimazione quadratica si ottiene mediante la funzione <code>quap()</code> contenuta nel pacchetto <code>rethinking</code>. Tale pacchetto, creato da Richard McElreath per accompagnare il suo testo <em>Statistical Rethinking</em><span class="math inline">\(^2\)</span>, può essere scaricato utilizzando le istruzioni seguenti<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="chapter-stima-funzione-aposteriori.html#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">&quot;coda&quot;</span>, <span class="st">&quot;mvtnorm&quot;</span>, <span class="st">&quot;devtools&quot;</span>, <span class="st">&quot;loo&quot;</span>, <span class="st">&quot;dagitty&quot;</span>))</span>
<span id="cb113-2"><a href="chapter-stima-funzione-aposteriori.html#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;devtools&quot;</span>)</span>
<span id="cb113-3"><a href="chapter-stima-funzione-aposteriori.html#cb113-3" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;rmcelreath/rethinking&quot;</span>)</span></code></pre></div>
<p>Vedremo nel capitolo XX come tale funzione possa essere usata<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. Dal nostro punto di vista non è importante capire come si svolgono in pratica i calcoli necessari per la stima della distribuzione a posteriori con il metodo dell’approssimazione quadratica. Quello che è importante capire è il significato della distribuzione a posteriori e questo significato è stato chiarito nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:appross_numerica">La procedura dell’approssimazione numerica</a>. L’approssimazione quadratica fornisce risultati simili (o identici) a quelli ottenuti con il metodo descritto nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:appross_numerica">La procedura dell’approssimazione numerica</a>. Il vantaggio dell’approssimazione quadratica è che disponiamo di una serie di funzioni R che svolgono tutti i calcoli per noi.</p>
<p>In realtà, l’approssimazione quadratica è poco usata in pratica, perché per problemi complessi è più conveniente usare i metodi Monte Carlo basati su Catena di Markov (MCMC) che verranno descritti nella successiva sezione <a href="chapter-stima-funzione-aposteriori.html#sec:integr_MC">Integrazione con metodo Monte Carlo</a>. Per potere utilizzare i metodi MCMC è necessario installare sul proprio computer del software aggiuntivo e tale operazione, talvolta, può risultare complessa. Non è l’obiettivo di questo insegnamento affrontare questo problema Per questa ragione, per svolgere gli esercizi che discuteremo sarà sufficiente fare ricorso al metodo dell’approssimazione quadratica; ovvero sarà sufficiente usare la funzione <code>rethinking::quap()</code>.</p>
</div>
<div id="sec:integr_MC" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Integrazione con metodo Monte Carlo</h2>
<p>Prima di introdurre i metodi MCMC per la stima della funzione a posteriori, spendiamo due parole sul metodo Monte Carlo quale tecnica che consente il calcolo degli integrali mediante simulazione numerica. Il termine Monte-Carlo si riferisce al fatto che per la computazione si ricorre ad un ripetuto campionamento casuale attraverso la generazioni di sequenze di numeri casuali.</p>
<div id="sec:legge_forte_grandi_numeri" class="section level3" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> Legge forte dei grandi numeri</h3>
<p>L’integrazione con metodo Monte Carlo trova la sua giustificazione nella <em>Legge forte dei grandi numeri</em> la quale può essere espressa nei termini seguenti. Data una successione di variabili casuali <span class="math inline">\(Y_{1}, Y_{2},\dots, Y_{n},\dots\)</span> indipendenti e identicamente distribuite con media <span class="math inline">\(\mu\)</span>, ne segue che
<span class="math display">\[
P\left( \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n Y_i = \mu \right) = 1.
\]</span>
Ciò significa che, al crescere di <span class="math inline">\(n\)</span>, la media delle realizzazioni di <span class="math inline">\(Y_{1}, Y_{2},\dots, Y_{n},\dots\)</span> converge con probabilità 1 al vero valore <span class="math inline">\(\mu\)</span>.</p>
<p>Un esempio della legge forte dei grandi numeri riguarda una serie di lanci di una moneta dove <span class="math inline">\(Y=1\)</span> significa “testa” e <span class="math inline">\(Y=0\)</span> significa “croce.” Per la legge forte dei grandi numeri, nel caso di una moneta equilibrata la proporzione di eventi “testa” converge alla vera probabilità dell’evento “testa”
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n Y_i \rightarrow \frac{1}{2}
\]</span>
con probabilità di uno.</p>
<p>Quello che è stato detto sopra non è che un modo sofisticato per dire che, se vogliamo calcolare un’approssimazione del valore atteso di una variabile aleatoria, non dobbiamo fare altro che la media aritmetica di un grande numero di realizzazione di tale variabile aleatoria. Come è facile intuire, l’approssimazione migliora al crescere del numero di dati che abbiamo a disposizione.</p>
</div>
</div>
<div id="sec:mmmc" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Metodi MC basati su Catena di Markov</h2>
<p>I metodi Monte Carlo basati su Catena di Markov consentono di costruire sequenze di punti (le “catene”) nello spazio dei parametri, la cui densità è proporzionale alla distribuzione di probabilità a posteriori a cui siamo interessati. Questo, evidentemente, è il risultato vorremmo
ottenere. Ma cosa sono le catene di Markov? In termini formali possiamo dire che una catena di Markov è una sequenza di variabili aleatorie <span class="math inline">\(Y_{1}, Y_{2},\dots, Y_{n}\)</span> tale che la dipendenza della distribuzione di <span class="math inline">\(Y_{i+1}\)</span> dai valori di <span class="math inline">\(Y_{1}, \dots, Y_{i}\)</span> è interamente dovuta al
valore di <span class="math inline">\(Y_i\)</span>, cioè il passaggio ad uno stato del sistema dipende unicamente dallo stato immediatamente precedente e non dal come si è giunti a tale stato (dalla storia). Per questo motivo si dice che un processo markoviano è senza memoria.</p>
<p>In generale è possibile generare catene di Markov che convergono ad una soluzione unica e stazionaria tale per cui gli elementi della catena sono campioni dalla distribuzione di interesse. Nel caso dell’inferenza Bayesiana la distribuzione di interesse è la distribuzione a posteriori, <span class="math inline">\(p(\theta \mid x)\)</span>. Le catene di Markov possono quindi essere utilizzate per stimare i valori di aspettazione di variabili rispetto alla distribuzione a posteriori. In altre parole, possiamo utilizzare le catene di Markov per stimare i valori a posteriori dei parametri sconosciuti di un modello statistico – un esempio è il parametro <span class="math inline">\(p\)</span> nel problema del mappamondo che abbiamo discusso in precedenza.</p>
<p>La generazioni di elementi di una catena ha una natura probabilistica e esistono diversi algoritmi per costruire catene di Markov. Due aspetti da tenere in considerazione sotto questo punto di vista sono il periodo di <em>burn-in</em> e le correlazioni tra punti. Al crescere degli step della catena si ottiene una migliore approssimazione della distribuzione target. All’inizio del campionamento però la distribuzione può essere significativamente lontana dalla distribuzione stazionaria. Ci vuole un certo tempo prima di raggiungere la distribuzione stazionaria di equilibrio e tale periodo è detto di <em>burn-in</em>. Perciò i campioni provenienti da tale parte iniziale della catena vanno tipicamente scartati poiché non rappresentano accuratamente la distribuzione desiderata.</p>
<p>Normalmente, un algoritmo MCMC genera catene di Markov di campioni, ognuno dei quali è autocorrelato a quelli generati immediatamente prima e dopo di lui. Conseguentemente campioni successivi non sono indipendenti ma formano una catena di Markov con un certo grado di
correlazione. Questa correlazione introduce una distorsione nella soluzione che si ottiene con questo metodo. L’arte dei diversi algoritmi MCMC risiede nel rendere il meccanismo efficiente e capace di produrre un risultato non distorto, il che implica la riduzione al minimo del tempo di <em>burn-in</em> e della correlazione tra i diversi campioni.</p>
<p>Presentiamo ora, in una forma intuitiva, l’algoritmo di Metropolis, ovvero il primo algoritmo MCMC che è stato proposto. Tale algoritmo è stato sviluppato in seguito per renderlo via via più efficiente. Il nostro obiettivo, però, è solo quello di capire la logica sottostante – lasciamo che siano gli ingegneri a risolvere il problema di rendere l’algoritmo più efficiente.</p>
<div id="sec:turista_viagg" class="section level3" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Il problema del turista viaggiatore</h3>
<p>L’algoritmo di Metropolis è stato presentato usando varie metafore: quella di un politico che viaggia tra isole diverse <span class="citation">(<a href="bibliografia.html#ref-doing_bayesian_data_an" role="doc-biblioref">Kruschke, 2014</a>)</span>, o quella di un re che, anche lui, si sposta tra le isole di un arcipelago <span class="citation">(<a href="bibliografia.html#ref-McElreath_rethinking" role="doc-biblioref">McElreath, 2020</a>)</span>. Qui mutiamo leggermente la metafora e immaginiamo un turista in vacanza su un’isola che dispone di 10 spiagge di grandezza diversa. Muovendosi in senso orario, la grandezza delle spiagge aumenta: partendo dalla spiaggia più piccola si arriva ad una spiaggia un po’ più grande, via via fino ad arrivare all’ultima spiaggia, la decima, che è la più grande di tutte. Quindi indicheremo con i numeri da 1 a 10 le spiagge dell’isola. Tali numeri rappresentano anche la grandezza (relativa) di ciascuna spiaggia. Dato che l’isola è circolare, la decima spiaggia confina con la prima spiaggia.</p>
<p>Nella nostra metafora, immaginiamo un turista in vacanza sull’isola che abbiamo appena descritto. Per non annoiarsi, il nostro turista vuole passare un po’ di tempo su ogni spiaggia, ma con il vincolo che il tempo passato su ciascuna spiaggia deve essere proporzionale alla grandezza della spiaggia. Infatti, il turista preferisce le spiagge più grandi; nel contempo, però, vuole anche visitare spiagge diverse, quindi il vincolo descritto sopra sembra un buon compromesso tra il desiderio di cambiare spiaggia di tanto in tanto e il desiderio di passare più tempo sulle spiagge più grandi.</p>
<p>Essendo in vacanza, il turista non vuole preparare un calendario che stabilisca in anticipo la spiaggia da visitare ogni giorno, ma vuole decidere in maniera rilassata e un po’ casuale, ogni mattina, restando però fedele al vincolo che si è dato. Al bar incontra un altro turista, l’ingegnere Metropolis, che gli suggerisce come fare per ottenere l’obiettivo che si è prefissato. Seguendo le istruzioni di Metropolis, il nostro turista decide di comportarsi nel modo seguente.</p>
<ol style="list-style-type: decimal">
<li><p>Ogni mattina decide tra due alternative: ritornare sulla
spiaggia dove era stato il giorno prima (chiamiamola spiaggia
<em>corrente</em>) oppure andare in una delle due spiagge contigue.</p></li>
<li><p>Lancia una moneta. Se esce testa, considera la possibilità di andare
nella spiaggia a che confina con la spiaggia corrente muovendosi in
senso orario; se esce croce, considera la possibilità di andare
nella spiaggia a che confina con la spiaggia corrente muovendosi in
senso antiorario. La spiaggia individuata in questo modo viene
chiamata spiaggia <em>proposta</em>.</p></li>
<li><p>Dopo avere trovato la spiaggia proposta, il turista deve decidere se
effettivamente andare lì oppure no e, per decidere,
procede in questo modo. Prende un numero di conchiglie proporzionale
alla grandezza della spiaggia proposta – per esempio, se la
spiaggia proposta è la numero 7, allora prenderà 7 conchiglie.
Prende un numero di sassolini proporzionale alla grandezza della
spiaggia corrente – per esempio, se la spiaggia corrente è la
numero 6, allora prenderà 6 sassolini.</p></li>
<li><p>Se il numero di conchiglie è maggiore del numero di sassolini, il
turista si sposta sempre nella spiaggia proposta. Ma se ci sono meno
conchiglie che sassolini, scarta un numero di sassolini uguale al
numero di conchiglie e mette gli oggetti rimanenti in un sacchetto
– per esempio, se la spiaggia proposta è la 5 e la spiaggia
corrente è la 6, allora metterà nel sacchetto 5 conchiglie e 1
sassolino. Mescola bene ed estrae dal sacchetto un oggetto: se è una
conchiglia si sposta nella spiaggia proposta, se è un sassolino
resta nella spiaggia corrente. Di conseguenza, la probabilità che il
turista cambi spiaggia (ovvero <span class="math inline">\(\frac{5}{6}\)</span>) è uguale al numero di
conchiglie diviso per il numero originale di sassolini.</p></li>
</ol>
<p>Decidere di procedere in questo modo potrebbe sembrare un modo per rovinarsi le vacanze. Invece, questo algoritmo funziona! Seguendo la proposta di Metropolis, il turista passerà su ciascuna
spiaggia un numero di giorni proporzionale alla grandezza della spiaggia.</p>
<p><span class="citation"><a href="bibliografia.html#ref-McElreath_rethinking" role="doc-biblioref">McElreath</a> (<a href="bibliografia.html#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span> ha implementato in R l’algoritmo di Metropolis che abbiamo descritto sopra nel modo seguente:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="chapter-stima-funzione-aposteriori.html#cb114-1" aria-hidden="true" tabindex="-1"></a>num_weeks <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb114-2"><a href="chapter-stima-funzione-aposteriori.html#cb114-2" aria-hidden="true" tabindex="-1"></a>positions <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, num_weeks)</span>
<span id="cb114-3"><a href="chapter-stima-funzione-aposteriori.html#cb114-3" aria-hidden="true" tabindex="-1"></a>current <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb114-4"><a href="chapter-stima-funzione-aposteriori.html#cb114-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_weeks) {</span>
<span id="cb114-5"><a href="chapter-stima-funzione-aposteriori.html#cb114-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># record current position</span></span>
<span id="cb114-6"><a href="chapter-stima-funzione-aposteriori.html#cb114-6" aria-hidden="true" tabindex="-1"></a>  positions[i] <span class="ot">&lt;-</span> current</span>
<span id="cb114-7"><a href="chapter-stima-funzione-aposteriori.html#cb114-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># flip coin to generate proposal</span></span>
<span id="cb114-8"><a href="chapter-stima-funzione-aposteriori.html#cb114-8" aria-hidden="true" tabindex="-1"></a>  proposal <span class="ot">&lt;-</span> current <span class="sc">+</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb114-9"><a href="chapter-stima-funzione-aposteriori.html#cb114-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># now make sure he loops around the archipelago</span></span>
<span id="cb114-10"><a href="chapter-stima-funzione-aposteriori.html#cb114-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (proposal <span class="sc">&lt;</span> <span class="dv">1</span>) proposal <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb114-11"><a href="chapter-stima-funzione-aposteriori.html#cb114-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (proposal <span class="sc">&gt;</span> <span class="dv">10</span>) proposal <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb114-12"><a href="chapter-stima-funzione-aposteriori.html#cb114-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># move?</span></span>
<span id="cb114-13"><a href="chapter-stima-funzione-aposteriori.html#cb114-13" aria-hidden="true" tabindex="-1"></a>  prob_move <span class="ot">&lt;-</span> proposal <span class="sc">/</span> current</span>
<span id="cb114-14"><a href="chapter-stima-funzione-aposteriori.html#cb114-14" aria-hidden="true" tabindex="-1"></a>  current <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> prob_move, proposal, current)</span>
<span id="cb114-15"><a href="chapter-stima-funzione-aposteriori.html#cb114-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Le istruzioni seguenti sono state usate per generare la figura <a href="chapter-stima-funzione-aposteriori.html#fig:turista1">13.6</a>. Se guardiamo la figura e consideriamo un giorno qualsiasi è difficile capire qual è la spiaggia scelta dal turista.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="chapter-stima-funzione-aposteriori.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(</span>
<span id="cb115-2"><a href="chapter-stima-funzione-aposteriori.html#cb115-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">y =</span> positions[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>]),</span>
<span id="cb115-3"><a href="chapter-stima-funzione-aposteriori.html#cb115-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(x, y)</span>
<span id="cb115-4"><a href="chapter-stima-funzione-aposteriori.html#cb115-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb115-5"><a href="chapter-stima-funzione-aposteriori.html#cb115-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>) <span class="sc">+</span></span>
<span id="cb115-6"><a href="chapter-stima-funzione-aposteriori.html#cb115-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb115-7"><a href="chapter-stima-funzione-aposteriori.html#cb115-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Giorno&quot;</span>,</span>
<span id="cb115-8"><a href="chapter-stima-funzione-aposteriori.html#cb115-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Isola&quot;</span></span>
<span id="cb115-9"><a href="chapter-stima-funzione-aposteriori.html#cb115-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb115-10"><a href="chapter-stima-funzione-aposteriori.html#cb115-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:turista1"></span>
<img src="Psicometria_files/figure-html/turista1-1.png" alt="Risultati dell'algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra la spiaggia scelta dal turista (asse verticale) in funzione di ciascun giorno della sua vacanza (asse orizzontale). " width="70%" />
<p class="caption">
Figura 13.6: Risultati dell’algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra la spiaggia scelta dal turista (asse verticale) in funzione di ciascun giorno della sua vacanza (asse orizzontale).
</p>
</div>
<p>Tuttavia, se esaminiamo la figura <a href="chapter-stima-funzione-aposteriori.html#fig:turista2">13.7</a> che descrive il comportamento a lungo termine dell’algoritmo, ci rendiamo conto che l’algoritmo ha prodotto il risultato che si voleva ottenere: il tempo trascorso dal turista su ciascuna spiaggia è proporzionale alla grandezza della spiaggia.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="chapter-stima-funzione-aposteriori.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(</span>
<span id="cb116-2"><a href="chapter-stima-funzione-aposteriori.html#cb116-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">y =</span> <span class="fu">as.numeric</span>(<span class="fu">table</span>(positions))),</span>
<span id="cb116-3"><a href="chapter-stima-funzione-aposteriori.html#cb116-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">yend =</span> y)</span>
<span id="cb116-4"><a href="chapter-stima-funzione-aposteriori.html#cb116-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb116-5"><a href="chapter-stima-funzione-aposteriori.html#cb116-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">color =</span> <span class="st">&quot;#8184FC&quot;</span>, <span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb116-6"><a href="chapter-stima-funzione-aposteriori.html#cb116-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb116-7"><a href="chapter-stima-funzione-aposteriori.html#cb116-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Isola&quot;</span>,</span>
<span id="cb116-8"><a href="chapter-stima-funzione-aposteriori.html#cb116-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Numero di giorni&quot;</span></span>
<span id="cb116-9"><a href="chapter-stima-funzione-aposteriori.html#cb116-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb116-10"><a href="chapter-stima-funzione-aposteriori.html#cb116-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:turista2"></span>
<img src="Psicometria_files/figure-html/turista2-1.png" alt="Risultati dell'algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra che il numero di volte in cui ciascuna spiaggia è stata visitata è proporzionale alla grandezza della spiaggia." width="70%" />
<p class="caption">
Figura 13.7: Risultati dell’algoritmo di Metropolis utilizzato dal turista viaggiatore. La figura mostra che il numero di volte in cui ciascuna spiaggia è stata visitata è proporzionale alla grandezza della spiaggia.
</p>
</div>
<p>L’algoritmo di Metropolis funziona anche se il turista decide di spostarsi dalla spiaggia corrente a qualunque altra spiaggia, non solo su quelle confinanti. Inoltre, l’algoritmo funziona per qualunque numero di spiagge e anche se il turista non sa quante spiagge ci sono sull’isola. Affinché l’algoritmo funzioni è solo necessario conoscere la grandezza della spiaggia “corrente” e quella della spiaggia “proposta.”</p>
</div>
<div id="lalgoritmo-di-metropolis" class="section level3" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> L’algoritmo di Metropolis</h3>
<p>L’algoritmo descritto nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:turista_viagg">Il problema del turista viaggiatore</a> è un caso speciale dell’algoritmo di Metropolis e l’algoritmo di Metropolis è un caso speciale dei
metodi MCMC. L’algoritmo di Metropolis, al di là dell’uso che ne fa il fortunato turista dell’esempio discusso in precedenza, viene in realtà impiegato per per ottenere una sequenza di campioni casuali da una distribuzione a posteriori la cui forma è, solitamente, sconosciuta. Fuor di metafora:</p>
<ul>
<li><p>i <strong>numeri che identificano ciascuna spiaggia</strong> corrispondono ai valori del parametro che vogliamo stimare – non è necessario che il parametro assuma solo valori discreti, può anche assumere un insieme continuo di valori;</p></li>
<li><p>la <strong>grandezza della spiaggia</strong> corrisponde alla densità a posteriori associata a ciascuno dei possibili valori del parametro;</p></li>
<li><p>i <strong>giorni di permanenza su una spiaggia</strong> corrispondono al numero di campioni estratti dalla distribuzione a posteriori.</p></li>
</ul>
<p>L’aspetto cruciale di questa discussione è il fatto che, all’aumentare delle ripetizioni dell’algoritmo di Metropolis, la distribuzione dei valori così ottenuti diventa via via più simile alla distribuzione a posteriori del parametro <span class="math inline">\(\theta\)</span>, anche se questa è sconosciuta. Per un grande numero di passi della catena l’approssimazione è sufficiente. Con questo metodo è dunque possibile generare un grande numero di campioni casuali dalla distribuzione a posteriori per poi poterne calcolare misure di sintesi e potere fare inferenza.</p>
</div>
<div id="sec:metropolis_one_mean" class="section level3" number="13.4.3">
<h3><span class="header-section-number">13.4.3</span> Una applicazione concreta</h3>
<!-- https://www.youtube.com/watch?app=desktop&v=U561HGMWjcw -->
<p>L’algoritmo di Metropolis consente di effettuare quello che viene chiamato un <em>dependent sampling</em>, ovvero ci consente di generare campioni casuali dalla distribuzione a posteriori utilizzando soltanto il numeratore del teorema di Bayes:</p>
<p><span class="math display">\[
P(\theta \mid x) = \frac{P(x \mid \theta)P(\theta)}{P(x)}
\]</span>
ovvero</p>
<p><span class="math display">\[
P(\theta \mid x) \propto P(x \mid \theta)P(\theta)
\]</span>
L’algoritmo di Metropolis è la versione più semplice e più conosciuta degli algoritmi MCMC. Vediamo come funziona.</p>
<ul>
<li><p>Per prima cosa troviamo un valore casuale del parametro estraendolo da una distribuzione “proposta”: <span class="math inline">\(\theta_0 \sim \Pi(\theta)\)</span>. La distribuzione proposta può essere qualunque distribuzione, anche se, idealmente, è meglio che sia simile alla distribuzione a posteriori. Ma in pratica la distribuzione a posteriori è sconosciuta e quindi utilizziamo un qualche metodo arbitrario di iniziare la catena di Markov (ovvero utilizziamo un valore iniziale arbitrario).</p></li>
<li><p>In ciascuna iterazione <span class="math inline">\(t\)</span> viene proposto un nuovo valore del parametro, <span class="math inline">\(\theta&#39;_t\)</span>. Il valore <span class="math inline">\(\theta&#39;_t\)</span> viene estratto in maniera casuale da una qualsiasi distribuzione simmetrica centrata sul valore del parametro dell’interazione precedente, <span class="math inline">\(t-1\)</span>. Ad esempio, possiamo usare la distribuzione Normale con una appropriata deviazione standard: <span class="math inline">\(\theta_t \sim \mathcal{N}(\theta_{t-1}, \sigma)\)</span>. In pratica, questo significa che il valore proposto del parametro sarà un valore nella prossimità di quello attualmente considerato.</p></li>
<li><p>Calcoliamo poi il rapporto <span class="math inline">\(r\)</span> tra la distribuzione a posteriori non normalizzata determinata dal valore proposto <span class="math inline">\(\theta&#39;_t\)</span> e la distribuzione a posteriori non normalizzata determinata dal valore del parametro <span class="math inline">\(\theta&#39;_{t-1}\)</span> dell’iterazione precedente della catena: <span class="math inline">\(r = \frac{P(x \mid \theta&#39;_t) P(\theta&#39;_t)}{P(x \mid \theta&#39;_{t-1}) P(\theta&#39;_{t-1})}\)</span>. Soffermiamoci su tale formula per capire bene cosa significa. La distribuzione a posteriori non normalizzata corrisponde al numeratore del teorema di Bayes, ovvero <span class="math inline">\(P(x \mid \theta) P(\theta)\)</span>, laddove <span class="math inline">\(P(x \mid \theta)\)</span> è la verosimiglianza di <span class="math inline">\(x\)</span> dato <span class="math inline">\(\theta\)</span> e <span class="math inline">\(P(\theta)\)</span> è la distribuzione a priori di <span class="math inline">\(\theta\)</span>. Abbiamo visto nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:aspett_future_beta2_10">Un esempio pratico (versione 2)</a> che ciascuna di tali densità può essere rappresentata mediante una curva e che il prodotto di due densità si ottiene facendo il prodotto dei valori delle ordinate corrispondenti di discuna delle due curve. Il numeratore del teorema di Bayes ci fornisce la distribuzione a posteriori non normalizzata in quanto l’area sottesa alla curva così ottenuta non è unitaria (quindi tale curva non rappresenta una funzione di densità). Dato che qui facciamo un rapporto, però, questo è irrilevante. Al numeratore del rapporto <span class="math inline">\(r\)</span> dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza in corrispondenza del valore proposto <span class="math inline">\(x = \theta&#39;_t\)</span> e la densità della distribuzione a priori in corrispondenza del valore proposto <span class="math inline">\(x = \theta&#39;_t\)</span>. In maniera corrispondente, al denominatore del rapporto <span class="math inline">\(r\)</span> dobbiamo fare il prodotto tra due scalari: la densità (l’ordinata) della funzione di verosimiglianza in corrispondenza del valore <span class="math inline">\(\theta_{t-1}\)</span> e la densità della distribuzione a priori in corrispondenza del valore <span class="math inline">\(\theta_{t-1}\)</span>.</p></li>
<li><p>Utilizziamo poi il valore del rapporto <span class="math inline">\(r\)</span> per decidere se dobbiamo effettivamente muoverci nella nuova posizione <span class="math inline">\(\theta&#39;_t\)</span>, oppure se dobbiamo campionare un diverso valore <span class="math inline">\(\theta&#39;_t\)</span>. Per decidere, confrontiamo il valore <span class="math inline">\(r\)</span> con un valore casuale estratto da una distribuzione uniforme che assume valori tra zero e uno: <span class="math inline">\(U(0, 1)\)</span>. Se <span class="math inline">\(r &gt; u \sim U(0, 1)\)</span> allora accettiamo <span class="math inline">\(\theta&#39;_t\)</span> e la catena si muove in quella nuova posizione, ovvero <span class="math inline">\(\theta_t = \theta&#39;_t\)</span>. Altrimenti <span class="math inline">\(\theta_t = \theta_{t-1}\)</span> e ripetiamo la procedura descritta sopra campionando un nuovo valore <span class="math inline">\(\theta&#39;_t\)</span>.</p></li>
</ul>
<p>Per fare un esempio concreto, consideriamo nuovamente i 30 pazienti esaminati da <span class="citation"><a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al.</a> (<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">2019</a>)</span> e discussi nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:es_pratico_zetsche">Un esempio pratico</a>. Di essi, 23 hanno manifestato delle aspettative distorte negativamente sul loro stato d’animo futuro. Utilizzando l’algoritmo di Metropolis, ci poniamo il problema di ottenere la stima a posteriori di <span class="math inline">\(\theta\)</span> (probabilità di manifestare un’aspettativa distorta negativamente) dati i 23 “successi” in 30 prove e usando la stessa distribuzione a priori per <span class="math inline">\(\theta\)</span> che è stata usata nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:aspett_future_beta2_10">Un esempio pratico (versione 2)</a>.</p>
<div id="verosimiglianza" class="section level4" number="13.4.3.1">
<h4><span class="header-section-number">13.4.3.1</span> Verosimiglianza</h4>
<p>Per trovare la funzione di verosimiglianza usando i 30 valori di <span class="citation"><a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">Zetsche et al.</a> (<a href="bibliografia.html#ref-zetsche_future_2019" role="doc-biblioref">2019</a>)</span> definisco la funzione <code>likelihood()</code> come indicato sotto. Tale funzione ritorna l’ordinata della funzione di verosimiglianza binomiale per ciascun valore del vettore <code>param</code> che viene dato in input alla funzione.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="chapter-stima-funzione-aposteriori.html#cb117-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">23</span></span>
<span id="cb117-2"><a href="chapter-stima-funzione-aposteriori.html#cb117-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb117-3"><a href="chapter-stima-funzione-aposteriori.html#cb117-3" aria-hidden="true" tabindex="-1"></a>param <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb117-4"><a href="chapter-stima-funzione-aposteriori.html#cb117-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-5"><a href="chapter-stima-funzione-aposteriori.html#cb117-5" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(param, <span class="at">x =</span> <span class="dv">23</span>, <span class="at">N =</span> <span class="dv">30</span>) {</span>
<span id="cb117-6"><a href="chapter-stima-funzione-aposteriori.html#cb117-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dbinom</span>(x, N, param)</span>
<span id="cb117-7"><a href="chapter-stima-funzione-aposteriori.html#cb117-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb117-8"><a href="chapter-stima-funzione-aposteriori.html#cb117-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-9"><a href="chapter-stima-funzione-aposteriori.html#cb117-9" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x=</span>param, <span class="at">y=</span><span class="fu">likelihood</span>(param)) <span class="sc">%&gt;%</span> </span>
<span id="cb117-10"><a href="chapter-stima-funzione-aposteriori.html#cb117-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb117-11"><a href="chapter-stima-funzione-aposteriori.html#cb117-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb117-12"><a href="chapter-stima-funzione-aposteriori.html#cb117-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb117-13"><a href="chapter-stima-funzione-aposteriori.html#cb117-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(theta),</span>
<span id="cb117-14"><a href="chapter-stima-funzione-aposteriori.html#cb117-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Verosimiglianza&quot;</span></span>
<span id="cb117-15"><a href="chapter-stima-funzione-aposteriori.html#cb117-15" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="Psicometria_files/figure-html/unnamed-chunk-131-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="distribuzione-a-priori-1" class="section level4" number="13.4.3.2">
<h4><span class="header-section-number">13.4.3.2</span> Distribuzione a priori</h4>
<p>Se abbiamo ragioni forti per avere delle aspettative rispetto al valore possibile della nostra stima, una distribuzione a priori <em>informativa</em> verrà combinata con le informazioni fornite dal campione per produrre una stima ``razionale’’ a posteriori. Nel caso presente utilizziamo la distribuzione informativa presentata nella sezione <a href="chapter-stima-funzione-aposteriori.html#sec:aspett_future_beta2_10">Un esempio pratico (versione 2)</a> unicamente a scopo esemplicativo, ovvero per fare in modo da “allontanare” la distribuzione a posteriori dalla distribuzione di verosimiglianza.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="chapter-stima-funzione-aposteriori.html#cb118-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="cf">function</span>(param, <span class="at">alpha =</span> <span class="dv">2</span>, <span class="at">beta =</span> <span class="dv">10</span>) {</span>
<span id="cb118-2"><a href="chapter-stima-funzione-aposteriori.html#cb118-2" aria-hidden="true" tabindex="-1"></a>  param_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb118-3"><a href="chapter-stima-funzione-aposteriori.html#cb118-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dbeta</span>(param, alpha, beta) <span class="co"># / sum(dbeta(param_vals, alpha, beta))</span></span>
<span id="cb118-4"><a href="chapter-stima-funzione-aposteriori.html#cb118-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb118-5"><a href="chapter-stima-funzione-aposteriori.html#cb118-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-6"><a href="chapter-stima-funzione-aposteriori.html#cb118-6" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x=</span>param, <span class="at">y=</span><span class="fu">prior</span>(param)) <span class="sc">%&gt;%</span> </span>
<span id="cb118-7"><a href="chapter-stima-funzione-aposteriori.html#cb118-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb118-8"><a href="chapter-stima-funzione-aposteriori.html#cb118-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb118-9"><a href="chapter-stima-funzione-aposteriori.html#cb118-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb118-10"><a href="chapter-stima-funzione-aposteriori.html#cb118-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(theta),</span>
<span id="cb118-11"><a href="chapter-stima-funzione-aposteriori.html#cb118-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità&quot;</span></span>
<span id="cb118-12"><a href="chapter-stima-funzione-aposteriori.html#cb118-12" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="Psicometria_files/figure-html/unnamed-chunk-132-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="distribuzione-a-posteriori" class="section level4" number="13.4.3.3">
<h4><span class="header-section-number">13.4.3.3</span> Distribuzione a posteriori</h4>
<p>Abbiamo visto in precedenza come la funzione a posteriori è data dal prodotto della densità a priori e della verosimiglianza.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="chapter-stima-funzione-aposteriori.html#cb119-1" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(param) {</span>
<span id="cb119-2"><a href="chapter-stima-funzione-aposteriori.html#cb119-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">likelihood</span>(param) <span class="sc">*</span> <span class="fu">prior</span>(param)</span>
<span id="cb119-3"><a href="chapter-stima-funzione-aposteriori.html#cb119-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb119-4"><a href="chapter-stima-funzione-aposteriori.html#cb119-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-5"><a href="chapter-stima-funzione-aposteriori.html#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x=</span>param, <span class="at">y=</span><span class="fu">posterior</span>(param)) <span class="sc">%&gt;%</span> </span>
<span id="cb119-6"><a href="chapter-stima-funzione-aposteriori.html#cb119-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb119-7"><a href="chapter-stima-funzione-aposteriori.html#cb119-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb119-8"><a href="chapter-stima-funzione-aposteriori.html#cb119-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb119-9"><a href="chapter-stima-funzione-aposteriori.html#cb119-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(theta),</span>
<span id="cb119-10"><a href="chapter-stima-funzione-aposteriori.html#cb119-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità&quot;</span></span>
<span id="cb119-11"><a href="chapter-stima-funzione-aposteriori.html#cb119-11" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="Psicometria_files/figure-html/unnamed-chunk-133-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Questo è il risultato che vogliamo ottenere utilizzando l’algoritmo di Metropolis. Dalla figura precedente vediamo che la moda della distribuzione a posteriori è pari a circa 0.6. Questo è il valore più verosimile a posteriori per il parametro <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="algoritmo-di-metropolis" class="section level4" number="13.4.3.4">
<h4><span class="header-section-number">13.4.3.4</span> Algoritmo di Metropolis</h4>
<p>Implementiamo ora l’algoritmo di Metropolis. Utilizziamo una distribuzione proposta gaussiana. Il valore proposto da tale distribuzione ausiliaria corrisponde ad un valore selezionato a caso da una distribuzione gaussiana con media uguale al valore del parametro attualmente considerato nella catena e con una deviazione standard ``adeguata’’. In questo esempio, la deviazione standard è stata scelta empiricamente in modo tale da ottenere un tasso di accettazione sensato. È stato mostrato che un tasso di accettazione ottimale dovrebbe essere tra il 20% e il 30%. Se il tasso di accettazione è troppo grande, infatti, l’algoritmo esplora uno spazio troppo ristretto della distribuzione a posteriori. Il tasso di accettazione è influenzato dalla distribuzione proposta: in generale, tanto più la distribuzione proposta è simile alla distribuzione target, tanto più alto diventa il tasso di accettazione.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="chapter-stima-funzione-aposteriori.html#cb120-1" aria-hidden="true" tabindex="-1"></a>proposal_distribution <span class="ot">&lt;-</span> <span class="cf">function</span>(param) {</span>
<span id="cb120-2"><a href="chapter-stima-funzione-aposteriori.html#cb120-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span>(<span class="dv">1</span>) {</span>
<span id="cb120-3"><a href="chapter-stima-funzione-aposteriori.html#cb120-3" aria-hidden="true" tabindex="-1"></a>    res <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> param, <span class="at">sd =</span> <span class="fl">0.9</span>)</span>
<span id="cb120-4"><a href="chapter-stima-funzione-aposteriori.html#cb120-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (res <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;</span> res <span class="sc">&lt;</span> <span class="dv">1</span>)</span>
<span id="cb120-5"><a href="chapter-stima-funzione-aposteriori.html#cb120-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb120-6"><a href="chapter-stima-funzione-aposteriori.html#cb120-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-7"><a href="chapter-stima-funzione-aposteriori.html#cb120-7" aria-hidden="true" tabindex="-1"></a>  res</span>
<span id="cb120-8"><a href="chapter-stima-funzione-aposteriori.html#cb120-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>In questa implementazione molto semplice della distribuzione proposta ho inserito dei controlli che fanno in modo che il valore proposto da tale distribuzione ausiliaria sia incluso nell’intervallo [0, 1]. Si possono trovare implementazioni migliori di questa idea di quella fornita qui. Ma lo scopo è solo quello di spiegare la struttura logica dell’algoritmo di Metropolis, non quella di proporre un’implementazione efficente dell’algoritmo. Per i nostri scopi, tale implementazione “ingenua” funziona, e tanto basta.</p>
<p>L’algoritmo di Metropolis è implementato nella funzione seguente:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="chapter-stima-funzione-aposteriori.html#cb121-1" aria-hidden="true" tabindex="-1"></a>run_metropolis_MCMC <span class="ot">&lt;-</span> <span class="cf">function</span>(startvalue, iterations) {</span>
<span id="cb121-2"><a href="chapter-stima-funzione-aposteriori.html#cb121-2" aria-hidden="true" tabindex="-1"></a>  chain <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length =</span> iterations <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb121-3"><a href="chapter-stima-funzione-aposteriori.html#cb121-3" aria-hidden="true" tabindex="-1"></a>  chain[<span class="dv">1</span>] <span class="ot">&lt;-</span> startvalue</span>
<span id="cb121-4"><a href="chapter-stima-funzione-aposteriori.html#cb121-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb121-5"><a href="chapter-stima-funzione-aposteriori.html#cb121-5" aria-hidden="true" tabindex="-1"></a>    proposal <span class="ot">&lt;-</span> <span class="fu">proposal_distribution</span>(chain[i])</span>
<span id="cb121-6"><a href="chapter-stima-funzione-aposteriori.html#cb121-6" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> <span class="fu">posterior</span>(proposal) <span class="sc">/</span> <span class="fu">posterior</span>(chain[i])</span>
<span id="cb121-7"><a href="chapter-stima-funzione-aposteriori.html#cb121-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> r) {</span>
<span id="cb121-8"><a href="chapter-stima-funzione-aposteriori.html#cb121-8" aria-hidden="true" tabindex="-1"></a>      chain[i <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> proposal</span>
<span id="cb121-9"><a href="chapter-stima-funzione-aposteriori.html#cb121-9" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb121-10"><a href="chapter-stima-funzione-aposteriori.html#cb121-10" aria-hidden="true" tabindex="-1"></a>      chain[i <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> chain[i]</span>
<span id="cb121-11"><a href="chapter-stima-funzione-aposteriori.html#cb121-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb121-12"><a href="chapter-stima-funzione-aposteriori.html#cb121-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb121-13"><a href="chapter-stima-funzione-aposteriori.html#cb121-13" aria-hidden="true" tabindex="-1"></a>  chain</span>
<span id="cb121-14"><a href="chapter-stima-funzione-aposteriori.html#cb121-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Generiamo dunqe una catena di valori <span class="math inline">\(\theta\)</span> con le seguenti istruzioni:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="chapter-stima-funzione-aposteriori.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb122-2"><a href="chapter-stima-funzione-aposteriori.html#cb122-2" aria-hidden="true" tabindex="-1"></a>startvalue <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb122-3"><a href="chapter-stima-funzione-aposteriori.html#cb122-3" aria-hidden="true" tabindex="-1"></a>niter <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb122-4"><a href="chapter-stima-funzione-aposteriori.html#cb122-4" aria-hidden="true" tabindex="-1"></a>chain <span class="ot">&lt;-</span> <span class="fu">run_metropolis_MCMC</span>(startvalue, niter)</span></code></pre></div>
<p>Otteniamo così 4,000 valori della distribuzione a posteriori per il parametro <span class="math inline">\(\theta\)</span>. Di questi valori, 2,000 vengono considerati burn-in e vengono esclusi. Ci restano dunque con 2,000 stime a posteriori di <span class="math inline">\(\theta\)</span>.</p>
<p>Il tasso di accettazione è pari a</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="chapter-stima-funzione-aposteriori.html#cb123-1" aria-hidden="true" tabindex="-1"></a>burnIn <span class="ot">&lt;-</span> niter <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb123-2"><a href="chapter-stima-funzione-aposteriori.html#cb123-2" aria-hidden="true" tabindex="-1"></a>acceptance <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">duplicated</span>(chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]))</span>
<span id="cb123-3"><a href="chapter-stima-funzione-aposteriori.html#cb123-3" aria-hidden="true" tabindex="-1"></a>acceptance</span>
<span id="cb123-4"><a href="chapter-stima-funzione-aposteriori.html#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.2511498</span></span></code></pre></div>
<p>il che conferma che la deviazione standard che abbiamo scelto per la distribuzione proposta (<span class="math inline">\(\sigma\)</span> = 0.9) è adeguata.</p>
<p>Una figura che rappresenta la distribuzione a posteriori per <span class="math inline">\(\theta\)</span>, insieme alla rappresentazione dei valori della catena di Markov realizzata dall’algoritmo di Metropolis, può essere prodotta mediante le seguenti istruzioni:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="chapter-stima-funzione-aposteriori.html#cb124-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]) <span class="sc">%&gt;%</span> </span>
<span id="cb124-2"><a href="chapter-stima-funzione-aposteriori.html#cb124-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x)) <span class="sc">+</span></span>
<span id="cb124-3"><a href="chapter-stima-funzione-aposteriori.html#cb124-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb124-4"><a href="chapter-stima-funzione-aposteriori.html#cb124-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb124-5"><a href="chapter-stima-funzione-aposteriori.html#cb124-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(theta),</span>
<span id="cb124-6"><a href="chapter-stima-funzione-aposteriori.html#cb124-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Frequenza&quot;</span>, </span>
<span id="cb124-7"><a href="chapter-stima-funzione-aposteriori.html#cb124-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Distribuzione a posteriori&quot;</span></span>
<span id="cb124-8"><a href="chapter-stima-funzione-aposteriori.html#cb124-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb124-9"><a href="chapter-stima-funzione-aposteriori.html#cb124-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]))</span>
<span id="cb124-10"><a href="chapter-stima-funzione-aposteriori.html#cb124-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-11"><a href="chapter-stima-funzione-aposteriori.html#cb124-11" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]), <span class="at">y=</span>chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]) <span class="sc">%&gt;%</span> </span>
<span id="cb124-12"><a href="chapter-stima-funzione-aposteriori.html#cb124-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb124-13"><a href="chapter-stima-funzione-aposteriori.html#cb124-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb124-14"><a href="chapter-stima-funzione-aposteriori.html#cb124-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb124-15"><a href="chapter-stima-funzione-aposteriori.html#cb124-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Numero di passi&quot;</span>,</span>
<span id="cb124-16"><a href="chapter-stima-funzione-aposteriori.html#cb124-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">expression</span>(theta), </span>
<span id="cb124-17"><a href="chapter-stima-funzione-aposteriori.html#cb124-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Valori della catena&quot;</span></span>
<span id="cb124-18"><a href="chapter-stima-funzione-aposteriori.html#cb124-18" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb124-19"><a href="chapter-stima-funzione-aposteriori.html#cb124-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)]))</span>
<span id="cb124-20"><a href="chapter-stima-funzione-aposteriori.html#cb124-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-21"><a href="chapter-stima-funzione-aposteriori.html#cb124-21" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2</span>
<span id="cb124-22"><a href="chapter-stima-funzione-aposteriori.html#cb124-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</span></span></code></pre></div>
<p><img src="Psicometria_files/figure-html/unnamed-chunk-138-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>A questo punto è molto facile trovare il massimo a posteriori per il parametro <span class="math inline">\(\theta\)</span>:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="chapter-stima-funzione-aposteriori.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(chain[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>burnIn)])</span>
<span id="cb125-2"><a href="chapter-stima-funzione-aposteriori.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5921799</span></span></code></pre></div>
<!-- Nella funzione precedente, le prime due righe definiscono un vettore, chiamato `chain`, dove vengono salvati i valori del parametro che sono campionati dalla distribuzione a posteriori, e inizializzano tale vettore utilizzando il valore iniziale che verrà prescelto in seguito. Il ciclo `for ()` viene utilizzato per specificare i valori della passeggiata aleatoria (*random walk*) che definisce l'insieme di punti che vengono campionati dalla distribuzione a posteriori. L'algoritmo procede nel modo seguente. -->
<!-- - Si inizia scegliendo un valore a caso del parametro. -->
<!-- - Si sceglie un nuovo valore del parametro, simile a quelo precedente, sulla base di una distribuzione di probabilità chiamata ``distribuzione proposta'' -- nell'algoritmo, la distribuzione proposta è una gaussiana centrata sul valore corrente della catena.  -->
<!-- - Si calcola il rapporto $\frac{p(new)}{p(old)}$, laddove la funzione $p(\cdot)$ è la funzione a posteriori, `new` è il valore proposto e `old` è il valore attuale della catena. Dato che le funzioni `likelihood` e `posterior` hanno trasformato i valori su scala logaritmica, dobbiamo fare una differenza. Inoltre, dato che vogliamo calcolare un rapporto tra probabilità, e siamo su una scala logaritmica, esponenziamo il risultato della differenza in modo tale da ritornare sulla scala delle probabilità. -->
<!-- - A questo punto dobbiamo decidere se mantenere il valore corrente del parametro (`old`) o se accettare il valore proposto (`new`). Le istruzioni condizionali `if-else` fanno in modo che venga accettato il valore `new` con probabilità $\frac{p(new)}{p(old)}$. Come nel caso del turista viaggiatore, se $p(new) > p(old)$, si accetta sempre il valore proposto del parametro. Questo significa che si accettano sempre valori del parametro che sono più vicini al massimo della funzione a posteriori rispetto al valore corrente (assumiamo qui che la distribuzione a posteriori sia unimodale). Questo comportamento dipende dalla linea di codice specificata da `else`. Altrimenti, si accetta il valore proposto con probabilità $\frac{p(new)}{p(old)}$. Infatti, ogni volta che viene eseguita, la condizione `(runif(1) < probab)` estrae un numero casuale compreso tra 0 e 1 e lo confronta con `probab`. Se il numero casuale è minore di `probab`, la condizione viene verificata e la proposta viene accettata. Se `probab` è un numero vicino allo zero, la probabilità di estrarre un numero casuale (compreso tra 0 e 1) minore di `probab` è ovviamente molto piccola; il contrario accade se `probab` è un numero vicino ad 1. In questo modo, il valore proposto viene accettato con una probabilità uguale al rapporto $\frac{p(new)}{p(old)}$. Se la distribuzione proposta ha una forma simile alla distribuzione a posteriori, questo algoritmo ha la conseguenza che la probabilità di accettare un valore proposto del parametro sarà proporzionale alla densità a posteriori -- in altri termini, i valori proposti accettati saranno un campione casuale della distribuzione a posteriori. Si sceglie la distribuzione gaussiana come distribuzione proposta in quanto la distribuzione a posteriori ha solitamente una forma simile alla gaussiana.  -->
<!-- ```{r} -->
<!-- startvalue <- runif(1, 0, 100) -->
<!-- niter <- 1e4 -->
<!-- chain <- run_metropolis_MCMC(startvalue, niter) -->
<!-- ``` -->
<!-- Il blocco di codice precedente specifica una catena di 10000 elementi mentre, nelle istruzioni successive si specifica che la prima metà dei valori proposti accettati saranno scartati, in quanto potrebbero essere influenzati dal valore iniziale -- nel caso presente abbiamo certamente esagerato! -->
<!-- ```{r} -->
<!-- burnIn <- niter / 2 -->
<!-- acceptance <- 1 - mean(duplicated(chain[-(1:burnIn)])) -->
<!-- acceptance -->
<!-- ``` -->
<!-- La funzione `duplicate()` nel blocco di codice precedente individua il numero di righe duplicate, ovvero il numero di casi nei quali la proposta non è stata accettata. Otteniamo qui un livello di accettazione del 29% che è buono. -->
<!-- #### Descrivere la distribuzione a posteriori -->
<!-- ```{r} -->
<!-- par(mfrow = c(1, 2)) -->
<!-- hist( -->
<!--   chain[-(1:burnIn)], -->
<!--   nclass = 30, -->
<!--   main = "Distribuzione\na posteriori", -->
<!--   xlab = expression(mu), -->
<!--   ylab = "Frequenza" -->
<!-- ) -->
<!-- abline(v = mean(chain[-(1:burnIn)])) -->
<!-- # abline(v = true_mean, col = "red") -->
<!-- plot( -->
<!--   chain[-(1:burnIn)], -->
<!--   type = "l", -->
<!--   bty = "n",  -->
<!--   xlab =  "Numero di passi", -->
<!--   ylab = expression(mu), -->
<!--   main = "Valori della catena" -->
<!-- ) -->
<!-- # abline(h = true_mean, col = "red") -->
<!-- mean(x) -->
<!-- mean(chain[-(1:burnIn)]) -->
<!-- par(mfrow = c(1, 1)) -->
<!-- ``` -->
<!-- Ma l'algoritmo funziona abbastanza bene anche se la distribuzione proposta è molto diversa dalla distribuzione target. -->
</div>
</div>
</div>
<div id="sec:pregi_inferenza_bayes" class="section level2 unnumbered">
<h2>Conclusioni</h2>
<p>Lo scopo di questa discussione è stato quello di mostrare come sia possibile combinare le nostre conoscenze a priori (espresse nei termini di una densità di probabilità) con le evidenze fornite dai dati (espresse nei termini della funzione di verosimiglianza), così da determinare, mediante il teorema di Bayes, una distribuzione a posteriori, la quale condensa l’incertezza che si ha sul parametro <span class="math inline">\(\theta\)</span>. Per illustrare tale problema, nel caso più semplice abbiamo considerato una situazione nella quale <span class="math inline">\(\theta\)</span> corrisponde alla probabilità di successo in una sequenza di prove Bernoulliane. Abbiamo visto come, in queste circostanze, è ragionevole esprimere le nostre credenze a priori mediante la densità Beta, con opportuni parametri. L’inferenza rispetto ad una proporzione rappresenta un caso particolare, ovvero un caso nel quale la distribuzione a priori è Beta e la verosimiglianza è Binomiale. In tali circostanze, anche la distribuzione a posteriori sarà una distribuzione Beta. Per questa ragione, in questo caso specifico, i parametri della distribuzione a posteriori possono essere determinati analiticamente (la soluzione richiede una serie di passaggi algebrici che qui non vengono discussi). In generale, però, tale approccio non è perseguibile.</p>
<p>La determinazione della distribuzione a posteriori richiede il calcolo della funzione di verosimiglianza e dell’integrale che si trova al denominatore del rapporto di Bayes. Nel caso di parametri continui, però, spesso tale integrale può essere impossibile da risolvere analiticamente. In passato, tale difficoltà è stata affrontata limitando l’analisi statistica al caso di funzioni di verosimiglianza semplici, le quali possono essere combinate con distribuzioni a priori coniugate per la verosimiglianza, così da produrre un integrale trattabile.</p>
<p>Invece di approcci matematici analitici, un’altra classe di metodi fa ricorso all’approssimazione numerica dell’integrale. Tale approssimazione numerica dipende dall’uso di metodi MCMC, ovvero dipende dall’uso di una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla grande potenza di calcolo dei moderni computer. Lo sviluppo di software che rendono sempre più semplice l’uso dei metodi MCMC, insieme all’incremento della potenza di calcolo dei computer, ha contribuito a rendere sempre più popolare il metodo dell’inferenza Bayesiana che, in questo modo, può essere estesa a
problemi di qualunque grado di complessità.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>In seguito, quest’idea è stata completamente screditata.<a href="chapter-stima-funzione-aposteriori.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>I metodi di stima MCMC costituiscono la modalità usuale per
generare la distribuzione a posteriori nell’analisi Bayesiana. In
queste dispense, però, ci limitiamo ai metodi di stima basati
sull’approssimazione quadratica. Abbiamo deciso di svolgere gli
esercizi mediante l’approssimazione quadratica piuttosto che con il
metodo MCMC perché l’installazione sul proprio computer del software
necessario per le analisi MCMC costituisce un problema di tipo
informatico che esula dagli scopi di questo insegnamento.<a href="chapter-stima-funzione-aposteriori.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Le analisi Bayesiane che discuteremo in queste dispense, per la
maggior parte, faranno uso della funzione <code>quap()</code>. È dunque
fondamentale che gli studenti installino il pacchetto <code>rethinking</code>
sul loro computer.<a href="chapter-stima-funzione-aposteriori.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter-distr-congiunta.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter-reglin.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ccaudek/bookdown_psicometria/edit/master/23_grid_approx.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/ccaudek/bookdown_psicometria/blob/master/23_grid_approx.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
