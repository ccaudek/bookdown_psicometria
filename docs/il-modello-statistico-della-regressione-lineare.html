<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capitolo 27 Il modello statistico della regressione lineare | Data Science per psicologi</title>
<meta name="author" content="Corrado Caudek">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.7/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science per psicologi</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Benvenuti</a></li>
<li class="book-part">Obiettivi formativi</li>
<li><a class="" href="conoscenza-dichiarativa-e-imperativa.html"><span class="header-section-number">1</span> Conoscenza dichiarativa e imperativa</a></li>
<li class="book-part">Introduzione al linguaggio R</li>
<li><a class="" href="introduzione.html">Introduzione</a></li>
<li><a class="" href="chapter-pacchetti.html"><span class="header-section-number">2</span> Pacchetti</a></li>
<li><a class="" href="chapter-install-r.html"><span class="header-section-number">3</span> Per cominciare</a></li>
<li><a class="" href="chapter-sintassi.html"><span class="header-section-number">4</span> Sintassi di base</a></li>
<li><a class="" href="chapter-strutture-dati.html"><span class="header-section-number">5</span> Strutture di dati</a></li>
<li><a class="" href="chapter-strut-contr.html"><span class="header-section-number">6</span> Strutture di controllo</a></li>
<li><a class="" href="chapter-input-output.html"><span class="header-section-number">7</span> Input/Output</a></li>
<li><a class="" href="manipolazione-dei-dati.html"><span class="header-section-number">8</span> Manipolazione dei dati</a></li>
<li><a class="" href="flusso-di-lavoro-riproducibile.html"><span class="header-section-number">9</span> Flusso di lavoro riproducibile</a></li>
<li class="book-part">Statistica descrittiva ed analisi esplorativa dei dat̀i</li>
<li><a class="" href="introduzione-1.html">Introduzione</a></li>
<li><a class="" href="terminologia.html"><span class="header-section-number">10</span> Terminologia</a></li>
<li><a class="" href="chapter-misurazione.html"><span class="header-section-number">11</span> La misurazione in psicologia</a></li>
<li><a class="" href="chapter-descript.html"><span class="header-section-number">12</span> Statistica descrittiva</a></li>
<li class="book-part">Nozioni di base</li>
<li><a class="" href="introduzione-2.html">Introduzione</a></li>
<li><a class="" href="il-calcolo-delle-probabilit%C3%A0.html"><span class="header-section-number">13</span> Il calcolo delle probabilità</a></li>
<li><a class="" href="chapter-prob-cond.html"><span class="header-section-number">14</span> Probabilità condizionata</a></li>
<li><a class="" href="chapter-teo-bayes.html"><span class="header-section-number">15</span> Il teorema di Bayes</a></li>
<li><a class="" href="chapter-prob-congiunta.html"><span class="header-section-number">16</span> Probabilità congiunta</a></li>
<li><a class="" href="la-distribuzione-binomiale.html"><span class="header-section-number">17</span> La distribuzione binomiale</a></li>
<li><a class="" href="densit%C3%A0-per-variabili-aleatorie-continue.html"><span class="header-section-number">18</span> Densità per variabili aleatorie continue</a></li>
<li><a class="" href="la-funzione-di-verosimiglianza.html"><span class="header-section-number">19</span> La funzione di verosimiglianza</a></li>
<li class="book-part">Inferenza frequentista</li>
<li><a class="" href="introduzione-3.html">Introduzione</a></li>
<li><a class="" href="distribuzione-campionaria.html"><span class="header-section-number">20</span> Distribuzione campionaria</a></li>
<li><a class="" href="significativit%C3%A0-statistica.html"><span class="header-section-number">21</span> Significatività statistica</a></li>
<li><a class="" href="inferenza-sulle-medie.html"><span class="header-section-number">22</span> Inferenza sulle medie</a></li>
<li><a class="" href="critiche-e-difese.html"><span class="header-section-number">23</span> Critiche e difese</a></li>
<li class="book-part">Inferenza Bayesiana</li>
<li><a class="" href="introduzione-4.html">Introduzione</a></li>
<li><a class="" href="modellistica-bayesiana.html"><span class="header-section-number">24</span> Modellistica bayesiana</a></li>
<li><a class="" href="stima-della-funzione-a-posteriori.html"><span class="header-section-number">25</span> Stima della funzione a posteriori</a></li>
<li><a class="" href="una-breve-introduzione-al-modello-di-regressione.html"><span class="header-section-number">26</span> Una breve introduzione al modello di regressione</a></li>
<li><a class="active" href="il-modello-statistico-della-regressione-lineare.html"><span class="header-section-number">27</span> Il modello statistico della regressione lineare</a></li>
<li><a class="" href="inferenza-bayesiana.html"><span class="header-section-number">28</span> Inferenza Bayesiana</a></li>
<li class="book-part">Informazioni generali</li>
<li><a class="" href="citazione.html">Citazione</a></li>
<li class="book-part">Appendici</li>
<li><a class="" href="un-piccolo-ripasso.html">Un piccolo ripasso</a></li>
<li><a class="" href="bibliografia.html">Bibliografia</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="il-modello-statistico-della-regressione-lineare" class="section level1" number="27">
<h1>
<span class="header-section-number">Capitolo 27</span> Il modello statistico della regressione lineare<a class="anchor" aria-label="anchor" href="#il-modello-statistico-della-regressione-lineare"><i class="fas fa-link"></i></a>
</h1>
<p>Lo scopo della ricerca è trovare le associazioni tra le variabili e fare
confronti fra le condizioni sperimentali. Nel caso della psicologia, il
ricercatore vuole scoprire le leggi generali che descrivono le relazioni
tra i costrutti psicologici e le relazioni che intercorrono tra i
fenomeni psicologici e quelli non psicologici (sociali, economici,
storici, …). Abbiamo già visto come la correlazione di Pearson sia uno
strumento adatto a questo scopo. Infatti, essa ci informa sulla
direzione e sull’intensità della relazione lineare tra due variabili.
Tuttavia, la correlazione non è sufficiente, in quanto il ricercatore ha
a disposizione solo i dati di un campione, mentre vorrebbe descrivere la
relazione tra le variabili nella popolazione. A causa della variabilità
campionaria, le proprietà dei campioni sono necessariamente diverse da
quelle della popolazione: ciò che si può osservare nella popolazione
potrebbe non emergere nel campione e, al contrario, il campione
manifesta caratteristiche che non sono necessariamente presenti nella
popolazione. È dunque necessario chiarire, dal punto di vista
statistico, il legame che intercorre tra le proprietà del campione e le
proprietà della popolazione da cui esso è stato estratto. Come nel caso
della media, anche in questo caso dovrà essere costruita la
distribuzione di una statistica; ma però, nel caso presente, la
statistica di interesse sarà costruita utilizzando, non i dati di una
sola variabile, ma bensì i dati che descrivono l’andamento congiunto di
due variabili.</p>
<p>Il modello di regressione utilizza la funzione matematica più semplice
per descrivere la relazione fra due variabili, ovvero la funzione
lineare. Inizieremo a descrivere le proprietà geometriche della funzione lineare per poi utilizzare questa semplice funzione per costruire il modello statistico della regressione lineare.</p>
<div id="la-funzione-lineare" class="section level2" number="27.1">
<h2>
<span class="header-section-number">27.1</span> La funzione lineare<a class="anchor" aria-label="anchor" href="#la-funzione-lineare"><i class="fas fa-link"></i></a>
</h2>
<p>Si chiama <em>funzione lineare</em> una funzione del tipo</p>
<p><span class="math display">\[
f(x) = a + b x,
\]</span>
dove <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> sono delle costanti.
Il grafico di tale funzione è una retta di cui il parametro <span class="math inline">\(b\)</span> è detto <em>coefficiente angolare</em> e il parametro <span class="math inline">\(a\)</span> è detto <em>intercetta</em> con l’asse delle <span class="math inline">\(y\)</span> (infatti, la retta interseca l’asse <span class="math inline">\(y\)</span> nel punto <span class="math inline">\((0,a)\)</span>, se <span class="math inline">\(b \neq 0\)</span>).</p>
<p>Per assegnare un’interpretazione geometrica alle costanti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> si consideri la funzione</p>
<p><span class="math display">\[
y = b x.
\]</span>
Tale funzione rappresenta un caso particolare, ovvero quello della <em>proporzionalità diretta</em> tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Il caso generale della linearità</p>
<p><span class="math display">\[
y = a + b x
\]</span>
non fa altro che sommare una costante <span class="math inline">\(a\)</span> a ciascuno dei valori <span class="math inline">\(y = b x\)</span>. Nella funzione lineare <span class="math inline">\(y = a + b x\)</span>, se <span class="math inline">\(b\)</span> è positivo allora <span class="math inline">\(y\)</span> aumenta al crescere di <span class="math inline">\(x\)</span>; se <span class="math inline">\(b\)</span> è negativo allora <span class="math inline">\(y\)</span> diminuisce al crescere di <span class="math inline">\(x\)</span>; se <span class="math inline">\(b=0\)</span> la retta è orizzontale, ovvero <span class="math inline">\(y\)</span> non muta al variare di <span class="math inline">\(x\)</span>.</p>
<p>Consideriamo ora il coefficiente <span class="math inline">\(b\)</span>. Si consideri un punto <span class="math inline">\(x_0\)</span> e un incremento arbitrario <span class="math inline">\(\varepsilon\)</span> come indicato nella figura <a href="il-modello-statistico-della-regressione-lineare.html#fig:linearfunction">27.1</a>. Le differenze <span class="math inline">\(\Delta x = (x_0 + \varepsilon) - x_0\)</span> e <span class="math inline">\(\Delta y = f(x_0 + \varepsilon) - f(x_0)\)</span> sono detti <em>incrementi</em> di <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Il coefficiente angolare <span class="math inline">\(b\)</span> è uguale al rapporto</p>
<p><span class="math display">\[
    b = \frac{\Delta y}{\Delta x} = \frac{f(x_0 + \varepsilon) - f(x_0)}{(x_0 + \varepsilon) - x_0},
\]</span>
indipendentemente dalla grandezza degli incrementi <span class="math inline">\(\Delta x\)</span> e <span class="math inline">\(\Delta y\)</span>. Il modo più semplice per assegnare un’interpretazione geometrica al coefficiente angolare (o pendenza) della retta è dunque quello di porre <span class="math inline">\(\Delta x = 1\)</span>. In tali circostanze infatti <span class="math inline">\(b = \Delta y\)</span>.</p>
<div class="figure" style="text-align: center">
<span id="fig:linearfunction"></span>
<img src="images/linear_function.png" alt="La funzione lineare $y = a + bx$." width="70%"><p class="caption">
Figura 27.1: La funzione lineare <span class="math inline">\(y = a + bx\)</span>.
</p>
</div>
</div>
<div id="lerrore-di-misurazione" class="section level2" number="27.2">
<h2>
<span class="header-section-number">27.2</span> L’errore di misurazione<a class="anchor" aria-label="anchor" href="#lerrore-di-misurazione"><i class="fas fa-link"></i></a>
</h2>
<p>Per descrivere l’associazione tra due variabili, tuttavia, la funzione lineare non è sufficiente. Nel mondo empirico, infatti, la relazione tra variabili non è mai perfettamente lineare. È dunque necessario includere nel modello di regressione anche una componente d’errore, ovvero una componente della <span class="math inline">\(y\)</span> che non può essere spiegata dal modello lineare. Nel caso di due sole variabili, questo ci conduce alla seguente formulazione del modello di regressione:</p>
<p><span class="math display" id="eq:regbivpop">\[\begin{equation}
y = \alpha + \beta x + \varepsilon,
\tag{27.1}
\end{equation}\]</span></p>
<p>laddove i parametri <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> descrivono l’associazione tra le variabili aleatorie <span class="math inline">\(y\)</span> e <span class="math inline">\(x\)</span> (nella popolazione), e il termine d’errore <span class="math inline">\(\varepsilon\)</span> specifica quant’è grande la porzione della variabile <span class="math inline">\(y\)</span> che non può essere predetta nei termini di una relazione lineare con la <span class="math inline">\(x\)</span>.</p>
<p>Si noti che l’eq. <a href="il-modello-statistico-della-regressione-lineare.html#eq:regbivpop">(27.1)</a> ci consente di formulare una predizione, nei termini di un modello lineare, del valore atteso della <span class="math inline">\(y\)</span> conoscendo <span class="math inline">\(x\)</span>, ovvero</p>
<p><span class="math display" id="eq:regbivpop2">\[\begin{equation}
\hat{y} = \mathbb{E}(y \mid x) = \alpha + \beta x.
\tag{27.2}
\end{equation}\]</span></p>
<p>In altri termini, se i parametri del modello (<span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>) sono noti, allora è possibile predire <span class="math inline">\(y\)</span> sulla base della nostra conoscenza della <span class="math inline">\(x\)</span>.</p>
<p>Per esempio, se conosciamo la relazione lineare tra quoziente di intelligenza ed aspettativa di vita, allora possiamo prevedere quanto a lungo vivrà una persona sulla base del suo QI. Sì, c’è una relazione lineare tra intelligenza e aspettativa di vita! – per una discussione, si veda, ad esempio, l’articolo di David Z. Hambrick pubblicato su Scientific American il 22 dicembre 2015. Ma quando sarà accurata la nostra previsione? Ciò dipende dal termine d’errore dell’eq. <a href="il-modello-statistico-della-regressione-lineare.html#eq:regbivpop">(27.1)</a>. L’analisi di regressione ci fornisce un metodo per rispondere a domande di questo tipo.</p>
</div>
<div id="scopi-della-regressione-lineare" class="section level2" number="27.3">
<h2>
<span class="header-section-number">27.3</span> Scopi della regressione lineare<a class="anchor" aria-label="anchor" href="#scopi-della-regressione-lineare"><i class="fas fa-link"></i></a>
</h2>
<p>Il modello di regressione lineare si pone tre obiettivi:</p>
<ol style="list-style-type: decimal">
<li>descrivere l’associazione tra le variabili <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nel campione esaminato;</li>
<li>misurare la bontà dell’adattamento dell’associazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>;</li>
<li>fare inferenze sull’associazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nella popolazione da cui il campione è stato estratto.</li>
</ol>
<p>Il primo obiettivo intende rispondere alla stessa domanda a cui risponde il coefficiente di correlazione: quali sono l’intensità e il segno della relazione lineare che descrive l’associazione tra due variabili? Vedremo che c’è una precisa relazione tra il coefficiente <span class="math inline">\(b\)</span> del modello di regressione (che rappresenta la pendenza della retta di regressione) e
il coefficiente di correlazione <span class="math inline">\(r\)</span> di Pearson: il coefficiente di
correlazione non è altro che la pendenza della retta di regressione
quando i dati sono standardizzati. Vi è però un’importante differenza
tra la correlazione ed il modello di regressione. La correlazione è un
indicatore simmetrico di associazione tra due caratteri. Il modello di
regressione, invece, si chiede come varia una variabile, detta
<em>dipendente</em> e solitamente denotata con <span class="math inline">\(y\)</span>, al variare di un’altra
variabile, detta <em>indipendente</em> (o predittore), solitamente denotata con
<span class="math inline">\(x\)</span>. L’analisi della regressione lineare si pone dunque il problema di
studiare la relazione asimmetrica tra due variabili.</p>
<p>Il secondo obiettivo del modello di regressione lineare si chiede se il
modello di regressione sia sensato per descrivere l’associazione
osservata tra le due variabili. Vogliamo trovare un indice che descriva
quanto distanti sono i dati dalla retta di regressione. Se i punti di un
diagramma a dispersione sono molto vicini alla retta di regressione,
allora il modello di regressione è adeguato per descrivere
l’associazione tra le due variabili. In questo caso la bontà di
adattamento del modello ai dati è grande. Oppure può succedere che i
punti di un diagramma a dispersione siano molto lontani alla retta di
regressione e/o che la retta di regressione sia piatta. In questi due
ultimi casi non vi è evidenza di una associazione lineare tra le due
variabili e l’indice che misura la bontà dell’adattamento
dell’associazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> assume un valore basso e prossimo allo
zero. Tale indice va sotto il nome di coefficiente di determinazione.</p>
<p>Il terzo obiettivo è quello più ambizioso: ci chiediamo quale potrebbe
essere l’associazione tra le variabili <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nella popolazione, alla
luce delle informazioni che sono state osservate nel campione. Quello che vorremmo conoscere è <span class="math inline">\(p(\theta \mid \text{dati})\)</span>, laddove <span class="math inline">\(\theta\)</span> è il parametro sconosciuto che rappresenta la pendenza della retta di regressione nella popolazione. Vedremo come l’approccio Bayesiano può essere usato per rispondere a questa domanda.</p>
<p>Il modello di regressione è, probabilmente, il più importante dei modelli statistici. Noi qui ne
esamineremo solo le sue caratteristiche di base. Ma tale modello può
essere esteso in modo tale da includere più di un predittore (nel qual
caso si parla di modello di regressione multipla), oppure una variabile
dipendente qualitativa (il che produce il modello di regressione
logistica), oppure molteplici variabili dipendenti continue (il che
produce il modello di regressione multivariato). Sviluppi più moderni di
questo modello considerano inoltre il caso della violazione
dell’assunzione di indipendenza tra le osservazioni, il che conduce alla
costruzione dei modelli ad effetti misti (<em>mixed-effects models</em>).
Infine, uno sviluppo importante del modello di regressione lineare è
l’analisi fattoriale, nel qual caso viene ipotizzata l’esistenza di
variabili indipendenti <em>inosservabili</em> (latenti), le quali corrispondono
ai costrutti psicologici. Il modello fattoriale, così formulato,
costituisce il fondamento della Psicometria, ovvero di quelle tecniche
statistiche che stanno alla base della costruzione e della validazione
dei reattivi psicologici.</p>
</div>
<div id="quantificare-lassociazione-fra-due-caratteri-quantitativi" class="section level2" number="27.4">
<h2>
<span class="header-section-number">27.4</span> Quantificare l’associazione fra due caratteri quantitativi<a class="anchor" aria-label="anchor" href="#quantificare-lassociazione-fra-due-caratteri-quantitativi"><i class="fas fa-link"></i></a>
</h2>
<p>Consideriamo tre variabili aleatorie <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> ed <span class="math inline">\(\varepsilon\)</span> legate dalla relazione lineare</p>
<p><span class="math display">\[
y = \alpha + \beta x + \varepsilon,
\]</span></p>
<p>dove <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> sono numeri reali e <span class="math inline">\(\mathbb{E}(\varepsilon) = 0\)</span>. Chiameremo <em>modello lineare</em> (semplice) la relazione dell’eq. <a href="il-modello-statistico-della-regressione-lineare.html#eq:regbivpop">(27.1)</a> e chiameremo <em>retta di regressione</em> la retta</p>
<p><span class="math display" id="eq:regrline">\[\begin{equation}
y = \alpha + \beta x.
\tag{27.3}
\end{equation}\]</span></p>
<p>Il parametro <span class="math inline">\(\alpha\)</span> è l’ordinata all’origine (o intercetta) mentre il parametro <span class="math inline">\(\beta\)</span> è il coefficiente angolare della retta. Possiamo interpretare l’eq. <a href="il-modello-statistico-della-regressione-lineare.html#eq:regbivpop">(27.1)</a> pensando che le variabili aleatorie <span class="math inline">\(x\)</span> ed <span class="math inline">\(y\)</span> siano legate tra loro da una relazione lineare perturbata da un errore casuale <span class="math inline">\(\varepsilon\)</span>.</p>
<p>Dato un insieme di realizzazioni campionarie delle variabili aleatorie <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, ci poniamo lo scopo di determinare la retta di regressione campionaria</p>
<p><span class="math display" id="eq:fittedval">\[\begin{equation}
\hat{y}_i  =  a + b x_i
\tag{27.4}
\end{equation}\]</span></p>
<p>che approssima il meglio possibile la distribuzione dei punti <span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>, <span class="math inline">\(i = 1, \dots, n\)</span>. Lo studio di questo problema è detto <em>regressione lineare</em>.</p>
</div>
<div id="stime-dei-minimi-quadrati" class="section level2" number="27.5">
<h2>
<span class="header-section-number">27.5</span> Stime dei minimi quadrati<a class="anchor" aria-label="anchor" href="#stime-dei-minimi-quadrati"><i class="fas fa-link"></i></a>
</h2>
<p>Il primo obiettivo dell’analisi di regressione è quello di trovare la retta che meglio descrive l’andamento dei dati osservati in un campione. Iniziamo con il definire i <em>residui</em> <span class="math inline">\(e_i\)</span> tramite la relazione</p>
<p><span class="math display" id="eq:residuals">\[\begin{equation}
e_i  = y_i - (a + b x_i).
\tag{27.5}
\end{equation}\]</span></p>
<p>In altri termini, il residuo <span class="math inline">\(i\)</span>-esimo è la differenza fra l’ordinata del punto (<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>) e quella del punto di ascissa <span class="math inline">\(x_i\)</span> sulla retta di regressione campionaria.</p>
<p>Per determinare i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> della retta <a href="il-modello-statistico-della-regressione-lineare.html#eq:fittedval">(27.4)</a> non è sufficiente minimizzare la somma dei residui <span class="math inline">\(\sum_{i=1}^{n}e_i\)</span>, in quanto i residui possono essere sia positivi che negativi e la loro somma può essere molto prossima allo zero anche per differenze molto grandi tra i valori osservati e la retta
di regressione. Infatti, ciascuna retta passante per il punto (<span class="math inline">\(\bar{x}, \bar{y}\)</span>) ha <span class="math inline">\(\sum_{i=1}^{n}e_i=0\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-24" class="proof"><em>Dimostrazione</em>. </span>Una retta passante per il punto (<span class="math inline">\(\bar{x}, \bar{y}\)</span>) soddisfa l’equazione <span class="math inline">\(\bar{y} = a + b \bar{x}\)</span>. Sottraendo tale equazione dall’equazione <span class="math inline">\(y_i = a + b x_i + e_i\)</span> otteniamo
<span class="math display">\[
y_i - \bar{y} =  b (x_i - \bar{x}) + e_i.
\]</span>
Sommando su tutte le osservazioni, si ha che</p>
<p><span class="math display">\[
\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \bar{y} ) -  b \sum_{i=1}^n (x_i - \bar{x}) = 0 - b(0) = 0. 
\]</span></p>
</div>
<p>Questo problema viene risolto scegliendo i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> che
minimizzano, non tanto la somma dei residui, ma bensì l’<em>errore
quadratico</em>, cioè la somma dei quadrati degli errori:</p>
<p><span class="math display">\[
S(a, b) = \sum_{i=1}^{n} e_i^2 = \sum (y_i - a - b x_i)^2.
\]</span></p>
<p>Il metodo più diretto per determinare quelli che vengono chiamati i <em>coefficienti dei minimi quadrati</em> è quello di trovare le derivate parziali della funzione <span class="math inline">\(S(a, b)\)</span> rispetto ai coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>:</p>
<p><span class="math display" id="eq:normderiv">\[\begin{equation}
\begin{aligned}
\frac{\partial S(a,b)}{\partial a} &amp;= \sum (-1)(2)(y_i - a - b x_i), \notag \\
\frac{\partial S(a,b)}{\partial b} &amp;= \sum (-x_i)(2)(y_i - a - b x_i).
\end{aligned}
\tag{27.6}
\end{equation}\]</span></p>
<p>Ponendo le derivate uguali a zero e dividendo entrambi i membri per <span class="math inline">\(-2\)</span> si ottengono le <em>equazioni normali</em></p>
<p><span class="math display" id="eq:eqnormali">\[\begin{equation}
\begin{aligned}
 an + b \sum x_i &amp;= \sum y_i, \notag \\
 a \sum x_i + b \sum x_i^2 &amp;= \sum x_i y_i. 
\end{aligned}
\tag{27.7}
\end{equation}\]</span></p>
<p>I coefficienti dei minimi quadrati <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> si trovano risolvendo le equazioni <a href="il-modello-statistico-della-regressione-lineare.html#eq:eqnormali">(27.7)</a> e sono uguali a:</p>
<p><span class="math display" id="eq:minsqab">\[\begin{equation}
\begin{aligned}
a &amp;= \bar{y} - b \bar{x},\\
b &amp;= \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sum (x_i - \bar{x})^2}.
\end{aligned}
\tag{27.8}
\end{equation}\]</span></p>
<div id="monotwinsiq" class="section level3" number="27.5.1">
<h3>
<span class="header-section-number">27.5.1</span> Un esempio concreto<a class="anchor" aria-label="anchor" href="#monotwinsiq"><i class="fas fa-link"></i></a>
</h3>
<p>Consideriamo i dati relativi a 34 coppie di gemelli monozigoti separati alla nascita (<span class="citation"><a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">Anderson &amp; Finn</a> (<a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">2012</a>)</span>). Dei gemelli conosciamo l’ordine della nascita e il quoziente di intelligenza misurato con il <em>Dominoes Intelligence test</em>. Il test è costituito da 48 domande a ciascuna delle quali viene assegnato un punto nel caso di risposta corretta. La media del test nella popolazione è di 28 punti, che corrisponde al punteggiodi 100 sulla scala WAIS. I dati sono:</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">iq1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">22</span>, <span class="fl">32</span>, <span class="fl">29</span>, <span class="fl">13</span>, <span class="fl">32</span>, <span class="fl">24</span>, <span class="fl">33</span>, <span class="fl">19</span>, <span class="fl">13</span>, <span class="fl">36</span>, <span class="fl">26</span>, <span class="fl">26</span>, <span class="fl">32</span>, <span class="fl">27</span>, <span class="fl">6</span>, <span class="fl">16</span>, <span class="fl">41</span>, <span class="fl">29</span>, <span class="fl">13</span>, <span class="fl">20</span>, <span class="fl">28</span>, <span class="fl">30</span>, <span class="fl">22</span>, <span class="fl">23</span>, <span class="fl">27</span>, <span class="fl">40</span>, <span class="fl">30</span>, <span class="fl">30</span>, <span class="fl">21</span>, <span class="fl">27</span>, <span class="fl">15</span>, <span class="fl">38</span>, <span class="fl">4</span>, <span class="fl">12</span><span class="op">)</span>

<span class="va">iq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">12</span>, <span class="fl">28</span>, <span class="fl">35</span>, <span class="fl">4</span>, <span class="fl">18</span>, <span class="fl">33</span>, <span class="fl">26</span>, <span class="fl">9</span>, <span class="fl">22</span>, <span class="fl">34</span>, <span class="fl">17</span>, <span class="fl">20</span>, <span class="fl">33</span>, <span class="fl">28</span>, <span class="fl">10</span>, <span class="fl">28</span>, <span class="fl">40</span>, <span class="fl">30</span>, <span class="fl">10</span>, <span class="fl">24</span>, <span class="fl">22</span>, <span class="fl">34</span>, <span class="fl">23</span>, <span class="fl">21</span>, <span class="fl">25</span>, <span class="fl">38</span>, <span class="fl">25</span>, <span class="fl">26</span>, <span class="fl">27</span>, <span class="fl">24</span>, <span class="fl">9</span>, <span class="fl">27</span>, <span class="fl">2</span>, <span class="fl">9</span><span class="op">)</span>

<span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">iq1</span>, <span class="va">iq2</span><span class="op">)</span></code></pre></div>
<p>Un diagramma di dispersione per questi dati, insieme alla retta di regressione dei minimi quadrati, è riportato nella figura <a href="il-modello-statistico-della-regressione-lineare.html#fig:twinmono">27.2</a>.</p>
<div class="sourceCode" id="cb276"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">p</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">iq1</span>, y <span class="op">=</span> <span class="va">iq2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="cn">FALSE</span>, color<span class="op">=</span><span class="st">"lightgrey"</span>, formula <span class="op">=</span> <span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="st">"Qi primo nato"</span>,
    y <span class="op">=</span> <span class="st">"QI secondo nato"</span>,
    title <span class="op">=</span> <span class="st">"Gemelli monozigoti separati alla nascita"</span>,
    caption <span class="op">=</span> <span class="st">"(Fonte: Anderson e Finn, 2012)"</span>
  <span class="op">)</span>
<span class="va">p</span></code></pre></div>
<div class="figure" style="text-align: center">
<span id="fig:twinmono"></span>
<img src="Data-Science-per-psicologi_files/figure-html/twinmono-1.png" alt="Retta di regressione che descrive la relazione lineare tra il quoziente di intelligenza del secondo nato e il quoziente di intelligenza del primo nato." width="90%"><p class="caption">
Figura 27.2: Retta di regressione che descrive la relazione lineare tra il quoziente di intelligenza del secondo nato e il quoziente di intelligenza del primo nato.
</p>
</div>
<p>I coefficienti di regressione si trovano con le formule dei minimi quadrati. Usando R, per <span class="math inline">\(b\)</span> otteniamo</p>
<div class="sourceCode" id="cb277"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq1</span>, <span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq1</span><span class="op">)</span> 
<span class="va">b</span>
<span class="co">#&gt; [1] 0.8498545</span></code></pre></div>
<p>e per <span class="math inline">\(a\)</span> otteniamo</p>
<div class="sourceCode" id="cb278"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span> <span class="op">-</span> <span class="va">b</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq1</span><span class="op">)</span>
<span class="va">a</span>
<span class="co">#&gt; [1] 1.838871</span></code></pre></div>
<p>Tali risultati corrispondono ai valori trovati dalla funzione <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> con la seguente sintassi:</p>
<div class="sourceCode" id="cb279"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">iq2</span> <span class="op">~</span> <span class="va">iq1</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></code></pre></div>
<p>L’oggetto creato da <code>{lm()</code> può essere visionato utilizzando <code><a href="https://rdrr.io/r/stats/coef.html">coef(fm)</a></code> o con <code><a href="https://rdrr.io/r/base/summary.html">summary(fm)</a></code>.</p>
<div class="sourceCode" id="cb280"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = iq2 ~ iq1, data = df)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      Min       1Q   Median       3Q      Max </span>
<span class="co">#&gt; -11.0342  -3.8218  -0.5852   3.4658  12.5635 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   1.8389     3.0269   0.608    0.548    </span>
<span class="co">#&gt; iq1           0.8499     0.1155   7.357 2.29e-08 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 6.102 on 32 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.6285, Adjusted R-squared:  0.6169 </span>
<span class="co">#&gt; F-statistic: 54.13 on 1 and 32 DF,  p-value: 2.288e-08</span></code></pre></div>
<p>I valori predetti dal modello di regressione sono dati da</p>
<div class="sourceCode" id="cb281"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">yhat</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">df</span><span class="op">$</span><span class="va">iq1</span></code></pre></div>
<p>o, in maniera equivalente, possono essere trovati con <code><a href="https://rdrr.io/r/stats/predict.html">predict(fm)</a></code></p>
<div class="sourceCode" id="cb282"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span> 
<span class="co">#&gt;         1         2         3         4         5         6         7         8         9 </span>
<span class="co">#&gt; 20.535671 29.034216 26.484652 12.886980 29.034216 22.235380 29.884070 17.986107 12.886980 </span>
<span class="co">#&gt;        10        11        12        13        14        15        16        17        18 </span>
<span class="co">#&gt; 32.433634 23.935089 23.935089 29.034216 24.784943  6.937998 15.436543 36.682907 26.484652 </span>
<span class="co">#&gt;        19        20        21        22        23        24        25        26        27 </span>
<span class="co">#&gt; 12.886980 18.835962 25.634798 27.334507 20.535671 21.385525 24.784943 35.833052 27.334507 </span>
<span class="co">#&gt;        28        29        30        31        32        33        34 </span>
<span class="co">#&gt; 27.334507 19.685816 24.784943 14.586689 34.133343  5.238289 12.037125</span></code></pre></div>
<p>I residui di regressione, ovvero la differenza tra il valore osservato e il valore predetto dal modello, si trovano mediante l’istruzione</p>
<div class="sourceCode" id="cb283"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">e</span> <span class="op">&lt;-</span> <span class="va">df</span><span class="op">$</span><span class="va">iq2</span> <span class="op">-</span> <span class="va">yhat</span></code></pre></div>
<p>o, in maniera equivalente, con <code><a href="https://rdrr.io/r/stats/residuals.html">residuals(fm)</a></code></p>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span>
<span class="co">#&gt;           1           2           3           4           5           6           7 </span>
<span class="co">#&gt;  -8.5356706  -1.0342160   8.5153476  -8.8869798 -11.0342160  10.7646203  -3.8840705 </span>
<span class="co">#&gt;           8           9          10          11          12          13          14 </span>
<span class="co">#&gt;  -8.9861070   9.1130202   1.5663659  -6.9350888  -3.9350888   3.9657840   3.2150567 </span>
<span class="co">#&gt;          15          16          17          18          19          20          21 </span>
<span class="co">#&gt;   3.0620019  12.5634566   3.3170932   3.5153476  -2.8869798   5.1640385  -3.6347978 </span>
<span class="co">#&gt;          22          23          24          25          26          27          28 </span>
<span class="co">#&gt;   6.6654931   2.4643294  -0.3855252   0.2150567   2.1669478  -2.3345069  -1.3345069 </span>
<span class="co">#&gt;          29          30          31          32          33          34 </span>
<span class="co">#&gt;   7.3141839  -0.7849433  -5.5866889  -7.1333432  -3.2382890  -3.0371253</span></code></pre></div>
<p>I residui possono essere rappresentanti graficamente come riportato nella figura <a href="il-modello-statistico-della-regressione-lineare.html#fig:reglinresiduals">27.3</a>.</p>
<div class="sourceCode" id="cb285"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">df</span><span class="op">$</span><span class="va">predicted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span>   
<span class="va">df</span><span class="op">$</span><span class="va">residuals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span> 

<span class="va">p1</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">iq1</span>, y <span class="op">=</span> <span class="va">iq2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span>, color <span class="op">=</span> <span class="st">"lightgrey"</span><span class="op">)</span> <span class="op">+</span>  
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_segment.html">geom_segment</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xend <span class="op">=</span> <span class="va">iq1</span>, yend <span class="op">=</span> <span class="va">predicted</span><span class="op">)</span>, alpha <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">predicted</span><span class="op">)</span>, shape <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="st">"Qi primo nato"</span>,
    y <span class="op">=</span> <span class="st">"QI secondo nato"</span>,
    title <span class="op">=</span> <span class="st">"Gemelli monozigoti separati alla nascita"</span>,
    caption <span class="op">=</span> <span class="st">"(Fonte: Anderson e Finn, 2012)"</span>
  <span class="op">)</span>
<span class="va">p1</span>
<span class="co">#&gt; `geom_smooth()` using formula 'y ~ x'</span></code></pre></div>
<div class="figure" style="text-align: center">
<span id="fig:reglinresiduals"></span>
<img src="Data-Science-per-psicologi_files/figure-html/reglinresiduals-1.png" alt="Residui del modello di regressione che esprime il quoziente di intelligenza del secondo nato in funzione del quoziente di intelligenza del primo nato." width="90%"><p class="caption">
Figura 27.3: Residui del modello di regressione che esprime il quoziente di intelligenza del secondo nato in funzione del quoziente di intelligenza del primo nato.
</p>
</div>
</div>
<div id="sec:beta_r" class="section level3" number="27.5.2">
<h3>
<span class="header-section-number">27.5.2</span> Coefficiente angolare e correlazione di Pearson<a class="anchor" aria-label="anchor" href="#sec:beta_r"><i class="fas fa-link"></i></a>
</h3>
<p>Ricordando che <span class="math inline">\(r_{xy}=s_{xy} / (s_x s_y)\)</span> è il coefficiente di
correlazione lineare e che <span class="math inline">\(b=s_{xy} /s_x^2\)</span> è la stima dei minimi
quadrati del coefficiente angolare della retta di regressione,
sostituendo <span class="math inline">\(r_{xy}s_xs_y\)</span> al numeratore dell’equazione di <span class="math inline">\(b\)</span> e
semplificando, si ottiene
<span class="math display">\[\begin{equation}
b = r_{yx}\frac{s_y}{s_x}.
\end{equation}\]</span>
Se i dati vengono standardizzati, dunque, l’equazione della retta di regressione
campionaria diventa
<span class="math display">\[\begin{equation}
z_{y_i} = r_{xy} z_{x_i} + e_i,
\end{equation}\]</span>
in quanto <span class="math inline">\(a = \bar{z}_y - b\bar{z}_x =0\)</span> e <span class="math inline">\(s_x = s_y = 1\)</span>.</p>
<p>Si può dunque assegnare al coefficiente di correlazione di Pearson la seguente
interpretazione: <span class="math inline">\(r_{xy}\)</span> è uguale alla pendenza <span class="math inline">\(b\)</span> della retta di
regressione quando le variabili <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> vengono standardizzate
(Rodgers &amp; Nicewander, 1988).</p>
<p>Facciamo un esempio calcolando i coefficienti di regressione sui punteggi standardizzati del
quoziente di intelligenza dei gemelli monozigoti.</p>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ziq1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq1</span><span class="op">)</span>
<span class="va">ziq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span>
<span class="va">fm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ziq2</span> <span class="op">~</span> <span class="va">ziq1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fm1</span><span class="op">)</span>
<span class="co">#&gt;  (Intercept)         ziq1 </span>
<span class="co">#&gt; 1.818206e-16 7.927594e-01</span></code></pre></div>
<p>Utilizzando i valori standardizzati del QI l’intercetta diventa pari a zero e la pendenza della retta di regressione diventa uguale alla correlazione tra le due variabili:</p>
<div class="sourceCode" id="cb287"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq1</span>, <span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.7927594</span></code></pre></div>
</div>
<div id="regressione-verso-la-media" class="section level3" number="27.5.3">
<h3>
<span class="header-section-number">27.5.3</span> Regressione verso la media<a class="anchor" aria-label="anchor" href="#regressione-verso-la-media"><i class="fas fa-link"></i></a>
</h3>
<p>Il termine regressione fu introdotto da Francis Galton (1822-1911), un
antropologo che fu, tra le altre cose, promotore dell’eugenetica. Nel
1886, nell’ambito dei suoi studi sull’ereditarietà dei caratteri, Galton
raccolse le stature di <span class="math inline">\(928\)</span> figli adulti e dei loro <span class="math inline">\(205\)</span> genitori
(padri e madri) – i dati sono disponibili nel data.frame <code>Galton</code>
contenuto nel pacchetto R  <code>HistData</code>. Galton esaminò la relazione tra
l’altezza media dei figli e l’altezza media dei genitori, che chiamò
“mid-parent height.” In questi dati, genitori e figli hanno la stessa
altezza media di <span class="math inline">\(68.2\)</span> pollici. Galton osservò però come l’altezza
media dei figli nati da genitori di una data altezza era più simile al
valore dell’altezza media della popolazione intera di quanto lo fosse la
mid-height dei genitori. Ad esempio, per genitori con una mid-height
compresa tra <span class="math inline">\(70\)</span> e <span class="math inline">\(71\)</span> pollici, l’altezza media dei figli risultò
essere di <span class="math inline">\(69.5\)</span> pollici. Nelle parole di Galton, questo corrispondeva
ad una <em>regression toward mediocrity</em>, un concetto che noi oggi
chiamiamo “regressione verso la media.” Nonostante l’interpretazione
(errata) di Galton, è importante capire come questo sia un fenomeno
statistico, non genetico. Esaminiamo la ragione per cui ciò si verifica.</p>
<p>In precedenza abbiamo visto come, nel caso di dati standardizzati, la retta di
regressione campionaria diventa: <span class="math display">\[\hat{z}_{y_i} = r_{xy} z_{x_i}.\]</span> Dal
momento che <span class="math inline">\(r_{xy}\)</span> è il coefficiente di regressione, esso assume
valori compresi tra <span class="math inline">\(-1\)</span> e <span class="math inline">\(1\)</span>. Assumiamo che <span class="math inline">\(r_{xy}\)</span> sia positivo e
minore di <span class="math inline">\(1\)</span> (ovvero, assumiamo che la correlazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> sia
positiva ma non perfetta). La formula <span class="math inline">\(\hat{z}_{y_i} = r_{xy} z_{x_i}\)</span>
implica che, se <span class="math inline">\(z_{x_i}\)</span> è positivo, allora il valore predetto
<span class="math inline">\(\hat{z}_{y_i}\)</span> dovrà essere minore di <span class="math inline">\(z_{x_i}\)</span>. In maniera equivalente,
si può dire che la ‘distanza’ tra il valore predetto <span class="math inline">\(\hat{y}\)</span> della
variabile di risposta e la media <span class="math inline">\(\bar{y}\)</span> tenderà ad essere minore della
distanza tra <span class="math inline">\(x\)</span> e <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[
\frac{\hat{y} - \bar{y}}{s_y} &lt; \frac{x - \bar{x}}{s_x}.
\]</span></p>
<p>Il termine ‘distanza’ è stato messo tra virgolette in quanto è necessario tenere in
considerazione l’unità di misura delle variabili. Per fare questo, la
distanza tra le osservazioni e il centro della distribuzione viene
misurata solo dopo avere standardizzato le variabili – ovvero, viene
misurata in unità di deviazioni standard.</p>
</div>
<div id="punti-influenti-e-valori-anomali" class="section level3" number="27.5.4">
<h3>
<span class="header-section-number">27.5.4</span> Punti influenti e valori anomali<a class="anchor" aria-label="anchor" href="#punti-influenti-e-valori-anomali"><i class="fas fa-link"></i></a>
</h3>
<p>La soluzione dei minimi quadrati è fortemente influenzata dalla presenza di punti influenti che sono anche delle osservazioni anomale. Un’osservazione anomala è un’osservazione con un residuo elevato (ovvero, avente un valore anomalo di <span class="math inline">\(y\)</span> rispetto alla previsione). Un punto di leva è un punto con un valore anomalo <span class="math inline">\(x\)</span>. Un punto influente è un’osservazione che influenza in maniera rilevante le stime dei minimi quadrati. Non sempre un punto anomalo è anche un punto influente. Per contro esistono punti non anomali che influiscono notevolmente sulle stime dei minimi quadrati – si veda la Figura <a href="il-modello-statistico-della-regressione-lineare.html#fig:puntianominfl">27.4</a>.</p>
<div class="figure" style="text-align: center">
<span id="fig:puntianominfl"></span>
<img src="images/leverage_outliers.pdf" alt="Osservazioni anomale e osservazioni influenti." width="70%"><p class="caption">
Figura 27.4: Osservazioni anomale e osservazioni influenti.
</p>
</div>
</div>
</div>
<div id="bontà-delladattamento" class="section level2" number="27.6">
<h2>
<span class="header-section-number">27.6</span> Bontà dell’adattamento<a class="anchor" aria-label="anchor" href="#bont%C3%A0-delladattamento"><i class="fas fa-link"></i></a>
</h2>
<p>Il secondo obiettivo dell’analisi della regressione è quello di misurare la bontà di adattamento del modello di regressione ai dati.</p>
<div id="errore-standard-della-stima-1" class="section level3" number="27.6.1">
<h3>
<span class="header-section-number">27.6.1</span> Errore standard della stima<a class="anchor" aria-label="anchor" href="#errore-standard-della-stima-1"><i class="fas fa-link"></i></a>
</h3>
<p>Un indice assoluto della bontà di adattamento è fornito dalla deviazione
standard dei residui, <span class="math inline">\(s_e\)</span>, chiamata anche <em>errore standard della
stima</em>. Uno stimatore non distorto della varianza dei residui nella
popolazione è dato da</p>
<p><span class="math display" id="eq:varres">\[\begin{equation}
s^2_e = \frac{\sum e_i^2}{n-2}
\tag{27.9}
\end{equation}\]</span>
e quindi l’errore standard della stima sarà
<span class="math display" id="eq:sdres">\[\begin{equation}
s_e = \sqrt{\frac{\sum e_i^2}{n-2}}.
\tag{27.10}
\end{equation}\]</span>
Dato che <span class="math inline">\(s_e\)</span> è possiede la stessa unità di misura della variabile <span class="math inline">\(y\)</span>, l’errore standard della stima può essere considerato come una sorta di “residuo medio.”</p>
<p>Consideriamo nuovamente l’esempio dei gemelli monozigoti separati alla
nascita. L’errore standard della regressione</p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">e</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> 
<span class="co">#&gt; [1] 6.101646</span></code></pre></div>
<p>è simile, anche se non identico, al valore medio dei residui</p>
<div class="sourceCode" id="cb289"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">fm</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span><span class="op">)</span> 
<span class="co">#&gt; [1] 4.91695</span></code></pre></div>
<p>In conclusione, se usiamo la retta di regressione per predire il quoziente di intelligenza del gemello nato per secondo a partire dal quoziente di intelligenza del gemello nato per primo compiamo, in media, un errore di circa 6 punti.</p>
</div>
<div id="indice-di-determinazione" class="section level3" number="27.6.2">
<h3>
<span class="header-section-number">27.6.2</span> Indice di determinazione<a class="anchor" aria-label="anchor" href="#indice-di-determinazione"><i class="fas fa-link"></i></a>
</h3>
<p>Un importante risultato dei minimi quadrati riguarda la cosiddetta <em>scomposizione della devianza di regressione</em> mediante la quale si definisce l’indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati
del campione. Come indicato nella figura <a href="il-modello-statistico-della-regressione-lineare.html#fig:scompdev">27.5</a>, per una generica osservazione
<span class="math inline">\(x_i, y_i\)</span>, la variazione di <span class="math inline">\(y_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span> può essere descritta come la somma di due componenti: il residuo <span class="math inline">\(e_i=y_i- \hat{y}_i\)</span> e lo scarto di <span class="math inline">\(\hat{y}_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span>: <span class="math inline">\(y_i - \bar{y} = (y_i- \hat{y}_i) + (\hat{y}_i - \bar{y}) = e_i + (\hat{y}_i - \bar{y})\)</span>.</p>
<div class="figure" style="text-align: center">
<span id="fig:scompdev"></span>
<img src="images/scomposizione.pdf" alt="Scomposizione della devianza." width="50%"><p class="caption">
Figura 27.5: Scomposizione della devianza.
</p>
</div>
<p>Se consideriamo tutte le osservazioni, la devianza delle <span class="math inline">\(y\)</span> può essere scomposta nel seguente modo:</p>
<p><span class="math display">\[
\begin{aligned}
 \sum (y_i - \bar{y})^2 &amp;= \sum \left[ e_i + (\hat{y}_i - \bar{y})
 \right]^2 
 = \sum e_i^2 + \sum (\hat{y}_i - \bar{y})^2 + 2 \sum e_i (\hat{y}_i -
 \bar{y}) \notag\end{aligned}
 \]</span></p>
<p>Per i vincoli imposti sui residui dalle equazioni normali, il doppio prodotto si annulla, infatti</p>
<p><span class="math display">\[
\begin{aligned}
\sum e_i (\hat{y}_i - \bar{y}) &amp;= \sum e_i \hat{y}_i - \bar{y}\sum e_i = \sum e_i (a + b x_i) \notag \\
&amp;= a \sum e_i + b \sum e_i x_i = 0 \notag\end{aligned}
\]</span></p>
<p>Di conseguenza, possiamo concludere che la devianza totale (<span class="math inline">\(\dev_T\)</span>) si scompone nella somma della devianza di dispersione (<span class="math inline">\(dev_E\)</span>) e della devianza di regressione (<span class="math inline">\(\dev_T\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
\underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\tiny{\text{Devianza
totale}}} &amp;= \underbrace{\sum_{i=1}^n e_i^2}_{\tiny{\text{Devianza
di dispersione}}} + \underbrace{\sum_{i=1}^n  (\hat{y}_i -
\bar{y})^2}_{\tiny{\text{Devianza di regressione}}} \notag
\end{aligned}
\]</span></p>
<p>La devianza di regressione, <span class="math inline">\(dev_R \triangleq dev_T - dev_E\)</span>, indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto <span class="math inline">\(dev_T/dev_T\)</span>, detto <em>indice di determinazione</em>, esprime tale riduzione degli errori in termini proporzionali e definisce il coefficiente di correlazione al quadrato:</p>
<p><span class="math display" id="eq:rsq">\[\begin{equation}
r^2 \triangleq \frac{dev_R}{dev_T} = 1 - \frac{dev_E}{dev_T}.
\tag{27.11}
\end{equation}\]</span></p>
<p>Quando l’insieme di tutte le deviazioni della <span class="math inline">\(y\)</span> dalla media è spiegato dall’insieme di tutte le deviazioni della variabile teorica <span class="math inline">\(\hat{y}\)</span> dalla media, si ha che l’adattamento (o accostamento) del modello al campione di dati è perfetto, la devianza residua è nulla ed <span class="math inline">\(r^2 = 1\)</span>; nel caso opposto, la variabilità totale coincide con quella residua, per cui <span class="math inline">\(r^2 = 0\)</span>. Tra questi due estremi, <span class="math inline">\(r\)</span> indica l’intensità della relazione lineare tra le due variabili e <span class="math inline">\(r^2\)</span>, con <span class="math inline">\(0 \leq r^2 \leq 1\)</span>, esprime la porzione della devianza totale della <span class="math inline">\(y\)</span> che è spiegata dalla regressione lineare sulla <span class="math inline">\(x\)</span>.</p>
<p>Per i dati dei gemelli monozitoti separati alla nascita, la devianza totale si scompone nelle componenti di “devianza spiegata” e “devianza non spiegata” nel modo seguente:</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">dev_t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> 
<span class="va">dev_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">yhat</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="va">dev_e</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">iq2</span> <span class="op">-</span> <span class="va">yhat</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></code></pre></div>
<p>le quali assumono i valori, rispettivamente, pari a <span class="math inline">\(3206.618\)</span>, <span class="math inline">\(2015.255\)</span> e <span class="math inline">\(1191.363\)</span>. Ne segue che il coefficiente di determinazione è <code>dev_r / dev_t = 0.628</code>, ovvero <code>1 - dev_e / dev_t = 0.628</code>. Questo risultato coincide con quello trovato con <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>:</p>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span>
<span class="co">#&gt; [1] 0.6284675</span></code></pre></div>
<p>Possiamo quindi concludere che, nel caso del campione esaminato, i fattori genetici spiegano circa il 63% della varianza del quoziente di intelligenza dei gemelli monozigoti (quando prevediamo il QI dei secondi nati dal QI dei primi nati).</p>
</div>
</div>
<div id="inferenza-sullassociazione-tra-x-e-y-nella-popolazione" class="section level2" number="27.7">
<h2>
<span class="header-section-number">27.7</span> Inferenza sull’associazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nella popolazione<a class="anchor" aria-label="anchor" href="#inferenza-sullassociazione-tra-x-e-y-nella-popolazione"><i class="fas fa-link"></i></a>
</h2>
<p>Il terzo obiettivo dell’analisi di regressione è quello di fare inferenze sull’associazione tra le due variabili nella popolazione da cui il campione deriva. Ci chiediamo se l’associazione osservata nel campione rifletta le proprietà della popolazione oppure sia imputabile agli errori di campionamento.</p>
<p>Se si segue la scuola frequentista, nella regressione bivariata il
problema dell’inferenza statistica è basato sulla stessa logica seguita
nel caso di una singola variabile aleatoria. Nell’inferenza su una
media, per esempio, viene valutata l’ipotesi nulla <span class="math inline">\(H_0: \mu=0\)</span> e il
parametro di interesse, la media <span class="math inline">\(\mu\)</span> della popolazione, viene stimato
mediante un’opportuna statistica, ovvero la media campionaria <span class="math inline">\(\bar{y}\)</span>.
Le inferenze statistiche sono basate sulla conoscenza delle proprietà
della distribuzione della statistica campionaria <span class="math inline">\(\bar{y}\)</span>.</p>
<p>È possibile però anche definire degli stimatori che dipendono da due (o
più) caratteri. Per esempio, il coefficiente <span class="math inline">\(b\)</span> della retta di
regressione campionaria, che viene usato quale stimatore del
coefficiente angolare <span class="math inline">\(\beta\)</span> della funzione di regressione nella
popolazione <span class="math inline">\(y = \alpha + \beta x + \varepsilon\)</span>, è definito rispetto a
due caratteri, <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Per ciascun campione casuale di <span class="math inline">\(n\)</span>
osservazioni <span class="math inline">\(x, y\)</span>, lo stimatore <span class="math inline">\(b\)</span> di <span class="math inline">\(\beta\)</span> assume un diverso
valore (<span class="math inline">\(b\)</span> è una variabile aleatoria). L’insieme delle stime <span class="math inline">\(b\)</span> di
<span class="math inline">\(\beta\)</span> nell’universo dei campioni di ampiezza <span class="math inline">\(n\)</span> costituisce la
<em>distribuzione campionaria</em> di <span class="math inline">\(b\)</span>. Analogamente si può dire dello
stimatore <span class="math inline">\(a\)</span> di <span class="math inline">\(\alpha\)</span>. Il problema che ci poniamo ora è appunto
quello di descrivere le proprietà delle distribuzioni campionarie dei
due stimatori dei minimi quadrati <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>. Per fare questo, dobbiamo
però prima introdurre il modello statistico della regressione lineare.</p>
<div id="modello-statistico-di-regressione-lineare" class="section level3" number="27.7.1">
<h3>
<span class="header-section-number">27.7.1</span> Modello statistico di regressione lineare<a class="anchor" aria-label="anchor" href="#modello-statistico-di-regressione-lineare"><i class="fas fa-link"></i></a>
</h3>
<p>In corrispondenza di a ciascun valore della variabile <span class="math inline">\(x\)</span>, che si
ipotizza essere costante da campione a campione, corrisponde nella
popolazione una distribuzione di valori <span class="math inline">\(y\)</span>. Ci chiediamo che relazione
intercorra tra le medie condizionali <span class="math inline">\(\bar{y}_i \mid x_i\)</span> e la variabile
<span class="math inline">\(x\)</span>. Se disponiamo di un campione di ciascuna distribuzione condizionata
<span class="math inline">\(y_i \mid x_i\)</span>, allora possiamo calcolare la media condizionale nel
campione per stimare la corrispondente media nella popolazione. Una tale
situazione si può verificare in un contesto sperimentale, in cui,
mantenendo fissi i valori del carattere <span class="math inline">\(x\)</span>, la ripetizione delle prove
produce un campione del carattere <span class="math inline">\(y\)</span> subordinatamente ad ogni <span class="math inline">\(x\)</span>.
Nel caso di dati di tipo osservazionale, invece, vengono osservate
coppie di valori (<span class="math inline">\(x_i, y_i\)</span>), con <span class="math inline">\(i=1, \dots, n\)</span>, e per ogni valore
<span class="math inline">\(x\)</span> si ha a disposizione un unico valore <span class="math inline">\(y\)</span>.</p>
<p>Allo scopo di attenuare le conseguenze derivanti dalle limitazioni di
cui soffrono i dati a disposizione, si definisce il <em>modello statistico
di regressione lineare</em> introducendo nell’analisi delle ipotesi sulla
popolazione. Il modello statistico di regressione è basato sulle quattro seguenti
ipotesi a proposito della struttura della popolazione.</p>
<ol style="list-style-type: decimal">
<li><p>La funzione di regressione è lineare (<em>linearità</em>):
<span class="math display">\[
\mathbb{E}(y_i \mid x_1, \dots, x_n) = \alpha + \beta x_i, \quad
i=1, \dots, n,
\]</span>
ovvero, le medie delle distribuzioni condizionali <span class="math inline">\(y \mid x_i\)</span> sono linearmente associate alla variabile esplicativa <em>x</em>.</p></li>
<li><p>Le varianze delle distribuzioni condizionali <span class="math inline">\(y \mid x_i\)</span> sono costanti al variare della <span class="math inline">\(x\)</span> (<em>omoschedasticità</em>):
<span class="math display">\[
var(y_i \mid x_1, \dots,  x_n) = \sigma^2, \quad i=1,
\dots, n.
\]</span></p></li>
<li><p>Le osservazioni <span class="math inline">\(y_i\)</span> sono tra loro incorrelate subordinatamente alle <span class="math inline">\(x_i\)</span> (<em>indipendenza</em>):
<span class="math display">\[
cov(y_i, y_j \mid x_1, \dots, x_n) = 0, \quad per \hskip.1 in i \neq j,
\]</span>
ovvero, l’osservazione <span class="math inline">\(y_i\)</span> è selezionata dalla distribuzione condizionale <span class="math inline">\(y_i \mid x_i\)</span> tramite un campionamento casuale indipendente.</p></li>
<li><p>La distribuzione di <span class="math inline">\(y_i\)</span> subordinata a <span class="math inline">\(X=x_i\)</span> segue la distribuzione gaussiana (<em>normalità</em>):
<span class="math display">\[
(y_i \mid x_i) \sim \mathcal{N}(\alpha+\beta x_i, \sigma^2).
\]</span></p></li>
</ol>
</div>
<div id="proprietà-degli-stimatori-dei-minimi-quadrati" class="section level3" number="27.7.2">
<h3>
<span class="header-section-number">27.7.2</span> Proprietà degli stimatori dei minimi quadrati<a class="anchor" aria-label="anchor" href="#propriet%C3%A0-degli-stimatori-dei-minimi-quadrati"><i class="fas fa-link"></i></a>
</h3>
<p>Può essere dimostrato (vedi Appendici) che, se le assunzioni del modello lineare sono soddisfatte, allora i coefficienti dei minimi quadrati avranno le seguenti proprietà:</p>
<p><span class="math display" id="eq:propcoefminquad">\[\begin{equation}
\begin{aligned}
b &amp;\sim \mathcal{N}\bigg(\beta,  \frac{\sigma^2_{\varepsilon}}{\sum(x_i-\bar{x})^2}\bigg),\\
a &amp;\sim \mathcal{N}\bigg(\alpha, \frac{\sigma^2_{\varepsilon}\textstyle\sum x_i^2}{n \textstyle\sum (x_i-\bar{x})^2} \bigg).
\end{aligned}
\tag{27.12}
\end{equation}\]</span></p>
</div>
<div id="le-inferenze-sul-modello-di-regressione" class="section level3" number="27.7.3">
<h3>
<span class="header-section-number">27.7.3</span> Le inferenze sul modello di regressione<a class="anchor" aria-label="anchor" href="#le-inferenze-sul-modello-di-regressione"><i class="fas fa-link"></i></a>
</h3>
<p>L’inferenza statistica sul modello di regressione può essere svolta in modi diversi. Esamineremo qui l’approccio frequentista per affrontare in seguito l’approccio Bayesiano.</p>
<p>L’inferenza statistica frequentista si articola nella formulazione degli intervalli di confidenza per i parametri di interesse e nei test di significatività statistica.
Un’ipotesi che viene frequentemente sottoposta a verifica è quella di significatività, cioè l’ipotesi che alla variabile esplicativa sia associato un coefficiente nullo. In tal caso, l’ipotesi nulla è
<span class="math display">\[
H_0:\beta=0
\]</span>
e l’ipotesi alternativa è
<span class="math display">\[
H_1:\beta \neq 0.
\]</span>
Sotto l’ipotesi nulla <span class="math inline">\(H_0: \beta = 0\)</span> la statistica
<span class="math display">\[
t_{\hat{\beta}} = \frac{\hat{\beta}}{s_{\hat{\beta}}}
\]</span>
si distribuisce come una variabile aleatoria <span class="math inline">\(t\)</span> di Student con <span class="math inline">\(n-2\)</span> gradi
di libertà.</p>
<p>Di fronte al problema di decidere se il valore stimato <span class="math inline">\(\hat{\beta}\)</span> sia
sufficientemente ‘distante’ da zero, in modo da respingere l’ipotesi
nulla che il vero valore <span class="math inline">\(\beta\)</span> sia nullo, non è sufficiente basarsi
soltanto sul valore numerico assunto da <span class="math inline">\(\hat{\beta}\)</span>, ma occorre tener
conto della variabilità campionaria. La statistica ottenuta dividendo
<span class="math inline">\(\hat{\beta}\)</span> per la stima del suo errore standard, <span class="math inline">\(s_{\hat{\beta}}\)</span>,
ci permette di utilizzare la distribuzione <span class="math inline">\(t\)</span> di Student come metrica
per stabilire se la stima trovata si debba considerare ‘diversa’ da
quanto ipotizzato sotto <span class="math inline">\(H_0\)</span>.</p>
<p>L’ipotesi nulla viene rifiutata quando il valore assoluto del rapporto è
esterno alla regione di accettazione, i cui limiti sono definiti dai
valori critici della distribuzione <span class="math inline">\(t\)</span> di Student con <span class="math inline">\(n - 2\)</span> gradi di
libertà per il livello di significatività <span class="math inline">\(\alpha\)</span> prescelto. Se
l’ipotesi nulla viene rifiutata si dice che il coefficiente
<span class="math inline">\(\hat{\beta}\)</span> è “statisticamente significativo” ammettendo così la
possibilità di descrivere con un modello lineare la relazione esistente
tra le variabili <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Quando non si può rifiutare l’ipotesi nulla
nel modello di regressione, si conclude che il coefficiente angolare
della retta non risulta significativamente diverso da zero, individuando
così nella popolazione una retta parallela all’asse delle ascisse.</p>
<p>Il valore-<span class="math inline">\(p\)</span> esprime la probabilità di ottenere un valore del test
uguale o superiore a quello ottenuto nel campione esaminato, utilizzando
la distribuzione campionaria del test sotto l’ipotesi nulla. Se
<span class="math inline">\(t_{\hat{\beta}}\)</span> è il valore osservato del rapporto <span class="math inline">\(t\)</span> per il
coefficiente angolare della retta di regressione, allora il <span class="math inline">\(p\)</span>-valore è
dato da <span class="math display">\[p = 2 \times Pr(t \geq |t_{\hat{\beta}}|),\]</span> dove <span class="math inline">\(t\)</span> è il
valore di una variabile aleatoria <span class="math inline">\(t\)</span> di Student con <span class="math inline">\((n-2)\)</span> gradi di
libertà.</p>
<p>Ogni volta che il <span class="math inline">\(p\)</span>-valore del test è inferiore al livello di
significatività che si è scelto per <span class="math inline">\(H_0\)</span>, il test porta al rifiuto
dell’ipotesi nulla. Solitamente si sceglie un livello <span class="math inline">\(\alpha\)</span> pari a
0.05 o 0.01.</p>
<p>Consideriamo nuovamente la regressione del QI del secondo nato sul QI del primo nato nei gemelli monozigoti esaminati da <span class="citation"><a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">Anderson &amp; Finn</a> (<a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">2012</a>)</span>. Dall’output prodotto dalla funzione <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> possiamo ricavare le informazioni per il calcolo della statistica <span class="math inline">\(t\)</span>:</p>
<div class="sourceCode" id="cb292"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = iq2 ~ iq1, data = df)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      Min       1Q   Median       3Q      Max </span>
<span class="co">#&gt; -11.0342  -3.8218  -0.5852   3.4658  12.5635 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   1.8389     3.0269   0.608    0.548    </span>
<span class="co">#&gt; iq1           0.8499     0.1155   7.357 2.29e-08 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 6.102 on 32 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.6285, Adjusted R-squared:  0.6169 </span>
<span class="co">#&gt; F-statistic: 54.13 on 1 and 32 DF,  p-value: 2.288e-08</span></code></pre></div>
<p>che risulta essere</p>
<p><span class="math display">\[
t = \frac{B}{s_{\hat{\beta}}}=\frac{0.8499}{0.1155} = 7.357.
\]</span>
Supponendo un’ipotesi alternativa bidirezionale, <span class="math inline">\(H_1: \beta \neq 0\)</span>, la regione critica sarà suddivisa nelle due code della distribuzione <span class="math inline">\(t\)</span> di Student con <span class="math inline">\(25\)</span> gradi di libertà. Essendo il valore critico <span class="math inline">\(t_{n-2, 1-\alpha/2}\)</span> pari a</p>
<div class="sourceCode" id="cb293"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">.975</span>, <span class="fl">32</span><span class="op">)</span>
<span class="co">#&gt; [1] 2.036933</span></code></pre></div>
<p>si può rifiutare <span class="math inline">\(H_0\)</span>.</p>
<p>In maniera corrispondente, possiamo considerare il <span class="math inline">\(p\)</span>-valore. Il <span class="math inline">\(p\)</span>-valore è l’area sottesa alla funzione di densità <span class="math inline">\(t\)</span> di Student con <span class="math inline">\(n-2=32\)</span> gradi di libertà nei due intervalli <span class="math inline">\([-\infty, -t_{\hat{\beta}}]\)</span> e <span class="math inline">\([t_{\hat{\beta}}, \infty]\)</span> è</p>
<div class="sourceCode" id="cb294"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fl">7.357</span>, <span class="fl">32</span><span class="op">)</span>
<span class="co">#&gt; [1] 1.145083e-08</span></code></pre></div>
<p>Dato che il <span class="math inline">\(p\)</span>-valore è minore di <span class="math inline">\(\alpha = 0.05\)</span>, l’approccio frequentista conclude rigettando <span class="math inline">\(H_0\)</span>. Il risultato si può riportare nel modo seguente:</p>
<blockquote>
<p>L’analisi della regressione bivariata ha rivelato una relazione lineare positiva tra il QI dei gemelli monozigoti primi nati e il QI dei gemelli secondi nati, <span class="math inline">\(\hat{\beta} = 0.85\)</span>, <span class="math inline">\(t_{32} = 7.36\)</span>, <span class="math inline">\(p = .0001\)</span>.</p>
</blockquote>
<p>I test di significatività possono essere eseguiti con R  utilizzando la funzione <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> applicata all’oggetto creato dal <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>: Il test statistico sul parametro <span class="math inline">\(\beta\)</span> del modello di regressione verifica l’ipotesi nulla di indipendenza, ovvero l’ipotesi che, nella popolazione, la pendenza della retta di regressione sia uguale a zero.</p>
<p>Più informativo del test statistico <span class="math inline">\(H_0: \beta=0\)</span> è l’intervallo di confidenza per il parametro <span class="math inline">\(\beta\)</span>:
<span class="math display">\[
\hat{\beta} \pm t_{\alpha/2} s_{\hat{\beta}}.
\]</span>
Nel caso presente, abbiamo</p>
<div class="sourceCode" id="cb295"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fm</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">.025</span>, <span class="fl">32</span><span class="op">)</span> <span class="op">*</span> <span class="fl">0.1155</span>
<span class="co">#&gt; [1] 1.0851203 0.6145887</span></code></pre></div>
<p>Dato che il limite inferiore dell’intervallo di confidenza è superiore allo zero, possiamo concludere che vi è un’<em>associazione</em> (lineare) <em>positiva</em> tra il QI del primo nato e il QI del secondo nato, nelle coppie di gemelli monozigoti che sono state esaminate da <span class="citation"><a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">Anderson &amp; Finn</a> (<a href="bibliografia.html#ref-anderson2012new" role="doc-biblioref">2012</a>)</span>.</p>
</div>
</div>
<div id="considerazioni-conclusive" class="section level2 unnumbered">
<h2>Considerazioni conclusive<a class="anchor" aria-label="anchor" href="#considerazioni-conclusive"><i class="fas fa-link"></i></a>
</h2>
<p>Il modello di regressione lineare semplice viene usato per descrivere la
relazione tra due variabili e per determinare il segno e l’intensità di
tale relazione. Inoltre, il modello di regressione ci consente di
prevedere il valore della variabile dipendente in base ad alcuni nuovi
valori della variabile indipendente. Il modello di regressione lineare
semplice è in realtà molto limitato, in quanto descrive soltanto la
relazione tra la variabile dipendente <span class="math inline">\(y\)</span> e una sola variabile
esplicativa <span class="math inline">\(x\)</span>. Esso diventa molto più utile quando incorpora più
variabili indipendenti. In questo secondo caso, però, i calcoli per la
stima dei coefficienti del modello diventano più complicati. Abbiamo
deciso qui di presentare solo il modello di regressione lineare semplice
perché, in quel caso, sia la logica dell’inferenza sia le procedure di
calcolo sono facilmente maneggiabili. Nel caso più generale, quello del
modello di regressione multipla, la logica dell’inferenza rimarrà
identica a quella discussa qui, ma le procedure di calcolo richiedono
l’uso dell’algebra matriciale che esula dagli scopi del presente
insegnamento. Il modello di regressione multipla può includere sia
regressori quantitativi, sia regressori qualitativi, utilizzando un
opportuna schema di codifica. È interessante notare come un modello di
regressione multipla che include una sola variabile esplicativa
quantitativa corrisponde all’analisi della varianza ad una via; un
modello di regressione multipla che include più di una variabile
esplicativa quantitativa corrisponde all’analisi della varianza più vie.
Questi argomenti verranno sviluppati negli insegnamenti di carattere
quantitativo più avanzati. Possiamo qui concludere dicendo che il
modello di regressione, nelle sue varie forme e varianti, costituisce la
tecnica di analisi dei dati maggiormente usata in psicologia.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="una-breve-introduzione-al-modello-di-regressione.html"><span class="header-section-number">26</span> Una breve introduzione al modello di regressione</a></div>
<div class="next"><a href="inferenza-bayesiana.html"><span class="header-section-number">28</span> Inferenza Bayesiana</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#il-modello-statistico-della-regressione-lineare"><span class="header-section-number">27</span> Il modello statistico della regressione lineare</a></li>
<li><a class="nav-link" href="#la-funzione-lineare"><span class="header-section-number">27.1</span> La funzione lineare</a></li>
<li><a class="nav-link" href="#lerrore-di-misurazione"><span class="header-section-number">27.2</span> L’errore di misurazione</a></li>
<li><a class="nav-link" href="#scopi-della-regressione-lineare"><span class="header-section-number">27.3</span> Scopi della regressione lineare</a></li>
<li><a class="nav-link" href="#quantificare-lassociazione-fra-due-caratteri-quantitativi"><span class="header-section-number">27.4</span> Quantificare l’associazione fra due caratteri quantitativi</a></li>
<li>
<a class="nav-link" href="#stime-dei-minimi-quadrati"><span class="header-section-number">27.5</span> Stime dei minimi quadrati</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#monotwinsiq"><span class="header-section-number">27.5.1</span> Un esempio concreto</a></li>
<li><a class="nav-link" href="#sec:beta_r"><span class="header-section-number">27.5.2</span> Coefficiente angolare e correlazione di Pearson</a></li>
<li><a class="nav-link" href="#regressione-verso-la-media"><span class="header-section-number">27.5.3</span> Regressione verso la media</a></li>
<li><a class="nav-link" href="#punti-influenti-e-valori-anomali"><span class="header-section-number">27.5.4</span> Punti influenti e valori anomali</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bont%C3%A0-delladattamento"><span class="header-section-number">27.6</span> Bontà dell’adattamento</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#errore-standard-della-stima-1"><span class="header-section-number">27.6.1</span> Errore standard della stima</a></li>
<li><a class="nav-link" href="#indice-di-determinazione"><span class="header-section-number">27.6.2</span> Indice di determinazione</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#inferenza-sullassociazione-tra-x-e-y-nella-popolazione"><span class="header-section-number">27.7</span> Inferenza sull’associazione tra \(x\) e \(y\) nella popolazione</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#modello-statistico-di-regressione-lineare"><span class="header-section-number">27.7.1</span> Modello statistico di regressione lineare</a></li>
<li><a class="nav-link" href="#propriet%C3%A0-degli-stimatori-dei-minimi-quadrati"><span class="header-section-number">27.7.2</span> Proprietà degli stimatori dei minimi quadrati</a></li>
<li><a class="nav-link" href="#le-inferenze-sul-modello-di-regressione"><span class="header-section-number">27.7.3</span> Le inferenze sul modello di regressione</a></li>
</ul>
</li>
<li><a class="nav-link" href="#considerazioni-conclusive">Considerazioni conclusive</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science per psicologi</strong>" was written by Corrado Caudek. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
